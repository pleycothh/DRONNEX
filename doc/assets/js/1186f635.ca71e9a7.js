"use strict";(self.webpackChunkdocs_site=self.webpackChunkdocs_site||[]).push([[9619],{2873:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"wlbp","metadata":{"permalink":"/doc/blog/wlbp","editUrl":"https://github.com/pleycothh/DRONNEX/tree/main/docs-site/blog/2025-06-30-work-log.md","source":"@site/blog/2025-06-30-work-log.md","title":"Work Log (Week 1)","description":"Current Progress","date":"2025-06-30T00:00:00.000Z","tags":[{"inline":true,"label":"simpleFOC","permalink":"/doc/blog/tags/simple-foc"}],"readingTime":2.49,"hasTruncateMarker":false,"authors":[{"name":"Ben Li","title":"Software engineer","url":"https://github.com/pleycothh","page":{"permalink":"/doc/blog/authors/ben"},"socials":{"x":"https://x.com/dronnex","github":"https://github.com/pleycothh"},"imageURL":"https://github.com/pleycothh.png","key":"ben"}],"frontMatter":{"slug":"wlbp","title":"Work Log (Week 1)","authors":["ben"],"tags":["simpleFOC"]},"unlisted":false,"nextItem":{"title":"Human Body vs. Humanoid Robot Design","permalink":"/doc/blog/hbhrd"}},"content":"## Current Progress\\r\\n- Deng FOC V4 board, can rule two motor with Deng FOC V4 library only, limitated to i2c. ( PID adjust required)\\r\\n- Simple FOC library works in one MOTOR with esp32 only with 2.2.1 library + 2.0.17 ESP32 library. ( some known issue with dual motor, will fix in 2.3.5)\\r\\n- Simple FOC with STM32-F103 works with encoder only, due to time group issue, i2c will not work simply.\\r\\n\\r\\n**Failed test:**\\r\\n- Simple FOC with F411 black pill board: Issue time group\\r\\n\\r\\n**Issue Found:**\\r\\n- Simple FOC will pull up (HIGH) PIN13, 14 by default, witch will make STlink fail.\\r\\n- ESP32 can not work with dual motor on Simple FOC library, known issue, will fix in 2.3.5.\\r\\n- STM32 other than F103 and G431 not worked yet, need more analysis.\\r\\n\\r\\n\\r\\n## Final Objective\\r\\nTo be able to have 12 DOF robot with BLDC control, I need cheap, reliaable BLDC FOC module that canble replciate. WIFI on esp32 is not required, also size is too big and no can, leave me STM32 as best option.\\r\\n\\r\\nI need to test mroe STM32 option, to find best MCU + Encoder + driver conbonation to be reliable and eazy to build also cheap.\\r\\n\\r\\nI want to fix the module selection, this case code and pid can be replciate easly.\\r\\n\\r\\nMotor I will decide between 5010 260kv and 5056 140kv, board I will decide between 3 options:\\r\\n- odrive v3.6 50$ for 2 motors (50A) - Support Odrive, SImpleFOC\\r\\n- **STM32G431-ESC-1** 30$ for 1 motors (30A) - support SimpleFOC and STM Motor SDK\\r\\n- Drive Shiled + STM32 G431 15$ for 1 motor (20A) - support SimpleFOC\\r\\n\\r\\nBuild simple project like balance bot or Desk Arm is good for intergrating test with above set up.\\r\\n- Balance bot (FOC control for 2 motor, PID control)\\r\\n- 2 leg balance bot  (FOC control for 2 motor, PID control, RC controll, )\\r\\n\\r\\n## Future Planning\\r\\n\\r\\nBecause currently I stuck on running simple FOC code with ESP32, I need to do more testing, with more valid working case, I can make final decision related to STM32 G4 serial.\\r\\n\\r\\n- I need stop work on esp32 two motor project untill ( Simple FOC 2.3.5).\\r\\n- I can start pick up 2 leg bot while wating G4 chip.\\r\\n- 3 5010 motor arm, greate for 1 leg project.\\r\\n    - G474 + 3 driver + 3 5010\\r\\n    - (F103 + driver + 5010) * 3\\r\\n\\r\\n- need to make decision, is odrive needed?\\r\\n- Or G431-esc with power shiled, 30A is enough? Need to torque test.\\r\\n\\r\\n**Decision:**\\r\\n1. 5010 260kv motor with 10:1 gear: torque output per Amps.\\r\\n2. 5056 140KV motor with 10:1 gear: torque output per Amps.\\r\\n\\r\\nif 5010 is enought, 30A drive is enought.\\r\\n\\r\\nif 5056 50A required, odrive is must have.\\r\\n\\r\\n\\r\\n**FOC TEST List:**\\r\\n- STM32G431-ESC \\r\\n    - Simple FOC\\r\\n    - STM Motor SDK (SK)\\r\\n- STM32G474 ( able to control 3 driver)\\r\\n    - Simple FOC\\r\\n- STM32F407\\r\\n    - Simple FOC \\r\\n- ESP32 ( Okay for hobby and POC, not sutable for dog.)\\r\\n- STM32 F103 ( back up for G431) ( only working version)\\r\\n- STM32 F411 ( no working)\\r\\n- STM32 H750 ( optional test)\\r\\n- STM32 H753 ( over kill for FOC only)\\r\\n- STM32 F405 ( 2 driver )"},{"id":"hbhrd","metadata":{"permalink":"/doc/blog/hbhrd","editUrl":"https://github.com/pleycothh/DRONNEX/tree/main/docs-site/blog/2025-05-24-human-vs-humanoid.md","source":"@site/blog/2025-05-24-human-vs-humanoid.md","title":"Human Body vs. Humanoid Robot Design","description":"Neural Control Frequencies: Human vs. Robot","date":"2025-05-24T00:00:00.000Z","tags":[{"inline":true,"label":"chatgpt","permalink":"/doc/blog/tags/chatgpt"}],"readingTime":22.51,"hasTruncateMarker":true,"authors":[{"name":"Ben Li","title":"Software engineer","url":"https://github.com/pleycothh","page":{"permalink":"/doc/blog/authors/ben"},"socials":{"x":"https://x.com/dronnex","github":"https://github.com/pleycothh"},"imageURL":"https://github.com/pleycothh.png","key":"ben"},{"name":"Chat GPT","title":"AI Assistant","url":"https://openai.com","page":{"permalink":"/doc/blog/authors/all-sebastien-lorber-articles"},"socials":{"x":"https://x.com/sebastienlorber","linkedin":"https://www.linkedin.com/in/sebastienlorber/","github":"https://github.com/slorber","newsletter":"https://thisweekinreact.com"},"imageURL":"https://openai.com/favicon.ico","key":"chatgpt"}],"frontMatter":{"slug":"hbhrd","title":"Human Body vs. Humanoid Robot Design","authors":["ben","chatgpt"],"tags":["chatgpt"]},"unlisted":false,"prevItem":{"title":"Work Log (Week 1)","permalink":"/doc/blog/wlbp"},"nextItem":{"title":"Roadmap to Embedded & Robotics Engineering","permalink":"/doc/blog/rtere"}},"content":"## Neural Control Frequencies: Human vs. Robot\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n**Human Low-Frequency Neural Signals:** The human motor system operates effectively with relatively\\r\\nlow-frequency control signals, on the order of only a few to a few tens of hertz. Voluntary movements\\r\\nand reflexes are orchestrated by neural impulses that typically range from about 5\u201350 Hz in frequency.\\r\\nFor example, the fastest monosynaptic stretch reflex in humans (the M1 response) has a latency of\\r\\nroughly 30\u201350 ms \u2013 equivalent to an update frequency on the order of 20\u201333 Hz. Longer-loop\\r\\nreflexes and voluntary reactions occur at even lower bandwidths (e.g. voluntary reaction times ~150 ms\\r\\nor ~6\u20137 Hz) . Despite these low neural update rates, the human body achieves smooth and stable\\r\\ncontrol of movement. This is possible because the musculoskeletal system and spinal cord provide builtin stability and rapid local responses that compensate for the limited signaling frequency of the nerves.\\r\\n\\r\\n**Robotic High-Frequency Control Loops:** By contrast, humanoid robots typically require much higher\\r\\ncontrol-loop frequencies \u2013 often in the hundreds of hertz to kilohertz range \u2013 to achieve stable control.\\r\\nModern robotic actuators (e.g. electric motors) and sensors can be polled and updated every 1\u20132 ms or\\r\\nfaster (hundreds to thousands of Hz). This high-frequency feedback is needed to satisfy control stability\\r\\ncriteria (Nyquist sampling of fast dynamics) and to actively correct errors before the rigid mechanism\\r\\nbecomes unstable . Indeed, engineered legged robots commonly run their feedback control at 1 kHz\\r\\nor more . Without such high-rate feedback, a stiff-legged robot can quickly become unstable after\\r\\nrapid disturbances like foot impacts. In effect, robots rely on \u201cbrute-force\u201d high-speed control to\\r\\nmaintain balance and performance, whereas humans rely more on intrinsic biomechanics and slower\\r\\nneural feedback. As a concrete example, one study notes that animals like cats (\u22484   kg) have\\r\\nsensorimotor delays up to ~41 ms, yet can run at ~5 Hz stride frequency, meaning neural feedback\\r\\ncannot even update for roughly half of a stance phase . Animals still remain stable thanks to passive\\r\\ndynamics, whereas a comparably rigid robot might demand a control loop in the kHz range to handle\\r\\nsuch rapid gait dynamics . This fundamental difference underscores why humans manage with\\r\\n**low-frequency signals while robots must often push toward high-frequency feedback loops for\\r\\nstability.** \\r\\n\\r\\n## Biological Elasticity, Proprioception, and Reflexes in Stability\\r\\n\\r\\n**Elasticity as a Stability Mechanism:** The human musculoskeletal system is replete with elastic\\r\\nelements \u2013 tendons, ligaments, connective tissues, and the intrinsic compliance of muscles themselves.\\r\\nThese elastic components store and release mechanical energy and provide passive stiffness and\\r\\ndamping. For instance, during running, the Achilles tendon and other elastic tissues act like springs to\\r\\nstore energy at foot contact and return it in push-off, significantly reducing the metabolic work muscles\\r\\nmust do . The **energetic economy** of human gait benefits greatly from this elasticity: tendons can\\r\\nreturn on the order of 50\u201360% of the positive work in activities like running, effectively recycling energy\\r\\nthat a rigid system would lose as heat . Beyond energy efficiency, elasticity also aids stability. An\\r\\nelastic leg will **absorb shock and self-stabilize** to some extent upon impact (like a pogo stick\\r\\ncompressing), which smooths out high-frequency disturbances **without requiring neural intervention\\r\\n.** Researchers refer to this instantaneous mechanical response as the preflex: the muscle-tendon\\r\\nunit\u2019s immediate reaction to perturbation, occurring within milliseconds \u2013 much faster than any nerve\\r\\nreflex can act . Experiments confirm that muscles exhibit a short-range stiffness and damping\\r\\nbehavior in the first few milliseconds of a perturbation, resisting sudden changes in length before the\\r\\nnervous system even has time to respond . This means the body\u2019s mechanics handle the first line\\r\\nof defense against disturbances, enabling the slower neural loops (tens of milliseconds or more) to\\r\\nissue corrections once the immediate shock is handled.\\r\\n\\r\\n**Proprioception and Reflex Loops:** The human nervous system is equipped with rich sensor feedback (proprioception) and fast reflexive control that complement mechanical elasticity. Muscle spindles measure stretch and speed in muscles, Golgi tendon organs sense tension, and joint receptors and the inner ear vestibular system provide information about body position and balance. These sensors feed into reflex pathways in the spinal cord and brainstem. Spinal **reflex loops** operate at latencies on the order of 30\u201380   ms in humans . For example, a sudden muscle stretch triggers a spinal reflex contraction (the stretch reflex) in ~40 ms, preventing the muscle from lengthening too quickly and contributing to joint stability. Slightly longer latency responses (50\u2013100 ms) involve multiple synapses or cortical input (e.g. postural reflexes and righting responses) . Collectively, these reflexes provide automatic feedback control that does not require conscious thought and runs at a higher frequency than conscious control. Crucially, though these neural feedback loops are \u201clow-frequency\u201d compared to modern computers, they are well-tuned to the dynamics of the body. The combination of passive elasticity and timely reflexes makes the human control system highly robust and efficient: perturbations are blunted mechanically and corrected reflexively, allowing the brain to operate with slow (~5\u201320 Hz) high-level signals without loss of stability.\\r\\n\\r\\n**Implications for Robots:** By contrast, traditional humanoid robots built with rigid joints and metal links lack the intrinsic elasticity and distributed sensing that biology provides. A rigid robot joint has little ability to absorb energy \u2013 a shock must be countered by active motor torque almost immediately. Furthermore, until recently robots had relatively sparse proprioceptive sensing (typically just joint encoders and maybe force/torque sensors), lacking the rich, built-in sensor density of human muscles and skin. This is why purely rigid robots have historically required very stiff control (high gain, high frequency) to emulate the stability that humans get \u201cfor free\u201d from muscles and reflexes. Modern robotics research has recognized this gap: adding compliance (springs, dampers, or flexible materials)\\r\\nto robots can dramatically improve stability and robustness in the face of delays or low feedback rates . In one study, adding a parallel elastic spring to a robot leg joint allowed stable landing control with feedback rates as low as 20 Hz despite significant sensorimotor delay, whereas the same task was unstable without compliance at such low frequencies . This mirrors the animal kingdom: biological elasticity and reflexes let animals tolerate slow neural loops, a design principle engineers are leveraging by building compliance and fast reflex-like controllers into robots.\\r\\n\\r\\n## Field-Oriented Motor Control and Proprioceptive Sensors in Robots\\r\\n**Muscle Control vs. Field-Oriented Control:** Humans control muscle force by modulating the firing rate\\r\\nof motor neurons and recruiting motor units, effectively adjusting the current sent to muscle fibers via\\r\\nneural impulses. In robotic systems, an analogous function is performed by Field-Oriented Control\\r\\n**(FOC)** of electric motors. FOC is an advanced control technique for brushless motors (BLDC or AC\\r\\nmotors) that regulates the motor\u2019s phase currents in real time, keeping the magnetic field oriented for\\r\\noptimal torque output. In practice, FOC algorithms run at very high frequency (often tens of kHz) on\\r\\nmotor drivers, far faster than any biological signaling, to ensure smooth and precise torque generation.\\r\\nThis inner loop can be seen as the robot\u2019s \u201calpha motor neuron\u201d equivalent \u2013 it drives actuators with the\\r\\nright amount of current (force) continuously. The combination of FOC and fast current sensing means a\\r\\nrobot\u2019s joint actuation is tightly controlled at the millisecond level, achieving a level of precision and\\r\\nresponsiveness in torque control that mimics the finely graded force control of human muscles.\\r\\nEncoder and IMU Feedback (Artificial Proprioception): Just as humans rely on proprioceptive\\r\\nfeedback from muscle sensors and the vestibular system, humanoid robots employ a suite of sensors to\\r\\nachieve closed-loop control. High-resolution rotary encoders on each joint provide continuous\\r\\nfeedback on joint angle and velocity \u2013 analogous to muscle spindle feedback about limb position. In\\r\\naddition, force/torque sensors or motor current sensors can indicate how much load a joint is bearing,\\r\\nakin to Golgi tendon organ sensing of tension. Whole-body orientation and balance in robots are\\r\\nmonitored by inertial measurement units (IMUs) (accelerometers and gyroscopes), which play a role\\r\\ncomparable to the human inner ear\u2019s vestibular system for detecting head orientation and acceleration.\\r\\nThese sensors enable reflex-like control loops in robots: for instance, a humanoid\u2019s balance controller\\r\\nmight use IMU data to trigger a rapid ankle adjustment when the robot tilts, much as a human\u2019s\\r\\nvestibulo-spinal reflex triggers muscle responses to prevent a fall.\\r\\n\\r\\nModern humanoid designs tightly integrate such sensors with their motor controllers. For example, a\\r\\nstate-of-the-art bionic leg or humanoid joint module might include a motor encoder, a joint output\\r\\nencoder, a 9-axis IMU, and even a 6-axis force sensor, all feeding into the control system . The motor\\r\\ncontroller uses these inputs to close multiple feedback loops \u2013 position, velocity, torque (current), and\\r\\neven impedance \u2013 in real time . This architecture parallels the multi-layered feedback in humans\\r\\n(spinal reflex loops, higher-level posture control, etc.). In effect, the robot\u2019s encoders and IMUs serve\\r\\nas its proprioceptive sense, and the FOC-based controllers act like artificial motor neurons and\\r\\nreflex circuits, keeping the machine balanced and on trajectory. The key difference is one of speed and\\r\\nimplementation: robotic sensors can be read thousands of times per second and control outputs\\r\\nupdated accordingly, whereas human proprioceptive feedback is slower but compensated by the body\u2019s\\r\\nmechanical design as discussed. Nonetheless, as robotics adopts bio-inspired approaches, the gap is\\r\\nnarrowing \u2013 robots are increasingly endowed with dense sensors and fast, layered control loops to\\r\\nachieve a more human-like finesse in movement.\\r\\n\\r\\n## The Need for High-Frequency Feedback in Robots\\r\\nOne fundamental question arises: given the effectiveness of biological low-frequency control, why do\\r\\nrobotic systems typically require such high-frequency feedback loops (100 Hz to 1 kHz or more) to\\r\\nremain stable? The answer lies in the differences in physical dynamics and inherent damping. Rigid\\r\\nrobotic systems have very low passive damping or compliance \u2013 a steel limb connected to a highstiffness servo will not naturally absorb shock or oscillations. Thus, if an unexpected perturbation\\r\\noccurs (say the robot\u2019s foot strikes an obstacle), the deviation in position or force can change very\\r\\nrapidly (within a few milliseconds). To correct this before it amplifies or causes tipping, the robot\u2019s\\r\\ncontrol system must sense the error and apply a counteracting torque almost immediately. This\\r\\ndemands a feedback cycle on the order of the system\u2019s fastest significant dynamics. In many robots,\\r\\nstructural vibrations or actuator electrical dynamics can have frequencies in the hundreds of hertz,\\r\\nrequiring controller update rates in the high hundreds or thousands of hertz (by Nyquist\u2019s criterion) .\\r\\nA practical rule in robotics is to run the inner control loops as fast as possible (often 1 kHz for torque/\\r\\nposition loops) to effectively make the robot \u201cfeel\u201d more damped and stable than its bare mechanical\\r\\nstructure would allow\\r\\n\\r\\nMoreover, unlike biological muscles, electric motors and gear trains have almost no intrinsic shock\\r\\nabsorption. Any delay or slack in responding to a disturbance can let energy build up, leading to\\r\\noscillation or instability. High-frequency control effectively adds active damping. For instance, if a\\r\\nhumanoid sways slightly, a 500 Hz balance controller can apply corrective joint torques 500 times per\\r\\nsecond, preventing the sway from growing. At 50 Hz control, the same robot might start to wobble or\\r\\novershoot because the corrections come too infrequently relative to its natural oscillation period. This is\\r\\nwhy humanoid robots like Boston Dynamics\u2019 Atlas or Honda\u2019s ASIMO historically run their low-level joint\\r\\ncontrollers at very high rates (0.5\u20131 kHz or more), and high-performance motor drivers often boast\\r\\nPWM/FOC frequencies of 10\u201320 kHz for smooth torque output. High feedback rates also help filter\\r\\nsensor noise and model uncertainties; with more frequent measurements and adjustments, the robot\\r\\ncan better reject disturbances.\\r\\n\\r\\nIt\u2019s worth noting that biological systems avoid this necessity via design: the combination of\\r\\ncompliance, distributed control (reflexes), and safe low gain tuning lets animals remain stable with\\r\\nslower updates. When engineers attempt to control robots with slower, human-like update rates\\r\\nwithout modifying hardware, they often encounter instability \u2013 illustrating how tightly coupled a robot\u2019s\\r\\nrequired control frequency is to its mechanical design. Research in legged robotics has demonstrated\\r\\nthat introducing elastic elements or other passive dynamics can allow stable control with much lower\\r\\nfeedback rates. One study achieved stable hopping and landing of a robot leg with only a 20 Hz control\\r\\nloop by incorporating a compliant parallel spring, effectively imitating the role of a tendon . The\\r\\nrobot could tolerate sensorimotor delays up to ~60 ms without falling over when this compliance was\\r\\npresent . This is a stark contrast to a fully rigid system, and it reinforces the principle that **if you give\\r\\na robot some \u201cmuscle and tendon\u201d properties, it no longer demands superhuman (or rather\\r\\nsupercomputer) reflexes to remain upright.**\\r\\n\\r\\n## Rigid Actuators vs. Compliant Muscle: Energy and Responsiveness\\r\\n\\r\\n**Limitations of Rigid Actuators:** Most current humanoid robots use electric motors coupled with highreduction gearboxes or harmonic drives to move their joints. These actuators are essentially rigid \u2013 they\\r\\noutput a fixed displacement or torque with very little compliance. While this yields precise position\\r\\ncontrol and high force, it comes with several drawbacks. Firstly, rigid actuators cannot store significant\\r\\nelastic energy. Any energy put into the system (e.g. an impact or the deceleration of a limb) must be\\r\\neither actively dissipated by the motor (often heating it) or it will cause a rebound or oscillation. This\\r\\nleads to poor energy efficiency, as motors work against sudden forces rather than with them. Secondly,\\r\\nlack of compliance means high impact forces are transmitted to the mechanism and environment. A\\r\\nstiff-legged robot landing from a jump generates impulsive forces that can damage hardware or make\\r\\nthe robot more prone to slipping. Humans, in contrast, have flex in their joints and tissues that cushion\\r\\nimpacts. Thirdly, rigid actuators make control extremely sensitive \u2013 a small error in commanded\\r\\nposition can create a large force (because the system is stiff), which can then lead to instability or jitter\\r\\nunless the control gains are kept low or the loop frequency is high. In summary, purely rigid, positioncontrolled robots tend to be energy-hungry, shock-sensitive, and require careful tuning to avoid\\r\\noscillations.\\r\\n\\r\\n**Biological Compliance and Softness:** The human body, and animals in general, demonstrate\\r\\nremarkable efficiency, agility, and robustness that outperform even state-of-the-art robots , and a\\r\\ncore reason is the compliance in biological actuators . Muscles and tendons act together as a\\r\\nseries elastic actuator: the tendon (and connective tissue) provides a spring in series with the muscle\u2019s\\r\\nforce generation. This has multiple benefits. It inherently limits peak forces (the spring stretches under\\r\\nextreme load, preventing infinite force transmission), thus protecting both the muscle and whatever the\\r\\nmuscle is pushing against \u2013 an aspect crucial for injury prevention and also for gentle interaction with\\r\\nthe environment. The elasticity allows energy storage: when you land from a jump, your Achilles\\r\\ntendon and arch of the foot stretch, storing energy that is given back as you push off . A rigid robot\\r\\nwould instead dissipate that energy or require the motors to actively perform negative work to\\r\\ndecelerate, wasting energy as heat. Compliance also enables power amplification: muscles can prestretch a tendon and then release it to get a catapult-like burst of motion (as in a jumping flea or a\\r\\nhuman vertical leap). In robotics, adding springs has shown to increase peak power output by releasing\\r\\nstored energy faster than motors alone could provide . Furthermore, compliance improves\\r\\nrobustness and adaptability. If the ground is uneven or softer than expected, a compliant leg\\r\\nautomatically adapts its shape slightly on contact (a phenomenon known as self-stability in\\r\\nlocomotion). This means the system is less dependent on perfect sensor information or timing. As\\r\\nAlexander and others have noted, animals use compliance to absorb impacts, to bounce like springs\\r\\nduring running, and to serve as return springs for repositioning limbs . Even metabolic efficiency is\\r\\nhigher: a review of animal locomotion studies found that compliance in muscles and tendons reduces\\r\\nthe metabolic cost of movement by taking on some of the work that muscles would otherwise have to\\r\\ndo, especially in cyclic tasks like hopping or running .\\r\\n\\r\\n**Series Elastic Actuators in Robotics:** To bridge this gap, roboticists have introduced compliance\\r\\nintentionally into actuators. A well-known approach is the Series Elastic Actuator (SEA), where a spring\\r\\n(often a steel coil or elastomer) is placed in series between the motor and the load. SEAs and related\\r\\ndesigns (variable stiffness actuators, parallel elastic actuators) confer many of the advantages of\\r\\nbiological muscle-tendon units: they can absorb and store energy, filter out high-frequency shocks, and\\r\\nproduce smoother, more controllable forces . The presence of a calibrated spring also allows\\r\\ndirect measurement of force (from spring deflection) and more robust force control. Studies have\\r\\ndemonstrated that adding series or parallel elasticity in robots can **improve energy efficiency and\\r\\nreduce peak power requirements** for motors . For example, adding a tailored compliance to a\\r\\nrobotic knee joint was shown to yield up to 50% energy savings in certain motions compared to a purely\\r\\nrigid actuation . Another result showed that parallel elastic elements in a bipedal robot improved\\r\\nlocomotion efficiency and reduced motor loads . Compliance can also significantly enhance\\r\\ninteraction safety \u2013 a soft joint is inherently safer for a robot working around humans, as it will yield if\\r\\nit hits a person, much like our muscles do, rather than imparting the full force of a rigid mechanism .\\r\\nThe trade-off is that introducing elasticity can make precise position control harder (since the spring can\\r\\noscillate); however, clever control strategies and the intrinsic benefits often outweigh this, especially for\\r\\ndynamic tasks. The current state-of-the-art humanoids often incorporate at least some passive or active\\r\\ncompliance to better emulate human muscle behavior. In short, **the softness of human tissue gives\\r\\nbiological systems a huge advantage in energy efficiency and responsiveness,** and robotics is\\r\\nincreasingly embracing that lesson by softening rigid actuators.\\r\\n\\r\\n## State of the Art in Artificial Muscle Technologies\\r\\n\\r\\nTo truly replicate the human body\u2019s capabilities, engineers are developing artificial muscle\\r\\ntechnologies \u2013 actuators that more directly mimic the properties of biological muscles (soft, compliant,\\r\\nhigh power-to-weight, efficient). Several cutting-edge approaches show promise.\\r\\n\\r\\nIn summary, artificial muscle technology is advancing rapidly, aiming to provide robots with contractile\\r\\nelements that rival or surpass human muscles in certain aspects. Electroactive polymers promise\\r\\nmuscle-like form factors and response; pneumatic and soft fluidic muscles provide power and\\r\\ncompliance; and new smart materials continue to emerge. These technologies address the limitations\\r\\nof traditional motors by bringing in softness, efficiency, and adaptability akin to biology. The challenge\\r\\nremains to integrate them into complete robotic systems that can also leverage suitable control\\r\\nstrategies \u2013 which brings us to the control paradigm inspired by the human nervous system.\\r\\n\\r\\n## Neuromorphic Control: Inspired by the Human Nervous System\\r\\n\\r\\nWhile new actuators mimic the muscles, neuromorphic control aims to mimic the brain and nervous\\r\\nsystem\u2019s way of processing information. Biological neural networks are fundamentally different from\\r\\nconventional robot control algorithms \u2013 they compute with spikes (discrete pulses) and operate in a\\r\\nhighly parallel, event-driven manner. This makes animal nervous systems extraordinarily efficient and\\r\\nrobust in the real world, easily outperforming today\u2019s robots in tasks like sensory integration, learning,\\r\\nand adaptive control . For example, our brains and spinal cords effortlessly handle balancing,\\r\\ngrasping, and walking on uncertain terrain, tasks that still challenge robots. Neuromorphic engineering\\r\\nseeks to capture some of these advantages by redesigning control systems to work more like neurons.\\r\\n\\r\\nSpiking Neural Networks (SNNs) and Brain-Inspired Chips: A key development is the use of spiking\\r\\nneural networks for robot control. In an SNN, neurons emit timed spikes and communicate in a way\\r\\nanalogous to real neurons, and such networks can be deployed on special neuromorphic hardware\\r\\nchips that run networks very efficiently. Research has shown that SNN-based controllers can be\\r\\nexceptionally fast and energy-efficient, since they naturally process information in parallel and only\\r\\nwhen events (spikes) occur . This is well-suited to robotics, where sensors often produce sparse\\r\\nsignals (e.g. a vision sensor detecting a change, or a touch sensor feeling contact). A neuromorphic\\r\\ncontroller can remain mostly idle (drawing negligible power) until relevant spikes indicate something to\\r\\nrespond to, much like our reflex circuits sit quiescent until triggered. Companies and research labs have\\r\\nbuilt neuromorphic chips (e.g. Intel\u2019s Loihi 2, IBM\u2019s TrueNorth, and others) that implement tens of\\r\\nthousands of spiking neurons in hardware, with event-driven computation that can run in real time for\\r\\nrobotic tasks. These chips have been tested in scenarios such as controlling a balancing robot, doing\\r\\npattern generation for locomotion, and even real-time adaptation \u2013 all with very low energy\\r\\nconsumption compared to a traditional CPU running equivalent control algorithms.\\r\\n\\r\\nReflexes and CPGs in Silicon: Another neuromorphic approach is to emulate specific neural circuits\\r\\nfrom biology, such as central pattern generators (CPGs) and reflex loops, using analog or spiking\\r\\nmodels. Central pattern generators are networks in the spinal cord that produce rhythmic outputs (for\\r\\nlocomotion, breathing, etc.) without needing continuous input from the brain. Roboticists have\\r\\nimplemented CPG models that run on neuromorphic hardware to control legged robots, producing\\r\\nadaptive gait patterns that are robust to perturbations \u2013 effectively giving the robot a \u201cspinal cord\u201d that\\r\\ntakes care of low-level leg coordination. For instance, a neuromorphic CPG controller can generate\\r\\nwalking or crawling rhythms and modulate them based on feedback spikes from sensors (like load\\r\\nsensors on the feet) . This mirrors how, in animals, the spinal CPG can adjust step timing or force in\\r\\nresponse to a sudden change (like stepping on a rock) quickly and without central intervention.\\r\\nSimilarly, neuromorphic implementations of reflex arcs have been explored: when a sensor spike\\r\\nindicating, say, an excessive joint stretch arrives, a spiking interneuron can trigger a motor neuron to\\r\\ncounteract it \u2013 just like a biological stretch reflex. Because these computations are local and eventdriven, the latency can be very low (a few milliseconds) and the computing load minimal.\\r\\n\\r\\nAdvantages of Neuromorphic Control: Neuromorphic and bio-inspired control strategies offer several\\r\\npotential advantages. They can be high-speed and low-latency (spikes propagate quickly through a\\r\\nhardware neural net, and many operations happen in parallel). They are inherently robust to noise and\\r\\ndamage, as neural networks can often function even if some neurons/spikes are lost, much like our\\r\\nbrains gracefully handle partial injuries or noise. They also promise lower power consumption \u2013 brains\\r\\nonly use ~20 W of power, and neuromorphic chips, by avoiding the overhead of clocked synchronous\\r\\nlogic, can be extremely energy efficient for certain tasks. Neuromorphic controllers can learn and adapt\\r\\non the fly using mechanisms analogous to synaptic plasticity. For example, a robot might use spiketiming-dependent plasticity (STDP) rules to \u201clearn\u201d how to improve its balance or adapt its gait,\\r\\ngradually tuning the synaptic weights in its network rather than requiring a programmer to tweak\\r\\ncontrol gains. This learning can even be done on the hardware in real time, leading to lifelong\\r\\nadaptation, something very natural for animals but difficult for conventional control systems.\\r\\n\\r\\nIn essence, neuromorphic approaches are bringing the architecture of robotic control closer to that\\r\\nof a biological nervous system. While a traditional robot might have a centralized processor running a\\r\\ncontrol loop at 1 kHz, a neuromorphic-controlled robot could have thousands of \u201cneurons\u201d monitoring\\r\\nsensors and driving motors asynchronously. Such a robot might exhibit more reflexive, fluid motions\\r\\nand be better at reacting to unforeseen events, since its control is distributed and event-driven rather\\r\\nthan strictly periodic. We are already seeing demonstrations of neuromorphic vision sensors (event\\r\\ncameras) combined with spiking neural nets to do things like ultra-fast obstacle avoidance in drones,\\r\\nwhere the entire perception-action loop is conducted with spiking hardware in microseconds. As these\\r\\ntechnologies mature, we expect humanoid robots to gain more brain-like qualities: low-power\\r\\nsensing, fast reflexes, and adaptive learning capabilities that traditional control architectures\\r\\nstruggle to realize.\\r\\n\\r\\n## Conclusion\\r\\nEngineering design for humanoid robots is increasingly informed by insights from human physiology.\\r\\nThe human body achieves a remarkable balancing act: it uses slow, low-frequency neural signals yet\\r\\nmaintains graceful, stable, and efficient movement. This is possible because of evolutionary adaptations\\r\\n\u2013 elasticity in tissues, decentralized reflex loops, and a robust neural architecture \u2013 that work in\\r\\nharmony with the body\u2019s mechanics. In contrast, early robots were built rigid and controlled like\\r\\nfactories, needing high-frequency, high-gain feedback to even approximate stability. Today, the frontier\\r\\nof humanoid robotics is all about closing the gap with biology: adding compliance and soft\\r\\nmaterials to mimic muscle and tendon behavior, and adopting neuromorphic, brain-inspired control to\\r\\nattain the efficiency and adaptability of neural systems.\\r\\nIn summary, a technical comparison reveals that for every clever strategy in human biomechanics or\\r\\nneural control, roboticists are developing an analogue. Low-frequency neural control is compensated by\\r\\nhigh intrinsic stability \u2013 so robots add springs or dampers to gain physical stability. Biological\\r\\nproprioception and reflexes are swift and automatic \u2013 so robots incorporate better sensors and local\\r\\nfeedback loops (even on-chip neural networks) to respond in kind. Muscles are soft, efficient, and multifunctional \u2013 so artificial muscles and compliant actuators are being created to replace or augment\\r\\nelectric motors. The human nervous system computes with spikes and adapts continuously \u2013 so\\r\\nneuromorphic processors and learning algorithms are being employed to give robots a more brain-like\\r\\nedge. \\r\\n\\r\\nUltimately, **the synergy of mechanical design and control architecture** in the human body sets a high\\r\\nbar that robotics is steadily striving toward. The comparative study of human vs. humanoid design not\\r\\nonly highlights our current technological limitations but also lights the way forward: future generalpurpose humanoid robots will likely blend soft, muscle-like actuators with brain-inspired controllers,\\r\\nachieving a level of agility, efficiency, and resilience that today remains unique to biological organisms\\r\\n. Each new development \u2013 be it a better artificial muscle or a smarter neural network controller \u2013\\r\\nis a step toward robots that move and respond with the fluidity of human beings, ultimately enabling\\r\\nsafer and more capable interactions in the real world.\\r\\n\\r\\n## Sources:\\r\\n- Bertec Corp., \u201cBody-Brain Connection Part II: Automatic Postural Control\u201d \u2013 reflex latency data .\\r\\n- Kristinn Heinrichs, et al., on human postural reflex loops (2023) .\\r\\n- Frontiers in Robotics & AI (Ashtiani et al., 2021) \u2013 comparison of animal vs. robot sensorimotor\\r\\n- delays and role of compliance .\\r\\n- Arthur Kuo et al., PLOS Comp. Biol. (2021) \u2013 on tendons saving energy in running .\\r\\n- ASME IDETC (2013) \u2013 on compliance improving efficiency, agility in animals vs robots .\\r\\n- Araz et al., Front. Bioeng. Biotech. (2023) \u2013 muscle preflex response in milliseconds .\\r\\n- Nature Biomedical Eng. (2021) \u2013 open-source bionic leg with FOC, encoders, IMUs .\\r\\n- StackExchange Robotics \u2013 practical note on 1 kHz control for 500 Hz dynamics .\\r\\n- MDPI Polymers (2025) \u2013 review of electroactive polymer muscles .\\r\\n- Daerden & Lefeber (2002) \u2013 overview of pneumatic artificial muscles .\\r\\n- Nature Communications (Buchner et al., 2024) \u2013 electrohydraulic artificial muscle leg performance .\\r\\n- Frontiers in Neurorobotics (Bing et al., 2018) \u2013 survey of spiking neural network control and neuromorphic advantages ."},{"id":"rtere","metadata":{"permalink":"/doc/blog/rtere","editUrl":"https://github.com/pleycothh/DRONNEX/tree/main/docs-site/blog/2025-05-20-ros2-learning-guid.md","source":"@site/blog/2025-05-20-ros2-learning-guid.md","title":"Roadmap to Embedded & Robotics Engineering","description":"Month 1: ROS 2 Foundations & Microcontroller Basics (Weeks 1\u20134)","date":"2025-05-20T00:00:00.000Z","tags":[{"inline":true,"label":"chatgpt","permalink":"/doc/blog/tags/chatgpt"}],"readingTime":22.67,"hasTruncateMarker":true,"authors":[{"name":"Ben Li","title":"Software engineer","url":"https://github.com/pleycothh","page":{"permalink":"/doc/blog/authors/ben"},"socials":{"x":"https://x.com/dronnex","github":"https://github.com/pleycothh"},"imageURL":"https://github.com/pleycothh.png","key":"ben"},{"name":"Chat GPT","title":"AI Assistant","url":"https://openai.com","page":{"permalink":"/doc/blog/authors/all-sebastien-lorber-articles"},"socials":{"x":"https://x.com/sebastienlorber","linkedin":"https://www.linkedin.com/in/sebastienlorber/","github":"https://github.com/slorber","newsletter":"https://thisweekinreact.com"},"imageURL":"https://openai.com/favicon.ico","key":"chatgpt"}],"frontMatter":{"slug":"rtere","title":"Roadmap to Embedded & Robotics Engineering","authors":["ben","chatgpt"],"tags":["chatgpt"]},"unlisted":false,"prevItem":{"title":"Human Body vs. Humanoid Robot Design","permalink":"/doc/blog/hbhrd"},"nextItem":{"title":"Companion Computers in Drones","permalink":"/doc/blog/ccid"}},"content":"## Month 1: ROS 2 Foundations & Microcontroller Basics (Weeks 1\u20134)\\r\\n- Week 1 \u2013 ROS 2 Setup and Fundamentals: Install ROS 2 (latest LTS) and set up your Linux development environment. Review ROS 2 architecture (nodes, topics, services, parameters) and command-line tools . Complete introductory tutorials (talker/listener, turtlesim) to build basic ROS 2 skills . Milestone: You can run a simple ROS 2 talker/listener demo and understand the core concepts.\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n- Week 2 \u2013 ROS 2 Nodes & Packages: Create your first custom ROS 2 package. Write a C++ node that publishes a dummy sensor message (e.g. a counter or random number) and a Python node that subscribes to it. Practice building with colcon and using rviz or rqt to visualize topics. Explore ROS 2 client libraries and differences from ROS 1 (if applicable). Milestone: A custom ROS 2 package with C++ and Python nodes communicating successfully.\\r\\n\\r\\n\\r\\n- Week 3 \u2013 Microcontroller Environment Setup: Set up development tools for STM32H753ZI (e.g. STM32CubeIDE or PlatformIO). Blink an LED and read a button input on the STM32 to verify the toolchain. If using an RTOS (FreeRTOS), create a basic task for blinking LED. Repeat a similar \u201cblinky\u201d test on the ESP32-S3 (using ESP-IDF or Arduino framework) to familiarize yourself with both controllers. Milestone: Both STM32 and ESP32 environments are verified by running basic firmware (LED blink, serial hello-world).\\r\\n\\r\\n\\r\\n- Week 4 \u2013 C++ on MCU & Sensor I/O: Write a simple C++ program on the STM32 to interface with hardware peripherals. For example, read an IMU sensor over I\xb2C (or SPI) and output the readings over UART. Implement a basic PID loop in the microcontroller: e.g., use a potentiometer as input and a LED brightness (PWM) as output to simulate controlling a \u201cprocess\u201d to a setpoint, just to practice control logic timing. Keep code efficient and use interrupts or RTOS for timing if needed. Milestone: You can read a sensor and actuate an output with a control loop on the STM32, and understand microcontroller C++ basics (registers, HAL libraries, interrupts).\\r\\n\\r\\n\\r\\nCheckpoint (End of Month 1): You have a solid grasp of ROS 2 basics and a working embedded dev setup. You can develop simple ROS 2 nodes  and microcontroller firmware in C++, setting the stage for integration.\\r\\n\\r\\n## Month 2: ROS 2 \u2194 Microcontroller Integration (Weeks 5\u20138)\\r\\n- Week 5 \u2013 Intro to micro-ROS: Learn how micro-ROS brings ROS 2 into microcontrollers . Read the micro-ROS docs about the client-agent architecture (ROS 2 nodes on MCU acting as clients to an agent on PC). Follow a tutorial to set up micro-ROS on the STM32 (e.g. using FreeRTOS + micro-ROS) . Start with a basic micro-ROS publisher on the STM32 that sends IMU or dummy data to a PC ROS 2 node (the micro-ROS agent). Milestone: STM32 board successfully publishes a ROS 2 topic via micro-ROS (e.g. a sensor_msgs/Imu or a custom message) and you can echo it on the PC.\\r\\n\\r\\n\\r\\n- Week 6 \u2013 Two-Way Communication: Extend to a subscriber on the microcontroller. For instance, a PC ROS 2 node could publish a command (like an LED toggle or motor speed), and the STM32 (or ESP32) running micro-ROS subscribes and acts on it. Ensure real-time behavior by running the micro-ROS node in its own task (if using RTOS). This solidifies understanding of DDS-XRCE communication. Experiment with both STM32 and ESP32-S3 for comparison \u2013 ESP32 has Wi-Fi, so you might try a UDP transport for micro-ROS. Milestone: Achieve bi-directional ROS 2 comms \u2013 send a command from a ROS 2 terminal to toggle an MCU LED or move a servo, and confirm feedback from MCU sensors in ROS 2.\\r\\n\\r\\n\\r\\n- Week 7 \u2013 Motor Control 101: Begin controlling motors with the microcontroller. Start simple: a DC motor or servo motor. If using a DC motor with encoder, set up a timer interrupt to read the encoder and a PWM output to drive the motor driver. Implement a basic velocity PID controller on the MCU to maintain a set speed. If using a servo (which has internal control), practice commanding positions via PWM. This will prepare you for Field-Oriented Control later by understanding control loops. Integrate ROS 2 by sending motor commands from a PC node to the MCU. Milestone: One motor can be controlled in closed-loop (speed or position) by the microcontroller, with commands and telemetry accessible in ROS 2 (e.g., you can set a speed via a ROS topic and the motor maintains it, reporting encoder ticks).\\r\\n\\r\\n\\r\\n- Week 8 \u2013 Real-Time Control & Sensing: Refine the motor control with real-time considerations. Use FreeRTOS on STM32 to run the control loop at a fixed frequency (e.g. 1 kHz for motor PID) in one thread, and ROS communication in another. Add the IMU into the loop: mount the IMU on the motor or a platform and try a simple balancing experiment. For example, if you mounted a rod or platform on the motor, attempt to keep it upright (inverted pendulum style) using the motor \u2013 a classic control problem to tune your PID. This may be challenging, but even partial success teaches a lot. Meanwhile, stream IMU and encoder data to ROS 2 for debugging (e.g. plot angles in rqt_plot). Milestone: Hard real-time control is running on the MCU, and you\u2019re comfortable with multitasking on FreeRTOS and tuning PIDs. You have possibly achieved a simple balance control or at least a well-tuned motor controller.\\r\\n\\r\\n\\r\\nCheckpoint (End of Month 2): Your microcontroller acts as a ROS 2 node , bridging the embedded and ROS worlds. You\u2019ve controlled a motor with feedback, a critical step for any robot. You\u2019re ready to build actual robotic hardware.\\r\\n\\r\\n## Month 3: Building a Mobile Robot & Advanced Control (Weeks 9\u201312)\\r\\n- Week 9 \u2013 Build a Basic Robot Platform: Using your hardware skills, construct a simple ground robot for testing control algorithms. One suggested project is a self-balancing two-wheeled robot (like a mini Segway): it uses two motors and an IMU \u2013 leveraging your Week 8 progress. Alternatively, build a single-leg test rig: a 2-DOF leg (hip and knee) with encoders. The balancing robot is easier as it\u2019s a well-known project, so consider starting there. Assemble the chassis, mount the STM32 (or ESP32) as the controller, and wire up the IMU and motors (and motor drivers or ESCs). Milestone: Mechanical build completed for a small robot (balancing bot or leg prototype), and all sensors/motors are interfaced with the microcontroller.\\r\\n\\r\\n\\r\\n- Week 10 \u2013 Balance Control with Feedback: Program the balancing robot to achieve upright balance. This involves using the IMU to sense tilt and adjusting motor speeds to correct it (a classic inverted pendulum control problem). Use a PID or even more advanced controllers if needed. You might spend a lot of time tuning gains here \u2013 that\u2019s normal. Integrate ROS 2 for monitoring and high-level control: for example, create a ROS 2 node that sends a target tilt angle or velocity to the robot. Achieve remote control: send a forward command, and the robot leans forward to roll that direction while self-balancing. If working with the single leg instead, focus on inverse kinematics \u2013 move the leg end-point in a straight line by coordinating hip and knee joints, using kinematic equations. Milestone: The balancing bot can stand upright for an extended period and respond to simple ROS 2 commands (e.g., move forward/backward while balancing), or the single leg can precisely move to commanded positions, showing mastery of kinematics and feedback control.\\r\\n\\r\\n\\r\\n- Week 11 \u2013 Toward a Quadruped \u2013 Actuators Setup: Begin planning your quadruped robot (robot dog). Determine the actuator setup: for high performance, use brushless BLDC motors with encoders on each joint to enable torque control; or initially, use simpler servos to prototype leg motion. Given your goal of 4\u20136 Field-Oriented Control (FOC) motors, you may choose to use 8\u201312 motors (3 per leg for a true quadruped). If you have brushless gimbal motors and encoders (common in drone gimbals), try the SimpleFOC library or reference designs to control them on the STM32. Each motor will run FOC for smooth torque control \u2013 critical for legged robots to act compliantly. Milestone: One joint module is set up with a BLDC motor + encoder under FOC control. For example, you can command a joint to hold a position or apply a certain torque. (FOC enables \u201csuperior torque control\u201d for brushless motors , which is why we pursue it.)\\r\\n\\r\\n\\r\\n- Week 12 \u2013 Quadruped Mechanical Assembly: Design or obtain a frame for your quadruped. You might 3D print parts or use an open-source design like Solo 8 as inspiration (an open-source torque-controlled quadruped) . Assemble the quadruped\u2019s legs and body, starting with a minimal setup (even two legs to form a biped can be a start, then add others). Mount motors, encoders, and necessary microcontrollers on the robot. If using multiple MCUs (one per leg or per few motors), set up a communication bus (CAN bus or serial) between them, or have each run micro-ROS and communicate via the ROS 2 agent on a central computer. Milestone: The physical quadruped robot is assembled with all actuators in place. When powered, you can individually command each joint via your microcontroller code (e.g., each leg can be moved through its range under control).\\r\\n\\r\\n\\r\\nCheckpoint (End of Month 3): You have a working small robot (balancer or leg) and a proto-quadruped assembled. Crucially, you\u2019ve implemented FOC motor control and multi-motor coordination on microcontrollers, a big step toward dynamic leg control . At this stage, you\u2019re comfortable with sensors, actuators, and ROS 2 integration on real hardware.\\r\\n## Month 4: Gait Development & ROS 2 Integration (Weeks 13\u201316)\\r\\nSolo 8 open-source quadruped performing dynamic motion. By this phase, you will have a basic quadruped robot and start developing locomotion gaits, leveraging torque-controlled joints similar to those in Solo 8 .\\r\\n- Week 13 \u2013 Basic Gait Implementation: With the quadruped\u2019s hardware ready, program a simple walking gait. Start with a crawl gait (one leg moves at a time, keeping 3 on ground for stability). Write a schedule for leg swings and use your motor controllers to move joints accordingly. Initially, use position control (move joints to predefined angles for each phase of gait). Test on the real robot slowly \u2013 you might need to support it to prevent falls during tuning. Use ROS 2 to send high-level commands like \u201cwalk forward\u201d or \u201cturn\u201d, which your MCU code translates into gait patterns. Milestone: The quadruped takes its first steps, albeit slowly and quasi-statically. You can command it via ROS 2 to walk forward a short distance on flat ground.\\r\\n\\r\\n\\r\\n- Week 14 \u2013 Feedback and Stability: Incorporate feedback to improve stability. Use the IMU on the robot body to detect if it\u2019s tilting too far. Implement a basic stabilization: e.g., adjust leg positions or timings if pitch or roll exceeds a threshold (a simple form of balance control). You could also add foot sensors (switches or pressure sensors) to detect contact \u2013 ensuring a leg has touched ground before lifting another. Start experimenting with more dynamic gaits like trot (moving diagonal legs together) once you have some feedback control. In trot, the robot will have moments with only two legs on ground, so your IMU-based balancing becomes crucial. Milestone: The robot can trot a few steps without falling, using IMU feedback to self-correct minor disturbances.\\r\\n\\r\\n\\r\\n- Week 15 \u2013 ROS 2 Perception Integration: Mount a camera (e.g. an ESP32-CAM or USB camera feeding into a PC) on the robot. While locomotion is still under refinement, begin integrating perception to move toward an \u201cAI-capable\u201d robot. For instance, use the camera feed in a ROS 2 node running on a companion computer (like a laptop or Raspberry Pi) and perform a simple vision task (like detecting a colored ball or AprilTag). This week, keep the task simple: e.g., the robot looks around (turns in place) until the camera sees a target, then you stop. This sets up the pipeline for more advanced uses where vision could inform locomotion (obstacle avoidance, path planning). Milestone: Camera data is accessible in ROS 2, and you have a basic vision-in-the-loop demo (even if just teleop: you can drive the robot via a gamepad using camera view).\\r\\n\\r\\n\\r\\n- Week 16 \u2013 Autonomous Control Mode: Combine your progress to enable a simple autonomous behavior. For example, implement a \u201cgo to goal\u201d: place a colored object and have the robot walk towards it. This involves tying vision to locomotion \u2013 e.g., a ROS 2 node processes camera frames (perhaps using your YOLO knowledge to detect the object), computes a direction, and sends a velocity command to the quadruped. You may use ROS 2 Navigation stack\u2019s planner for high-level path planning on a map, but since legged locomotion is complex, you can also do a simpler approach: if object is seen, turn towards it and move forward until a certain distance. Ensure your locomotion controller can handle commands for direction and speed (you might implement a rudimentary teleop interface that translates desired forward/turn velocities into gait parameters). Milestone: The robot demonstrates an autonomous task on flat terrain \u2013 e.g., it can walk toward a visual target or follow a simple preset route without constant human micromanagement.\\r\\n\\r\\n\\r\\nCheckpoint (End of Month 4): Your quadruped robot is walking under its own control. It handles basic gaits with sensor feedback and can be controlled via ROS 2 commands. You\u2019ve also integrated a camera sensor, laying groundwork for AI and autonomy. At this point, your skill set spans embedded motor control, real-time feedback loops, ROS 2 middleware, and even some high-level perception \u2013 a huge stride toward advanced robotics engineering.\\r\\n## Month 5: Reinforcement Learning for Locomotion (Weeks 17\u201320)\\r\\n- Week 17 \u2013 Simulation Setup for RL: Transition into the learning phase to push the robot\u2019s capabilities. Set up a physics simulation of your quadruped (using Gazebo Ignition, PyBullet, or Isaac Gym). Import your robot\u2019s URDF model into the simulator; simplify if needed (e.g., use a simplified collision model, approximate motor behavior). Verify that the simulated robot can stand and manually walk with the same gait commands as the real robot. Simulation allows safe experimentation with advanced control. Milestone: You have a working simulation environment for the quadruped, and you can teleop the simulated robot in ROS 2, confirming that the model behaves reasonably like the real hardware.\\r\\n\\r\\n\\r\\n- Week 18 \u2013 RL Problem Formulation: Define the reinforcement learning problem for locomotion. Decide on the state representation (e.g., IMU angles, body orientation, joint angles, joint velocities, foot contact indicators) and the action space (e.g., target joint positions or torques). A common approach is to have the policy output desired joint angles or torque offsets for each motor at each control step . Design a reward function that encourages forward walking while penalizing falls, high tilt, or unstable motions . For example, reward forward velocity and penalize deviation from upright posture. Use an existing RL library (Stable Baselines3 with PPO is a good choice, as Proximal Policy Optimization (PPO) is popular for legged robots ). Set up the training code, connecting it to the simulation (maybe through ROS topics or a direct physics engine API for speed). Milestone: Your RL training environment is ready \u2013 you can run episodes in simulation where the quadruped starts, attempts to walk, and gets rewards. You\u2019ve essentially defined \u201cwhat\u201d the robot should learn (policy maps observations to motor commands) .\\r\\n\\r\\n\\r\\n- Week 19 \u2013 Train Locomotion Policy: Run training for your RL policy in simulation. This may take many iterations, so use your powerful PC or cloud if available, and consider parallel simulations for efficiency . Monitor training progress: the robot should start off random, but over time learn to move forward without falling. Tweak hyperparameters or reward terms if it gets stuck (for example, add a small reward for moving any leg to encourage exploration). Implement domain randomization during training \u2013 randomize factors like friction, motor strength, or sensor noise . This helps the learned policy be robust for the real world (improving sim-to-real transfer). By the end of the week, you might have a policy that can make the simulated robot walk with decent stability. Milestone: A trained RL policy (neural network controller) that enables the simulated quadruped to walk forward (and maybe turn) consistently. You should also document results \u2013 e.g., the policy achieves a certain average reward or distance without falling, in simulation.\\r\\n\\r\\n\\r\\n- Week 20 \u2013 Real-World Deployment Prep: Prepare to deploy the learned policy on the real robot (this is where things get exciting!). Ensure you can run the neural network in ROS 2 \u2013 likely on a companion computer (Raspberry Pi, NVIDIA Jetson, or a laptop) because a STM32/ESP32 is not powerful enough for runtime inference of a large network. You might use TensorFlow Lite or ONNX runtime on the companion computer. Connect the policy output to your robot\u2019s ROS control interface: for instance, the policy outputs desired joint angles at 20 Hz, and you have a ROS 2 node sending those to the microcontrollers which execute position control. Plan safety measures: start with lower torques/speeds, have an e-stop ready. Milestone: The RL policy is integrated into the ROS 2 control stack, ready to drive the real robot in testing. All components (policy inference node, micro-ROS motor controllers, etc.) are communicating properly.\\r\\n\\r\\n\\r\\nCheckpoint (End of Month 5): You have gained experience with deep reinforcement learning for locomotion. In simulation, your robot learned to walk via trial and error \u2013 a cutting-edge capability  . You\u2019ve set the stage to attempt sim-to-real transfer of this learned behavior onto your actual quadruped.\\r\\n## Month 6: Sim-to-Real Transfer & Advanced Control (Weeks 21\u201324)\\r\\n- Week 21 \u2013 Deploy RL Policy on Robot: Time to see your robot walk under AI control. Run the trained policy on the real quadruped. Initially, keep the robot on safety harness or support to prevent hard falls. It will likely stumble as the dynamics differ from simulation. Observe its behavior: does it move its legs in a coordinated way? If it falls quickly, you may need to refine the policy or increase randomization. You could also try online learning: have the robot attempt to walk and, if safe, let it continue learning on hardware in small bursts (though this is risky and requires careful reward handling). More practically, adjust the policy using slight system identification tweaks \u2013 e.g., scale outputs or add a secondary balance controller (PID using IMU) that corrects small errors on top of the policy. Milestone: The quadruped can take a few steps in reality with the RL policy. It might not be perfect, but even a few seconds of autonomous learned walking is a big achievement in sim-to-real reinforcement learning.\\r\\n\\r\\n\\r\\n- Week 22 \u2013 Iterate and Improve: Likely, you will iterate between simulation and real testing. Analyze failure cases on the real robot: for example, maybe the policy wasn\u2019t exposed to enough friction variation and the feet slip, or it oscillates. Update your simulation environment to include those cases (e.g., add slight delays or sensor noise, or use a higher fidelity physics engine if needed). Train a new policy or fine-tune the existing one with domain randomization more aggressively to bridge the gap . Another approach is Residual Learning: keep a classical controller (from Month 4) and have the RL policy learn a residual correction on top of it \u2013 this can ease the learning burden. Continue this sim-to-real refinement cycle. Milestone: A revised policy that significantly improves real-world performance \u2013 e.g., the robot can walk across your living room floor reliably for a minute or more without falling. Each iteration reduces the gap between simulated and real behavior.\\r\\n\\r\\n\\r\\n- Week 23 \u2013 Advanced Locomotion Skills: With a working walking policy, you can push further. For instance, train (or program) the robot to turn and maneuver to follow a path. Extend the observation space to include a target direction or point, so the policy can guide the robot toward it. You could also work on speed control \u2013 train the policy to walk at different commanded speeds (include the desired speed as an input). Meanwhile, test the robot on slightly uneven terrain (a foam mat or a small ramp) to see how it adapts. This will highlight the robustness of your controller. You might incorporate Rapid Motor Adaptation (RMA) techniques or other recent research that allow on-the-fly adaptation to terrain by estimating ground parameters \u2013 cutting-edge stuff, but worth reading up on if interested . Milestone: The robot demonstrates turning and variable-speed walking, possibly using the learned policy with additional inputs. It can handle mild terrain variations or recover from small pushes, showing a degree of robustness and adaptability.\\r\\n\\r\\n\\r\\n- Week 24 \u2013 Evaluation and Demo: Consolidate everything you\u2019ve achieved in a final demonstration. For example, set up an obstacle course or a set of tasks: the robot must walk to a location, avoid an obstacle (you can use simple logic or a separate planner for avoidance), and maybe perform a trick like standing up after a fall or climbing a small step (if hardware permits). Use ROS 2 bags to record data during these tests for later analysis of performance (e.g., analyze how stable the roll/pitch were, how often it had to stop to regain balance, etc.). Compare the performance of your RL controller to the earlier hand-crafted gait: is it smoother or more efficient? Often RL can discover gait patterns that are non-intuitive but effective . Document these findings. Milestone: A complete demonstration of your robot dog\u2019s capabilities is recorded \u2013 you\u2019ve essentially built and trained a legged robot that can perceive and navigate simple scenarios on its own.\\r\\n\\r\\n\\r\\nCheckpoint (End of Month 6): You have brought state-of-the-art techniques to fruition: your robot learned to walk through reinforcement learning and operates in the real world. This experience is directly in line with modern legged robotics research, where learning-based controllers are unlocking agility previously seen only in very expensive systems . You\u2019re now truly functioning at the level of a professional embedded/robotics engineer pushing the boundaries of locomotion.\\r\\n## Months 7\u20139: Mastery and Expansion (Weeks 25\u201336)\\r\\n- Weeks 25\u201328 \u2013 Deep Dive into Embedded Systems: Use this period to reinforce low-level expertise that underpins advanced robots. Revisit your STM32 motor control firmware and attempt to optimize it. For example, implement Field Oriented Control from scratch (without relying on SimpleFOC) to deepen your understanding of the math (Park/Clarke transforms, PID for Id/Iq currents). Test the custom FOC on one joint and compare torque control performance to the library version. Additionally, explore advanced microcontroller features: use DMA to handle sensor data without CPU, implement CAN bus communications between multiple controllers for coordinating legs (many quadrupeds use one MCU per leg or per joint, networked over CAN for synchronization ). Also experiment with FreeRTOS features like message queues or timers to make your firmware more robust. This deepens your embedded systems mastery, ensuring you can build reliable real-time systems.\\r\\n\\r\\n\\r\\n- Weeks 29\u201332 \u2013 Bipedal Robotics (Stretch Goal): Tackle the next challenge: bipedal robots. Repurpose your quadruped by using only two legs (or design a new small biped). Start with getting a biped to stand using feedback control (this is essentially two balancing robots stacked). Implement a balance controller using both feet (shifting weight). Reuse your IMU and foot sensors for detecting when a foot is on the ground. Gradually work on walking \u2013 start with side-to-side weight shifting, then stepping one foot forward (perhaps with a boom or support to catch it). This project will test everything you\u2019ve learned: real-time control, feedback loops, and possibly RL (bipedal walking is even harder than quadrupedal). Even if full dynamic walking is not achieved in this short time, the attempt will massively increase your understanding of humanoid balance and control algorithms (inverted pendulum models, zero-moment point, etc.). By the end of Week 32, you might have a biped that can take a step or at least stand and recover from a push for a few seconds. This is excellent preparation for eventually tackling drones again, since balancing a biped has parallels to stabilizing a drone (both are complex dynamic systems).\\r\\n\\r\\n\\r\\n- Weeks 33\u201336 \u2013 Final Projects and Transition to Drones: In the final stretch, consolidate your knowledge by undertaking a capstone project. One idea: integrate an arm or manipulator onto your quadruped (or use a 6th motor on one of the legs as a small grabbing arm) and program a manipulation task (like pushing a door or picking up an object). This adds another layer (manipulator kinematics and control) on top of locomotion \u2013 reflecting real-world robotics problems where a robot needs to both move and interact with objects. Another idea is to revisit drones, now armed with ROS 2 and advanced control skills: build a drone that uses ROS 2 flight controllers (PX4 has ROS 2 support) and maybe implement an autonomous vision-based landing or navigation system to tie in AI skills. Allocate time to document your projects, polish your code, and perhaps contribute to open-source projects or publish a blog/video about your robot dog\u2019s journey. This will not only solidify what you learned by teaching others, but also showcase your skills to potential employers or collaborators. Milestone (End of Month 9): You have one or two impressive portfolio projects (e.g., a walking robot dog with an arm, or an autonomous drone mission) demonstrating broad mastery. You can confidently call yourself an Embedded & Robotics Engineer with expertise in ROS 2, microcontrollers, motor control, and machine learning for robots.\\r\\n\\r\\n\\r\\n## Conclusion and Next Steps\\r\\nBy following this 9-month roadmap, dedicating ~3 hours on weekdays and heavy practice on weekends, you\u2019ve progressed from a software developer to a hands-on roboticist. Along the way, you focused on ROS 2 and relevant tools (no time wasted on unrelated simulations), dove deep into STM32 and ESP32 microcontroller programming (timers, RTOS, drivers, etc.), and implemented real-time control for multiple FOC motors with sensor feedback. You built a legged robot from scratch and leveraged modern reinforcement learning to achieve locomotion that would be very difficult to hand-engineer, aligning with cutting-edge developments in the field . Each phase included 1\u20132 week projects and milestones, ensuring steady progress and a sense of achievement.\\r\\nGoing forward, you can extend these foundations to more complex systems (like full humanoid robots or aerial vehicles). The focus on ROS 2 and microcontrollers will serve you well in any advanced robotics project \u2013 including future drone development when you circle back to it with your new skill set. By continuously challenging yourself with projects (as we structured with regular checkpoints), you\u2019ve built not only knowledge but also a robust engineering workflow for solving robotics problems. Congratulations on your journey to becoming a professional embedded/robotics engineer, and best of luck building the next robot dog or whatever innovative robot comes next!\\r\\n\\r\\nSources:\\r\\n- ROS 2 Tutorials \u2013 ROS 2 official documentation (Humble)\\r\\n- micro-ROS Overview \u2013 ROS 2 for Microcontrollers\\r\\n- FOC for Torque Control \u2013 Brushless motor controller for legged robots\\r\\n- Solo 8 Open-Source Quadruped \u2013 NYU & MPI torque-controlled robot\\r\\n- RL Locomotion Training \u2013 Federico Sarrocco, \u201cQuadrupeds Learning to Walk\u201d\\r\\n- Robot Learning Applications \u2013 Solo 8 press release (NYU)"},{"id":"ccid","metadata":{"permalink":"/doc/blog/ccid","editUrl":"https://github.com/pleycothh/DRONNEX/tree/main/docs-site/blog/2025-04-20-companion-computer.md","source":"@site/blog/2025-04-20-companion-computer.md","title":"Companion Computers in Drones","description":"Introduction","date":"2025-04-20T00:00:00.000Z","tags":[{"inline":true,"label":"chatgpt","permalink":"/doc/blog/tags/chatgpt"}],"readingTime":40.43,"hasTruncateMarker":true,"authors":[{"name":"Ben Li","title":"Software engineer","url":"https://github.com/pleycothh","page":{"permalink":"/doc/blog/authors/ben"},"socials":{"x":"https://x.com/dronnex","github":"https://github.com/pleycothh"},"imageURL":"https://github.com/pleycothh.png","key":"ben"},{"name":"Chat GPT","title":"AI Assistant","url":"https://openai.com","page":{"permalink":"/doc/blog/authors/all-sebastien-lorber-articles"},"socials":{"x":"https://x.com/sebastienlorber","linkedin":"https://www.linkedin.com/in/sebastienlorber/","github":"https://github.com/slorber","newsletter":"https://thisweekinreact.com"},"imageURL":"https://openai.com/favicon.ico","key":"chatgpt"}],"frontMatter":{"slug":"ccid","title":"Companion Computers in Drones","authors":["ben","chatgpt"],"tags":["chatgpt"]},"unlisted":false,"prevItem":{"title":"Roadmap to Embedded & Robotics Engineering","permalink":"/doc/blog/rtere"}},"content":"## Introduction\\r\\nDrones across various industries are increasingly equipped with onboard computers (also called companion computers) that serve as the \u201cbrain\u201d alongside the flight controller. These onboard systems process sensor data (especially camera feeds) in real time to enable high-level functions such as autonomous navigation, obstacle avoidance, and AI-based analysis (Top 5 Companion Computers for UAVs | ModalAI, Inc.). \\r\\nTraditionally, a drone\u2019s flight controller handled basic stabilization and GPS waypoint following, but modern use cases demand greater autonomy and on-site intelligence. Advances in compact, powerful processors (GPUs, NPUs, etc.) now allow drones to perform complex tasks like object detection, tracking, and mapping locally at the edge, which was not feasible just a few years ago (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). This shift to onboard edge computing yields low latency decisions and reduces reliance on constant communication links, an important benefit since drones often operate beyond reliable network coverage (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs).\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n**Onboard Hardware Choices:** The choice of onboard computer depends on the required complexity and performance. NVIDIA\u2019s Jetson family (e.g. Nano, TX2, Xavier, Orin) is popular for drone AI workloads, featuring CPU/GPU architectures that can run neural networks for vision tasks in real time (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). These deliver high TOPS (trillions of operations per second) for deep learning, enabling on-device inference for applications like image classification, object detection (e.g. using YOLO models), and SLAM. In contrast, hobbyist boards like the Raspberry Pi or Radxa single-board computers offer a low-cost, lightweight platform suitable for less intensive tasks or early prototyping (Top 5 Companion Computers for UAVs | ModalAI, Inc.) (Top 5 Companion Computers for UAVs | ModalAI, Inc.). However, such boards lack dedicated neural accelerators \u2013 heavy computer vision models run slowly on them, incurring latency for deep learning frameworks (Top 5 Companion Computers for UAVs | ModalAI, Inc.). To bridge this gap, developers sometimes augment lower-power boards with USB AI accelerators (e.g. Intel Neural Compute Stick or Google Coral) to offload neural network inference. In practice, real-world drone deployments span this spectrum: from simple microcontrollers paired with a Raspberry Pi for basic image capture, up to powerful AI mission computers like Jetson Xavier or Qualcomm Snapdragon Flight in fully autonomous drones. The following sections provide a comprehensive overview of how onboard computers are used in major industries \u2013 agriculture, security, delivery, mapping/inspection, and other industrial applications \u2013 detailing the typical hardware, vision vs. non-vision workloads, and why on-board computing is essential in each context.\\r\\n## Agriculture (Precision Farming)\\r\\nAgriculture was an early adopter of drone technology for monitoring crops and automating fieldwork. Today, many precision farming drones carry onboard computers to analyze sensor data and make split-second decisions in the field. In crop scouting and surveying, a drone might capture multispectral or RGB images of fields and use onboard processing to stitch images or calculate vegetation indices on the fly. More advanced systems perform real-time computer vision: for example, identifying crop stress, detecting diseased plants, or locating weeds among crops during the flight. UAVs are now widely used in precision agriculture for crop monitoring and targeted spraying, which improves farming efficiency and reduces environmental impact (Frontiers | Ag-YOLO: A Real-Time Low-Cost Detector for Precise Spraying With Case Study of Palms). An onboard computer can directly interpret camera feeds to differentiate healthy crops from weeds or pests, enabling immediate action such as precision spraying of agrochemicals only where needed. This is a leap from the traditional method of capturing images for later analysis \u2013 instead, the \u201csee-and-spray\u201d drone can act during the same flight.\\r\\n\\r\\nOn the hardware side, agricultural drones often leverage lightweight AI computers. NVIDIA Jetson modules (Nano, TX2, etc.) have been used in research prototypes to run neural networks that segment weeds vs. crops in real time, guiding an attached spray mechanism (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism) (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). For instance, one study deployed a Jetson Nano onboard a drone to perform semantic segmentation of weeds at ~25 FPS, enabling a UAV to spray herbicide precisely on detected weeds (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism) (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). Another low-cost approach integrated an Intel Neural Compute Stick 2 (Myriad X VPU) with a small single-board computer to run a custom \u201cAg-YOLO\u201d object detection model for palm tree disease, achieving 36 FPS detection with only a 1.5 W, 18-gram device (Frontiers | Ag-YOLO: A Real-Time Low-Cost Detector for Precise Spraying With Case Study of Palms) (Frontiers | Ag-YOLO: A Real-Time Low-Cost Detector for Precise Spraying With Case Study of Palms). These examples highlight that both high-end GPUs and specialized accelerators are employed to meet the performance needs of vision-based farming tasks.\\r\\n\\r\\nNot all agricultural use cases are vision-based, however. Some drones carry other sensors (such as thermal cameras, LiDAR altimeters, or hyperspectral sensors) and use onboard computing to interpret this data. For example, a crop-spraying drone might use a LiDAR or ultrasonic sensor to maintain ultra-low flight altitude over uneven fields; the onboard computer reads this sensor input in real time to adjust the drone\u2019s height, ensuring even coverage. Another non-vision example is soil and microclimate sensing: a drone might measure temperature, humidity, or soil moisture via IoT sensors and map these readings to GPS coordinates, requiring onboard logic to synchronize sensor data with location. In all these cases, the onboard computer is essential because farmland environments often lack reliable internet connectivity \u2013 the drone must make decisions on-site (where to spray, which areas need attention) without offloading data. This on-the-fly intelligence boosts efficiency (e.g. less chemical usage by targeting only weed-infested spots) and enables greater autonomy in agricultural operations. Below are some key use cases in agriculture leveraging onboard computation:\\r\\n\\r\\n- Real-Time Weed Detection & Spraying: Drones equipped with AI vision can distinguish weeds from crops in real time and actuate spot-spraying. For example, a Jetson-based drone uses a trained deep learning model (YOLO/segmentation) to identify weeds among crops and trigger a targeted herbicide spray, significantly reducing chemical use (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism) (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). This requires considerable processing power on the drone to run inference with low latency as the UAV moves.\\r\\n\\r\\n\\r\\n- Crop Health Monitoring: Onboard computers process multispectral or RGB images to assess crop health indicators (NDVI, pigment indices) during flight. The drone can immediately flag stressed crop regions (due to drought or disease) and perhaps even alter its route to closer inspect problem spots. These calculations can be done on modest hardware (a Raspberry Pi or Radxa board) since they involve simpler math, though more advanced analysis (like identifying specific diseases from images) would necessitate an AI module.\\r\\n\\r\\n\\r\\n- Autonomous Field Navigation: Farmland drones often fly beyond visual line of sight, so onboard processors handle path planning and obstacle avoidance. For instance, navigating around trees, power lines, or terrain is handled by computer vision (stereo cameras) or LiDAR sensors feeding into the onboard computer, which in turn instructs the flight controller to adjust course. This autonomy ensures safe operation at low altitudes over crops.\\r\\n\\r\\n\\r\\n- Variable-Rate Application: By merging sensor data and GPS maps, an onboard computer can control the variable release of seeds, water, or fertilizer. As the drone surveys a field, it might decide to increase fertilizer drop on an area of poor crop growth and decrease it elsewhere, all based on real-time processed data. While not strictly \u201cvision,\u201d this use of onboard analytics leads to site-specific farming that maximizes yield.\\r\\n\\r\\nOverall, agriculture showcases a range of onboard computing needs \u2013 from relatively low-complexity tasks like geo-tagging sensor readings (achievable with basic SBCs) to high-complexity AI tasks like vision-based weed control (demanding GPU-level performance). The autonomy level also varies: some drones merely assist a farmer by collecting data, whereas others (with onboard AI) can autonomously take actions like precision spraying with minimal human input. In each case, the onboard computer is a critical enabler for making timely decisions in the field, which improves efficiency, conserves resources, and reduces the need for constant human supervision.\\r\\n\\r\\n## Surveillance and Security\\r\\nIn surveillance, security, and public safety applications, drones act as mobile observers \u2013 often tasked with detecting intruders, monitoring crowds, or securing perimeters. These scenarios heavily rely on computer vision running on the drone\u2019s onboard computer to interpret video feeds in real time. A security drone might patrol a fenced facility and use onboard object detection to spot humans or vehicles in restricted areas, immediately alerting security personnel. Similarly, law enforcement can deploy drones at events or in crime response, where the UAV\u2019s onboard AI could track persons of interest through a crowd or follow a fleeing suspect from the air. The common thread is that the drone must \u201cunderstand\u201d what its camera sees, without waiting to stream video back to a control center \u2013 this demands robust edge processing. Indeed, one of the key requirements for UAV surveillance is the ability to detect and track objects of interest in real time (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). Modern deep-learning algorithms (object detectors like YOLO or trackers like SORT/DeepSORT) enable this but are computation-intensive, so a powerful onboard computer (often a GPU-equipped module) is typically used.\\r\\n\\r\\n**Typical Onboard Setup:** Drones for surveillance and security often carry high-end companion computers such as the NVIDIA Jetson TX2, Xavier NX, or newer Orin, which provide the CUDA cores and tensor accelerators needed for real-time vision. For example, researchers demonstrated a drone-based 4K pedestrian tracking system that achieved real-time performance by exploiting both the CPU and GPU of an onboard Jetson TX2 (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). This system could detect and follow people in ultra-high-resolution video (captured from 50 meters altitude) without any ground server, proving that onboard computing can handle even demanding surveillance tasks (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). In testing, the Jetson-based drone successfully detected and tracked individuals in 4K footage at full frame-rate, illustrating the level of performance now attainable on a compact drone (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). Another example is border security drones, which may use thermal cameras for night monitoring; the onboard computer can run thermal image analytics to pick out human heat signatures or vehicles in darkness. These use cases might leverage specialized models (e.g. thermal-trained object detectors) running on the same kind of GPU hardware. Some security drones also employ multiple sensors \u2013 optical cameras for day, IR cameras for night, and even acoustic or RF sensors \u2013 requiring sensor fusion on the onboard computer to make coherent decisions about potential threats.\\r\\n\\r\\nWhy is onboard processing essential for surveillance? Firstly, drones often operate in remote or wide areas (e.g. national borders, large industrial sites) where sending high-bandwidth video to a central server is impractical. The drone must be able to analyze video autonomously because real-time response is critical (it might need to decide to follow an intruder immediately). As one paper noted, wireless connections are not guaranteed aloft, so drones must rely on onboard mission computers for real-time tasks even though deep learning workloads demand high computing power (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). Secondly, latency needs to be minimal \u2013 a few seconds delay in detecting a person could mean the difference between losing or maintaining visual contact. Onboard AI eliminates the round-trip latency of streaming to the cloud and back. Finally, there\u2019s a privacy aspect: processing video on the device means only relevant alerts (e.g. a detected face or license plate) might be transmitted, rather than raw video, which can be important for data security in sensitive surveillance (this benefit of edge computing for privacy is also documented in research (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs)).\\r\\nKey vision-centered use cases in the security domain include:\\r\\n- Perimeter Intrusion Detection: Drones can autonomously patrol fences or borders using onboard vision to detect humans, vehicles, or boats entering restricted zones. A Jetson Xavier-class computer running object detection (like YOLOv5) can identify an intruder and geo-tag their location for response teams. The drone might also track the intruder\u2019s movement with onboard multi-object tracking algorithms, maintaining visual until ground units arrive. This use case demands robust performance to avoid missed detections and often uses thermal imaging at night (the onboard computer handles both IR and daylight video streams).\\r\\n\\r\\n- Crowd Monitoring and Anomaly Detection: Law enforcement or event security drones monitor large gatherings for public safety. Onboard image processing can count people, detect fights or unrest by recognizing specific motion patterns, or spot dangerous objects (e.g. a drone scanning a parking lot for unattended bags). These tasks use computer vision and sometimes machine learning classifiers (for action recognition) running on the drone\u2019s GPU. The autonomy level can range from decision support (flagging something for a remote operator) to fully automated response (e.g. circling a flagged person).\\r\\n\\r\\n- Follow and Track Missions: In police or military operations, a drone may be tasked to follow a target vehicle or person semi-autonomously. Using onboard vision, the drone locks onto the target and navigates to keep the target in frame. This involves real-time object tracking and continuous adjustment of flight \u2013 a compute-heavy loop handled by the onboard computer. Skydio drones, for example, are known for their autonomy in tracking moving subjects through complex environments by leveraging their onboard NVIDIA Orin AI engines ( Skydio Autonomy\u2122 | Skydio ). Such capability has been applied in tactical situations to reduce risk to human officers.\\r\\n\\r\\n- Non-Visual Sensors for Security: Although vision is primary, some security drones use other onboard sensor processing. For instance, a drone could carry a mini radar or LIDAR to detect other drones (counter-UAS scenarios) \u2013 the onboard computer would process these sensor signals to identify and triangulate rogue drones in protected airspace. Similarly, acoustic sensors onboard can pick up gunshot sounds and the computer can classify and localize them (functioning like a flying ShotSpotter). These examples show onboard computing beyond cameras, often supplementing vision for a more robust security solution.\\r\\n\\r\\nIn summary, surveillance/security drones typically represent high-complexity, high-autonomy use cases. They often operate independently for extended periods, making on-board AI indispensable. The performance requirements are among the highest in the drone world: real-time processing of high-res video, detection of small or fast-moving objects, and reliable operation in unpredictable environments. This is why we see cutting-edge embedded GPUs and sophisticated AI models deployed on security drones \u2013 effectively turning them into flying edge computers that \u201csee\u201d and interpret the world in order to keep it secure.\\r\\n\\r\\n## Delivery and Logistics\\r\\nDrone delivery \u2013 transporting packages by air \u2013 presents another major industry segment benefiting from onboard computers. In a delivery scenario, an unmanned aircraft must navigate from a distribution center to a customer\u2019s location, then execute a safe drop-off or landing, all with minimal human oversight. This requires a high degree of autonomy and environmental awareness, since the drone will encounter dynamic obstacles (birds, wires, trees, buildings) and must ensure safety around people on the ground. To achieve this, delivery drones rely on a suite of sensors (visual cameras, depth sensors, sonars, sometimes radar) connected to powerful onboard computing for Detect-and-Avoid (DAA) and precision landing. In fact, for drone delivery to be truly autonomous, the UAV \u201cneeds to be able to see the world around it\u201d via computer vision (Computer vision is key to Amazon Prime Air drone deliveries - Yahoo). Companies like Amazon Prime Air and Alphabet\u2019s Wing have invested heavily in onboard vision systems that allow drones to identify hazards in flight and at the drop site in real time.\\r\\n\\r\\nA prominent example is Amazon\u2019s Prime Air delivery drones. Amazon has developed an onboard \u201cdetect-and-avoid\u201d system with thermal cameras, depth cameras, and sonar working in concert, and machine learning models on the onboard computer to automatically identify obstacles and navigate around them (Amazon Drone Delivery Service Prime Air Is Advancing | Vaughn College). This means the drone\u2019s AI can detect objects such as people, animals, power lines, and even something as thin as a clothesline, and then alter course or abort a landing if needed (Amazon Drone Delivery Service Prime Air Is Advancing | Vaughn College). The drone effectively makes split-second decisions based on its perceptions \u2013 for instance, if during descent to a customer\u2019s yard the cameras spot a dog running underneath, the onboard computer will command the drone to pause or climb to avoid a close encounter. These capabilities are essential not only for safety but also for regulatory approval, as aviation authorities (FAA, etc.) require robust DAA for beyond-visual-line-of-sight flights.\\r\\n\\r\\nThe hardware used in delivery drones is typically at the high end of the spectrum. NVIDIA Jetson platforms or custom system-on-chips with AI accelerators are common, because the drone may be running multiple neural networks: one for sense-and-avoid in the air (finding other aircraft or birds), one for ground hazard detection while landing, perhaps another for reading markers (some delivery systems use fiducial markers or QR codes on landing pads to guide final approach). The autonomy level is very high \u2013 the goal is a drone that can depart, fly complex routes, and deliver with zero human intervention. Onboard computing is what makes this possible, handling tasks such as: route planning and re-planning (if it encounters a new no-fly zone or bad weather en route), real-time adjustment to wind or weather (using IMU and airspeed sensor data in control algorithms), and coordinating the delivery mechanism (like lowering a package by tether and releasing it when conditions are correct).\\r\\n\\r\\nImportantly, delivery drones must also communicate and integrate with logistics systems, and the onboard computer often manages this. For example, it might encrypt and transmit telemetry and video snapshots back to an operations center over LTE/5G, or receive last-second updates (e.g. \u201ccustomer moved 100m, update drop location\u201d). While a lot of heavy lifting (like large-scale route optimization) is done by cloud services, the drone\u2019s onboard system is the real-time executor that adapts on the fly.\\r\\n\\r\\nLet\u2019s break down key use cases and why onboard computing is crucial:\\r\\n\\r\\n- In-Flight Obstacle Avoidance: Delivery routes often go through suburban areas with trees, poles, buildings, and possibly low-flying aircraft. Onboard sensors feed into algorithms (stereo vision or LiDAR-based obstacle detection) that create a 3D map of the drone\u2019s surroundings as it flies. The onboard computer then either autonomously deviates around obstacles or notifies the autopilot to change course. This must happen within fractions of a second. For instance, Prime Air drones scan the skies and ahead for other aircraft or unexpected objects using computer vision, rather than relying solely on ADS-B or external trackers (A drone program taking flight - About Amazon) (Amazon gets FAA approval to expand drone deliveries - Axios). Such real-time perception and navigation absolutely require an onboard AI engine given the latency and reliability constraints.\\r\\n\\r\\n- Precision Landing and Drop-off: When arriving at the destination, a delivery drone might use downward-facing cameras and rangefinders to identify a safe landing zone or to position for dropping a package. Some systems use visual markers on the ground (like an augmented reality tag placed by the customer) \u2013 the drone\u2019s onboard vision will recognize the marker and home in to that spot. Other times, the drone simply analyzes the camera feed to ensure no people, pets, or obstacles are in the immediate landing area. All of this logic lives on the drone. If any hazard is detected last-moment (e.g., the customer walks under the drone), the onboard computer can decide to delay or move the drop location. These decisions can\u2019t wait for a human and are enabled by onboard image recognition and path planning.\\r\\n\\r\\n- Adaptive Route Autonomy: Delivery drones may be given a pre-planned route, but conditions can change. If wind conditions deteriorate or light rain begins, the drone\u2019s onboard computer might adjust its flight envelope or speed. If an area becomes GPS-denied (perhaps near tall buildings), the drone could switch to visual-inertial navigation, again handled by onboard processing of camera/IMU data. In congested airspace, some delivery drones might coordinate via vehicle-to-vehicle comms; the onboard computer would handle such coordination protocols to avoid other drones. These are non-vision computations but still require robust processing and software logic running locally for autonomy.\\r\\n\\r\\n- Payload Management and Other Sensors: In specialized deliveries (like medical deliveries of blood samples or organs, as done by Zipline and others), maintaining the payload\u2019s condition is critical. Onboard systems might regulate a cooler or monitor temperature sensors, ensuring the payload stays within safe conditions, and adjusting mid-flight if needed (e.g., return to base if a problem is detected). While not vision-related, this is another task for the onboard computer, integrating sensor data and acting on it. Additionally, post-delivery, the drone\u2019s computer might run a quick self-diagnostic before returning to ensure no damage occurred during landing \u2013 for example, analyzing motor currents or camera feed for any anomalies like a snagged tether.\\r\\n\\r\\nIn essence, drone delivery pushes the envelope on autonomy and complexity. These drones operate in unstructured environments among the general public, so the performance bar for onboard systems is very high: they must be fast, reliable, and redundant. It\u2019s common to have redundancy (multiple cameras, perhaps two onboard computers cross-checking in critical systems) for safety. The onboard computer is the linchpin that ties together all sensor inputs and executes the flight logic that makes autonomous delivery possible. Without powerful onboard processing, a delivery drone would be limited to very controlled environments or would require a human pilot, which negates the scalability. Thanks to modern edge computers and AI, companies have demonstrated drones that can deliver packages beyond visual line of sight while dynamically reacting to their surroundings in real time (Amazon Drone Delivery Service Prime Air Is Advancing | Vaughn College).\\r\\n\\r\\n## Mapping and Inspection\\r\\nOne of the most widespread drone applications is aerial mapping and infrastructure inspection. This spans use cases like land surveying (creating orthomosaic maps and 3D terrain models), inspecting bridges, wind turbines, power lines, solar farms, and more. Traditionally, many of these missions were semi-autonomous: the drone followed a pre-set path to capture photos, and the heavy data processing (stitching photos into maps or analyzing them for defects) was done later on a ground station or cloud service. However, the advent of powerful onboard computers has begun to transform mapping and inspection into real-time or near-real-time endeavors. Drones in this domain increasingly carry companion computers to perform tasks such as in-flight image processing, immediate detection of anomalies, or even constructing 3D models on the fly. The level of autonomy is also rising \u2013 some advanced inspection drones can navigate around structures and decide their own camera angles using onboard Spatial AI engines.\\r\\n\\r\\n(image) A drone inspects power substation infrastructure. Modern inspection drones utilize onboard vision and AI for collision avoidance and data analysis around complex structures. For instance, Skydio \u2013 a leading drone in autonomous inspection \u2013 is backed by an onboard NVIDIA Jetson Orin GPU, giving it the compute capacity to see, understand, and react in real time while inspecting assets ( Skydio Autonomy\u2122 | Skydio ). With six 360\xb0 navigation cameras feeding its neural networks, the drone builds an understanding of the environment and avoids even small obstacles (like wires) automatically ( Skydio Autonomy\u2122 | Skydio ) ( Skydio Autonomy\u2122 | Skydio ). This allows it to be flown in cluttered spaces by users with minimal training. Furthermore, Skydio\u2019s system can conduct targeted inspections and mapping on-board \u2013 its Spatial AI engine enables features like automated cell-tower scans and the creation of 3D models on the vehicle, in the field, in minutes ( Skydio Autonomy\u2122 | Skydio ). This is a breakthrough for mapping jobs: rather than waiting to process data later, the drone itself can generate a usable 2D map or 3D point cloud before it even lands, letting teams evaluate results immediately and take action (or re-fly if needed) ( Skydio Autonomy\u2122 | Skydio ) ( Skydio Autonomy\u2122 | Skydio ).\\r\\nThere are two broad categories of tasks here: mapping (surveying, reconstruction) and inspection (defect or feature identification). Both benefit from onboard computing in different ways.\\r\\n- Aerial Mapping & Surveying: In a typical mapping mission (say, mapping a construction site or farmland for a survey), a drone captures hundreds of images with GPS tags. While high-precision processing (to create an orthomosaic or 3D mesh) is usually done on powerful computers after the flight, an onboard computer can perform quick intermediate processing. For example, it might stitch a low-resolution preview map on the fly, or use SLAM (Simultaneous Localization and Mapping) techniques to ensure it has covered the area without gaps. In long linear mapping (like pipeline or railway surveys), an onboard computer can monitor image quality and coverage and prompt the drone to take additional images of any missed segment. Some research projects have even implemented on-board photogrammetry to continuously build maps \u2013 though limited by compute, this is improving with devices like the Jetson Orin. The advantage is instant insights: field crews can know right away if they have the data they need. Also, if the drone has RTK GPS and a companion computer, it could georeference images in real time, outputting a map almost ready for use upon landing.\\r\\n\\r\\n- Infrastructure Inspection (Vision-Based): When inspecting physical structures, drones now often carry computer vision models to detect faults or areas of interest. For instance, an inspection drone may use an onboard convolutional neural network to analyze video frames for cracks in a concrete bridge or corrosion on a tower. If the model flags a potential defect, the drone can autonomously hover and circle that spot to collect more data. This kind of adaptive inspection is empowered by onboard processing \u2013 the drone doesn\u2019t have to send all video to an operator; it can make preliminary judgments itself. Thermal cameras are also used (e.g. detecting hot spots in power lines or solar panels), and onboard analysis can immediately highlight abnormal heat signatures for the pilot. Using edge AI for this speeds up the inspection workflow and can reduce human error (the AI might see a subtle crack that a human could overlook in a live feed).\\r\\n\\r\\n- SLAM and GPS-Denied Navigation: Many inspection targets are indoors or in GPS-denied environments (inside large storage tanks, boilers, caves, or industrial plants). Here, drones rely on SLAM algorithms running on onboard computers to both map and navigate. For example, Flyability\u2019s Elios 3 drone is equipped with a LiDAR and onboard SLAM engine called FlyAware\u2122; as it flies inside a dark, cluttered space, it builds a 3D map in real time and uses that for stabilization and pilot feedback (Elios 3 vs. Elios 2: How do the Flyability drones compare) (Elios 3 vs. Elios 2: How do the Flyability drones compare). The Elios can instantaneously render a 3D map of its surroundings on the pilot\u2019s tablet by processing LiDAR and visual odometry data on-board (Elios 3 vs. Elios 2: How do the Flyability drones compare). This is invaluable for inspecting confined spaces safely \u2013 the pilot can see areas the drone might have missed and navigate accordingly, and the final map ensures no spot is left uninspected. SLAM is computationally heavy (involves processing point clouds and running algorithms like ICP or graph optimization), so a robust onboard computer (often an ARM CPU paired with GPU or specialized VIO hardware) is required. Drones like Elios or indoor mapping drones may use Qualcomm Snapdragon-based flight cores or Jetson NX modules to achieve this balance of weight and processing.\\r\\n\\r\\n- Adaptive Flight Planning: With on-board intelligence, an inspection drone can make decisions like a human pilot would. For example, Skydio\u2019s 3D Scan software (running partly onboard) allows the drone to autonomously map out a scanning pattern around a structure, adjusting angles to get complete coverage. It leverages the drone\u2019s real-time mapping to know where it has a line-of-sight and where it hasn\u2019t captured yet ( Skydio Autonomy\u2122 | Skydio ). This level of autonomy turns what used to be a manual, labor-intensive flight into an automated routine, made possible by the drone\u2019s onboard continuous computation of its position relative to the structure and the camera view planning.\\r\\n\\r\\nIt\u2019s clear that mapping and inspection use cases can range from moderate to very high complexity. Some mapping missions might use the onboard computer lightly (just for navigational assistance), whereas advanced inspection in tight environments pushes the limits of onboard processing (doing SLAM, AI defect detection, and path planning concurrently). In all cases, the onboard computer\u2019s role is to increase the autonomy and data quality of the mission: ensuring the drone can go places and capture information that a human or a less-intelligent drone might miss or struggle with. The performance needs scale with the difficulty \u2013 a basic quadcopter surveying an open field might get by with a Raspberry Pi coordinating camera triggers, but a drone inspecting a wind turbine in gusty winds while identifying blade damage in real time will likely have an NVIDIA Orin or similar at its core. The trend in the industry is clearly toward edge computing on drones for mapping/inspection, as it reduces the amount of data to transfer (only results or models are sent back) and enables faster decision-making. Research has shown that equipping drones with edge AI drastically cuts the latency and bandwidth needed for remote sensing tasks \u2013 drones can now detect and recognize objects onboard, making quick local decisions (e.g., find a person to rescue) before sending any data to the cloud (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). This paradigm is making drones more efficient tools for mapping our world and inspecting critical infrastructure in real time.\\r\\n\\r\\n## Industrial Applications (Warehousing, Manufacturing, and Beyond)\\r\\n\\r\\nBeyond the well-defined sectors above, onboard computers in drones are also driving innovative use cases in general industrial and enterprise environments. These include warehouse inventory drones, drones in manufacturing or chemical plants for monitoring, and other specialty uses like mining or oil-and-gas inspections that don\u2019t squarely fall under \u201cmapping\u201d or \u201csecurity.\u201d In many of these scenarios, drones operate indoors or in close proximity to industrial equipment and workers, requiring extremely reliable autonomous navigation and often integration with business systems. Onboard computers play a pivotal role by handling the necessary computer vision, sensor fusion, and planning to allow drones to carry out tasks that used to be manual.\\r\\n\\r\\n(Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology) An inventory drone scans warehouse shelves. In warehouses, drones are being used to automate inventory tracking by scanning barcodes on high shelves \u2013 a task traditionally done via forklifts and manual barcode readers. A prime example is Corvus One, an autonomous inventory drone. It carries an array of 14 cameras and an AI-based onboard system that lets it navigate in GPS-denied warehouse aisles and read pallet labels from the air (Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). Using computer vision, the drone localizes itself among the racks (without needing any external markers or infrastructure) and identifies barcodes on products or QR codes on shelving, associating them with the warehouse database. The on-board computer cross-references these scans with expected inventory and flags discrepancies automatically (Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology) (Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). Corvus Robotics (the company behind it) had to develop a learning-based autonomy stack for robust indoor flight, noting that traditional vision techniques alone were insufficient for \u201clifelong\u201d autonomy in such environments (Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). This means the drone likely uses deep neural networks (perhaps for visual localization or obstacle avoidance) running onboard to adapt to changing warehouse scenes. The result is an infrastructure-free solution \u2013 no need for AR tags or external sensors \u2013 the drone\u2019s onboard intelligence handles it all, making deployment much easier (Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). The complexity here is significant: the drone must avoid moving obstacles (workers, forklifts), adjust to varying lighting, and navigate narrow spaces, which demands high-performance processing and reliable algorithms on-board. Drones like these often utilize specialized computers; while exact hardware isn\u2019t always disclosed, a powerful NVIDIA Jetson or Qualcomm QRB board with multiple camera inputs and NPUs is a likely choice, given the need to process many video streams and run neural networks for navigation and reading text (OCR).\\r\\n\\r\\nAnother industrial use is inspection and monitoring inside facilities (complementing what we discussed in mapping/inspection, but focusing on routine operations in factories or plants). For example, a drone can fly along pipelines or machinery to check for leaks, abnormal vibrations, or temperature spikes. Equipped with an infrared camera and perhaps a sniffer sensor (for gas leaks), the drone\u2019s onboard computer can detect anomalies: e.g., using thermal image processing to spot an overheating motor, or reading analog gauges via computer vision to log pressure readings. Some drones have been prototyped to read instrument panels \u2013 using OCR on onboard video to record meter values, which is far faster than a human doing rounds. These tasks might not require as much AI as other examples; often classical image processing or simple threshold-based alerts are enough, which a Raspberry Pi-level computer could handle. However, as these drones become more autonomous (finding their own way through a factory floor), adding AI for navigation is key. We see some deployments combining SLAM for indoor navigation with specific sensor payloads. The onboard computer thus simultaneously maps the facility (so it knows its location and path) and analyzes sensor data for the monitoring task. This fusion ensures the drone can, say, autonomously inspect a series of checkpoints in a large plant at scheduled times, and alert personnel if it finds an anomaly \u2013 all without human control.\\r\\n\\r\\nA particularly challenging environment is underground mining or industrial confined spaces. In mines, drones are used to map tunnels or assess ore stockpile volumes. They rely on onboard LiDAR processing to avoid collisions in tight shafts and to create volumetric models of mined material (to calculate how much has been extracted or remains). The onboard computer might run point cloud processing to estimate the volume in real time. Likewise, in large oil tanks or pressure vessels, drones (like the aforementioned Elios) fly inside to look for corrosion; their onboard SLAM and lighting systems allow them to operate without GPS or external light. The data (videos, 3D models) is recorded onboard and often also streamed to operators, but initial processing (stabilization, mapping) is done locally to facilitate the mission.\\r\\nFrom a performance standpoint, industrial applications can vary widely. Warehouse inventory drones like Corvus require advanced vision and AI comparable to security drones \u2013 in fact, Corvus claims to be the first to deploy a neural network-based autonomy for indoor drones (Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology), highlighting the cutting-edge nature of that application. On the other hand, a drone that simply carries a gas sensor around a refinery might have modest computing needs (mostly for navigation and data logging). That said, even \u201csimple\u201d tasks benefit from edge computing: by processing data on-board, the drone can react immediately (e.g., hover in place when a gas reading spikes, to take more measurements, instead of continuing on a pre-planned path). Edge computing also ensures that data isn\u2019t lost if communication drops out in a steel-and-concrete facility \u2013 the drone keeps a local record and can upload when back in range.\\r\\nLet\u2019s summarize some industrial use case categories and how they use onboard computers:\\r\\n\\r\\nWarehouse Inventory Management: Drones navigate indoor warehouses to scan inventory (barcodes, RFID, images of products). They use onboard vision for localization and item identification. This is a vision-heavy use requiring AI/ML on board. The drone\u2019s computer interfaces with warehouse management systems, updating stock counts in real time. The benefit is continuous, error-free inventory tracking without interrupting operations (Startup\u2019s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). The complexity is high: multi-camera SLAM, object detection (for codes), and safe autonomous flight in tight spaces are all handled onboard.\\r\\n\\r\\n- Industrial Inspections (Interior): Drones perform scheduled inspections of equipment (pipes, tanks, conveyors) in factories or oil/gas facilities. Onboard computing may run anomaly detection on sensor data \u2013 for example, comparing current thermal images to baseline to catch a hotspot, or using sound/vibration analysis if the drone has a microphone. These drones often have to navigate cluttered interiors, so obstacle avoidance via LiDAR or vision is needed (again handled by the companion computer). Many such solutions emphasize removing humans from dangerous inspection jobs, so the drone must be trusted to fly safely and get the data. High autonomy and moderate to high compute is needed, depending on the sensors used.\\r\\n\\r\\n- Logistics and Parcel Movement: Beyond last-mile delivery outdoors, some drones are being tested for moving parts or goods within large facilities (e.g., shuttling components between factory stations or from storage to assembly line). Here the onboard computer manages routing in dynamic indoor environments. It might integrate with elevator or door controls, signaling them to open, etc. This is similar to an indoor warehouse drone but with a focus on carrying payloads. Vision or LiDAR can be used for navigation instead of relying on pre-laid routes, giving flexibility.\\r\\n\\r\\n- Non-Visual Sensing Missions: Industrial drones also carry non-camera payloads like chemical sensors, magnetometers, radiation detectors (for nuclear plant inspection). The onboard computer reads these sensors and can perform immediate data processing. For example, a radiation drone might map radiation levels to locations in real time to guide itself to hotspots; a magnetometer drone could follow a magnetic anomaly to find a crack in a pipeline. These tasks involve sensor fusion and decision-making by the onboard computer, albeit not as computationally heavy as image analysis. They demonstrate that onboard computing isn\u2019t just about vision \u2013 it\u2019s equally important for other sensor-driven missions.\\r\\n\\r\\nAcross these industrial use cases, the differences in requirements are notable. Warehouse drones and complex inspection drones require cutting-edge autonomy, rivaling that of security drones, because they work around people and assets in a complex 3D environment (with no GPS). They often use the highest-performance onboard computers available (e.g., Jetson AGX Xavier or Orin with multiple CPU cores and GPU acceleration) to run their AI and SLAM pipelines. Simpler tasks in industrial sensing might be done with lower-cost boards or microprocessor-based flight controllers, but even these benefit from at least some onboard computing to format data and respond to triggers. Industrial users also value reliability and integration \u2013 the onboard computer might need industrial-grade safety certifications or the ability to log data in formats directly usable for compliance. This adds another layer of complexity: the computing platform must be robust and secure.\\r\\n\\r\\nFinally, a common theme is connectivity: Industrial drones usually form part of a bigger system (warehouse management software, maintenance scheduling systems, etc.). The onboard computer enables this integration by running middleware or APIs that sync data when a link is available. But if the link is down or the drone is in a radio shadow, it can continue its mission thanks to local processing (improved reliability) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). It can store the collected info (inventory data or inspection footage) and later upload it. Thus, onboard computing in industrial contexts not only provides intelligence but also a buffer against network disruptions, ensuring the drone\u2019s operation is autonomous and fail-resilient.\\r\\n\\r\\n## Conclusion and Comparative Overview\\r\\n\\r\\nFrom the above, it\u2019s evident that onboard computers have become a fundamental enabling technology for drones in virtually every industry. They allow drones to go from remote-controlled cameras in the sky to truly autonomous robots that can interpret and interact with their environment. By processing data on the drone, these systems drastically reduce latency, improve reliability, and unlock capabilities that would be impossible with a strictly human-in-the-loop or offboard processing model (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs).\\r\\nDifferences Across Use Cases: The complexity and performance requirements of onboard computing vary across applications. In agriculture, many missions are pre-planned and relatively structured (flying over flat fields), so the autonomy demands are lower \u2013 a mid-range onboard computer can handle tasks like image capture and even basic AI for weed detection. However, for precision tasks (e.g. weed spraying) the complexity jumps, approaching that of security drones, as the system must recognize small objects (weeds) and act immediately. Surveillance and security drones consistently require high complexity and high autonomy \u2013 they deal with unpredictable environments and critical real-time constraints (tracking fast or small targets), so they use the most powerful processors (Jetson, Snapdragon, etc.) and sophisticated algorithms. Delivery drones also occupy the high end: the safety-critical nature and need to handle all corner cases (wind, moving objects, privacy of people below) force a robust, high-performance onboard computing solution with redundancy. Mapping and inspection drones cover a wide range: a simple mapping drone might not push its onboard computer hard (saving most processing for later), whereas an advanced inspection drone essentially functions as an AI-powered coworker, navigating and analyzing concurrently, which is a high-performance, high-autonomy scenario. Industrial indoor drones tend to need the most reliable autonomy (to operate around valuable equipment and people safely), often matching the complexity of security drones but in a constrained setting \u2013 they typically run cutting-edge SLAM and vision onboard. That said, some industrial tasks can be achieved with simpler means if the environment is controlled (e.g. following fixed routes with a light sensor payload).\\r\\n\\r\\nVision vs. Non-Vision: Another important distinction is between vision-centric use cases and non-vision ones. Many of the celebrated advances (obstacle avoidance, object tracking, etc.) are vision-based and thus demand GPUs and complex models. Non-vision tasks (like carrying a gas sensor or reading a digital meter) may not need as much compute; they could run on microcontrollers or low-power CPUs. Yet, even those often get paired with vision for navigation. For example, a gas-sensing drone might still use a camera to avoid obstacles. So in practice, pure non-vision drones are uncommon in advanced use cases \u2013 most have at least a camera for navigation if not for the primary mission. The onboard computer\u2019s job is to balance all these sensor inputs.\\r\\n\\r\\nWhy Onboard is Essential: Across all industries, a few common reasons explain why having a computer on the drone (versus processing data on the ground or cloud) is essential:\\r\\nLatency and Real-Time Action: Drones often must react in milliseconds (to avoid a collision or target a spray). Onboard processing enables this instantaneous response; sending data to a remote server would be too slow (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs).\\r\\n\\r\\n- Autonomy/Offline Capability: Drones operate in areas without reliable comms (farms, disaster zones, indoor facilities). Onboard intelligence lets them complete missions and make decisions even with zero connectivity (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). This autonomy also reduces the burden on pilots \u2013 one operator can supervise multiple drones that largely pilot themselves.\\r\\n\\r\\n- Bandwidth and Data Reduction: HD cameras generate huge data. Instead of streaming all of it, drones can process video onboard and send back only key results or alerts. This makes beyond-visual-line-of-sight operation feasible over limited bandwidth radio links (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). It also lowers cloud compute costs and can preserve privacy by not broadcasting raw feeds (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs).\\r\\n\\r\\n- Task-Specific Optimization: An onboard computer can be tightly integrated with the drone\u2019s control system. For instance, a vision algorithm can directly influence the drone\u2019s flight (closing the control loop internally). This synergy can achieve maneuvers (like precision landings or smooth target tracking) that would be hard if the processing were offboard due to induced lag.\\r\\n\\r\\n- Increasing Capabilities: As drone roles expand (swarm coordination, package delivery networks, etc.), onboard computing provides a scalable way to add capabilities. Each drone can carry its own \u201cintelligence\u201d without needing a proportionate increase in manpower or constant remote control. This is crucial for scaling drone operations commercially.\\r\\n\\r\\n\\r\\nIn practice, we see a blend of computing happening \u2013 some things are still done offboard (e.g., detailed 3D reconstruction for survey-grade maps) when not time-sensitive. But the frontier is continually moving towards more onboard processing as hardware improves. The NVIDIA Jetson series, Qualcomm Flight platforms, Intel Movidius VPUs, Google Coral TPUs, and others are all evolving to provide greater AI performance in smaller, more power-efficient packages, directly benefiting drone applications. For example, the latest Jetson Orin modules deliver dozens of TOPS of AI performance at under 20 W, enabling complex multi-model AI tasks on a drone in real time that used to require a full laptop or desktop GPU a few years ago (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). This trend will likely continue, with drones getting even more capable of understanding their environment \u2013 blurring the line between a flying camera and a thinking agent.\\r\\nCommercial and Research Momentum: Many commercial solutions mentioned (DJI in agriculture, Skydio in inspection, Amazon in delivery, Corvus in inventory, etc.) showcase what is already possible with today\u2019s onboard computers. Research projects often push the envelope further \u2013 for instance, swarms of drones performing cooperative tasks with minimal communication, each relying on onboard processing and only high-level coordination. The use cases will keep expanding as technology allows. One can envision future drones in emergency response that map a collapsing building interior in real time while searching for survivors using thermal and CO2 sensors, or agricultural drones that identify individual pest insects via onboard vision and precisely dispense biocontrol agents. All these hinge on edge AI and computing on the drone.\\r\\n\\r\\nIn conclusion, onboard computers have moved drones into a new era of autonomous, intelligent operation. Different industries leverage this capability in unique ways, but all benefit from the drone\u2019s ability to make sense of its world and act immediately. The complexity ranges from basic autopilot assistance to full AI-driven missions, and choosing the right onboard computer (from Raspberry Pi class to Jetson Orin class) is a matter of matching the use case needs for vision processing, decision speed, and reliability. What\u2019s common is that the drone is no longer just a remote sensor platform \u2013 it\u2019s an active node in the IoT/edge computing network, often deciding locally and only then sharing results. This improves efficiency and opens up missions that were once thought too difficult or too risky for unmanned systems. As hardware and algorithms continue to advance, we will see even greater autonomy and a proliferation of drone applications across industries, all made possible by that little onboard computer humming away in the sky. (Top 5 Companion Computers for UAVs | ModalAI, Inc.) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs)"}]}}')}}]);