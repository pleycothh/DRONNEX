<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="rss.xsl"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Dronnex Blog</title>
        <link>https://www.dronnex.com/doc/blog</link>
        <description>Dronnex Blog</description>
        <lastBuildDate>Mon, 30 Jun 2025 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Work Log (Week 1)]]></title>
            <link>https://www.dronnex.com/doc/blog/wlbp</link>
            <guid>https://www.dronnex.com/doc/blog/wlbp</guid>
            <pubDate>Mon, 30 Jun 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Current Progress]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="current-progress">Current Progress<a href="https://www.dronnex.com/doc/blog/wlbp#current-progress" class="hash-link" aria-label="Direct link to Current Progress" title="Direct link to Current Progress">​</a></h2>
<ul>
<li>Deng FOC V4 board, can rule two motor with Deng FOC V4 library only, limitated to i2c. ( PID adjust required)</li>
<li>Simple FOC library works in one MOTOR with esp32 only with 2.2.1 library + 2.0.17 ESP32 library. ( some known issue with dual motor, will fix in 2.3.5)</li>
<li>Simple FOC with STM32-F103 works with encoder only, due to time group issue, i2c will not work simply.</li>
</ul>
<p><strong>Failed test:</strong></p>
<ul>
<li>Simple FOC with F411 black pill board: Issue time group</li>
</ul>
<p><strong>Issue Found:</strong></p>
<ul>
<li>Simple FOC will pull up (HIGH) PIN13, 14 by default, witch will make STlink fail.</li>
<li>ESP32 can not work with dual motor on Simple FOC library, known issue, will fix in 2.3.5.</li>
<li>STM32 other than F103 and G431 not worked yet, need more analysis.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="final-objective">Final Objective<a href="https://www.dronnex.com/doc/blog/wlbp#final-objective" class="hash-link" aria-label="Direct link to Final Objective" title="Direct link to Final Objective">​</a></h2>
<p>To be able to have 12 DOF robot with BLDC control, I need cheap, reliaable BLDC FOC module that canble replciate. WIFI on esp32 is not required, also size is too big and no can, leave me STM32 as best option.</p>
<p>I need to test mroe STM32 option, to find best MCU + Encoder + driver conbonation to be reliable and eazy to build also cheap.</p>
<p>I want to fix the module selection, this case code and pid can be replciate easly.</p>
<p>Motor I will decide between 5010 260kv and 5056 140kv, board I will decide between 3 options:</p>
<ul>
<li>odrive v3.6 50$ for 2 motors (50A) - Support Odrive, SImpleFOC</li>
<li><strong>STM32G431-ESC-1</strong> 30$ for 1 motors (30A) - support SimpleFOC and STM Motor SDK</li>
<li>Drive Shiled + STM32 G431 15$ for 1 motor (20A) - support SimpleFOC</li>
</ul>
<p>Build simple project like balance bot or Desk Arm is good for intergrating test with above set up.</p>
<ul>
<li>Balance bot (FOC control for 2 motor, PID control)</li>
<li>2 leg balance bot  (FOC control for 2 motor, PID control, RC controll, )</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-planning">Future Planning<a href="https://www.dronnex.com/doc/blog/wlbp#future-planning" class="hash-link" aria-label="Direct link to Future Planning" title="Direct link to Future Planning">​</a></h2>
<p>Because currently I stuck on running simple FOC code with ESP32, I need to do more testing, with more valid working case, I can make final decision related to STM32 G4 serial.</p>
<ul>
<li>
<p>I need stop work on esp32 two motor project untill ( Simple FOC 2.3.5).</p>
</li>
<li>
<p>I can start pick up 2 leg bot while wating G4 chip.</p>
</li>
<li>
<p>3 5010 motor arm, greate for 1 leg project.</p>
<ul>
<li>G474 + 3 driver + 3 5010</li>
<li>(F103 + driver + 5010) * 3</li>
</ul>
</li>
<li>
<p>need to make decision, is odrive needed?</p>
</li>
<li>
<p>Or G431-esc with power shiled, 30A is enough? Need to torque test.</p>
</li>
</ul>
<p><strong>Decision:</strong></p>
<ol>
<li>5010 260kv motor with 10:1 gear: torque output per Amps.</li>
<li>5056 140KV motor with 10:1 gear: torque output per Amps.</li>
</ol>
<p>if 5010 is enought, 30A drive is enought.</p>
<p>if 5056 50A required, odrive is must have.</p>
<p><strong>FOC TEST List:</strong></p>
<ul>
<li>STM32G431-ESC<!-- -->
<ul>
<li>Simple FOC</li>
<li>STM Motor SDK (SK)</li>
</ul>
</li>
<li>STM32G474 ( able to control 3 driver)<!-- -->
<ul>
<li>Simple FOC</li>
</ul>
</li>
<li>STM32F407<!-- -->
<ul>
<li>Simple FOC</li>
</ul>
</li>
<li>ESP32 ( Okay for hobby and POC, not sutable for dog.)</li>
<li>STM32 F103 ( back up for G431) ( only working version)</li>
<li>STM32 F411 ( no working)</li>
<li>STM32 H750 ( optional test)</li>
<li>STM32 H753 ( over kill for FOC only)</li>
<li>STM32 F405 ( 2 driver )</li>
</ul>]]></content:encoded>
            <category>simpleFOC</category>
        </item>
        <item>
            <title><![CDATA[Human Body vs. Humanoid Robot Design]]></title>
            <link>https://www.dronnex.com/doc/blog/hbhrd</link>
            <guid>https://www.dronnex.com/doc/blog/hbhrd</guid>
            <pubDate>Sat, 24 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Neural Control Frequencies: Human vs. Robot]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="neural-control-frequencies-human-vs-robot">Neural Control Frequencies: Human vs. Robot<a href="https://www.dronnex.com/doc/blog/hbhrd#neural-control-frequencies-human-vs-robot" class="hash-link" aria-label="Direct link to Neural Control Frequencies: Human vs. Robot" title="Direct link to Neural Control Frequencies: Human vs. Robot">​</a></h2>
<p><strong>Human Low-Frequency Neural Signals:</strong> The human motor system operates effectively with relatively
low-frequency control signals, on the order of only a few to a few tens of hertz. Voluntary movements
and reflexes are orchestrated by neural impulses that typically range from about 5–50 Hz in frequency.
For example, the fastest monosynaptic stretch reflex in humans (the M1 response) has a latency of
roughly 30–50 ms – equivalent to an update frequency on the order of 20–33 Hz. Longer-loop
reflexes and voluntary reactions occur at even lower bandwidths (e.g. voluntary reaction times ~150 ms
or ~6–7 Hz) . Despite these low neural update rates, the human body achieves smooth and stable
control of movement. This is possible because the musculoskeletal system and spinal cord provide builtin stability and rapid local responses that compensate for the limited signaling frequency of the nerves.</p>
<p><strong>Robotic High-Frequency Control Loops:</strong> By contrast, humanoid robots typically require much higher
control-loop frequencies – often in the hundreds of hertz to kilohertz range – to achieve stable control.
Modern robotic actuators (e.g. electric motors) and sensors can be polled and updated every 1–2 ms or
faster (hundreds to thousands of Hz). This high-frequency feedback is needed to satisfy control stability
criteria (Nyquist sampling of fast dynamics) and to actively correct errors before the rigid mechanism
becomes unstable . Indeed, engineered legged robots commonly run their feedback control at 1 kHz
or more . Without such high-rate feedback, a stiff-legged robot can quickly become unstable after
rapid disturbances like foot impacts. In effect, robots rely on “brute-force” high-speed control to
maintain balance and performance, whereas humans rely more on intrinsic biomechanics and slower
neural feedback. As a concrete example, one study notes that animals like cats (≈4   kg) have
sensorimotor delays up to ~41 ms, yet can run at ~5 Hz stride frequency, meaning neural feedback
cannot even update for roughly half of a stance phase . Animals still remain stable thanks to passive
dynamics, whereas a comparably rigid robot might demand a control loop in the kHz range to handle
such rapid gait dynamics . This fundamental difference underscores why humans manage with
<strong>low-frequency signals while robots must often push toward high-frequency feedback loops for
stability.</strong></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="biological-elasticity-proprioception-and-reflexes-in-stability">Biological Elasticity, Proprioception, and Reflexes in Stability<a href="https://www.dronnex.com/doc/blog/hbhrd#biological-elasticity-proprioception-and-reflexes-in-stability" class="hash-link" aria-label="Direct link to Biological Elasticity, Proprioception, and Reflexes in Stability" title="Direct link to Biological Elasticity, Proprioception, and Reflexes in Stability">​</a></h2>
<p><strong>Elasticity as a Stability Mechanism:</strong> The human musculoskeletal system is replete with elastic
elements – tendons, ligaments, connective tissues, and the intrinsic compliance of muscles themselves.
These elastic components store and release mechanical energy and provide passive stiffness and
damping. For instance, during running, the Achilles tendon and other elastic tissues act like springs to
store energy at foot contact and return it in push-off, significantly reducing the metabolic work muscles
must do . The <strong>energetic economy</strong> of human gait benefits greatly from this elasticity: tendons can
return on the order of 50–60% of the positive work in activities like running, effectively recycling energy
that a rigid system would lose as heat . Beyond energy efficiency, elasticity also aids stability. An
elastic leg will <strong>absorb shock and self-stabilize</strong> to some extent upon impact (like a pogo stick
compressing), which smooths out high-frequency disturbances <strong>without requiring neural intervention
.</strong> Researchers refer to this instantaneous mechanical response as the preflex: the muscle-tendon
unit’s immediate reaction to perturbation, occurring within milliseconds – much faster than any nerve
reflex can act . Experiments confirm that muscles exhibit a short-range stiffness and damping
behavior in the first few milliseconds of a perturbation, resisting sudden changes in length before the
nervous system even has time to respond . This means the body’s mechanics handle the first line
of defense against disturbances, enabling the slower neural loops (tens of milliseconds or more) to
issue corrections once the immediate shock is handled.</p>
<p><strong>Proprioception and Reflex Loops:</strong> The human nervous system is equipped with rich sensor feedback (proprioception) and fast reflexive control that complement mechanical elasticity. Muscle spindles measure stretch and speed in muscles, Golgi tendon organs sense tension, and joint receptors and the inner ear vestibular system provide information about body position and balance. These sensors feed into reflex pathways in the spinal cord and brainstem. Spinal <strong>reflex loops</strong> operate at latencies on the order of 30–80   ms in humans . For example, a sudden muscle stretch triggers a spinal reflex contraction (the stretch reflex) in ~40 ms, preventing the muscle from lengthening too quickly and contributing to joint stability. Slightly longer latency responses (50–100 ms) involve multiple synapses or cortical input (e.g. postural reflexes and righting responses) . Collectively, these reflexes provide automatic feedback control that does not require conscious thought and runs at a higher frequency than conscious control. Crucially, though these neural feedback loops are “low-frequency” compared to modern computers, they are well-tuned to the dynamics of the body. The combination of passive elasticity and timely reflexes makes the human control system highly robust and efficient: perturbations are blunted mechanically and corrected reflexively, allowing the brain to operate with slow (~5–20 Hz) high-level signals without loss of stability.</p>
<p><strong>Implications for Robots:</strong> By contrast, traditional humanoid robots built with rigid joints and metal links lack the intrinsic elasticity and distributed sensing that biology provides. A rigid robot joint has little ability to absorb energy – a shock must be countered by active motor torque almost immediately. Furthermore, until recently robots had relatively sparse proprioceptive sensing (typically just joint encoders and maybe force/torque sensors), lacking the rich, built-in sensor density of human muscles and skin. This is why purely rigid robots have historically required very stiff control (high gain, high frequency) to emulate the stability that humans get “for free” from muscles and reflexes. Modern robotics research has recognized this gap: adding compliance (springs, dampers, or flexible materials)
to robots can dramatically improve stability and robustness in the face of delays or low feedback rates . In one study, adding a parallel elastic spring to a robot leg joint allowed stable landing control with feedback rates as low as 20 Hz despite significant sensorimotor delay, whereas the same task was unstable without compliance at such low frequencies . This mirrors the animal kingdom: biological elasticity and reflexes let animals tolerate slow neural loops, a design principle engineers are leveraging by building compliance and fast reflex-like controllers into robots.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="field-oriented-motor-control-and-proprioceptive-sensors-in-robots">Field-Oriented Motor Control and Proprioceptive Sensors in Robots<a href="https://www.dronnex.com/doc/blog/hbhrd#field-oriented-motor-control-and-proprioceptive-sensors-in-robots" class="hash-link" aria-label="Direct link to Field-Oriented Motor Control and Proprioceptive Sensors in Robots" title="Direct link to Field-Oriented Motor Control and Proprioceptive Sensors in Robots">​</a></h2>
<p><strong>Muscle Control vs. Field-Oriented Control:</strong> Humans control muscle force by modulating the firing rate
of motor neurons and recruiting motor units, effectively adjusting the current sent to muscle fibers via
neural impulses. In robotic systems, an analogous function is performed by Field-Oriented Control
<strong>(FOC)</strong> of electric motors. FOC is an advanced control technique for brushless motors (BLDC or AC
motors) that regulates the motor’s phase currents in real time, keeping the magnetic field oriented for
optimal torque output. In practice, FOC algorithms run at very high frequency (often tens of kHz) on
motor drivers, far faster than any biological signaling, to ensure smooth and precise torque generation.
This inner loop can be seen as the robot’s “alpha motor neuron” equivalent – it drives actuators with the
right amount of current (force) continuously. The combination of FOC and fast current sensing means a
robot’s joint actuation is tightly controlled at the millisecond level, achieving a level of precision and
responsiveness in torque control that mimics the finely graded force control of human muscles.
Encoder and IMU Feedback (Artificial Proprioception): Just as humans rely on proprioceptive
feedback from muscle sensors and the vestibular system, humanoid robots employ a suite of sensors to
achieve closed-loop control. High-resolution rotary encoders on each joint provide continuous
feedback on joint angle and velocity – analogous to muscle spindle feedback about limb position. In
addition, force/torque sensors or motor current sensors can indicate how much load a joint is bearing,
akin to Golgi tendon organ sensing of tension. Whole-body orientation and balance in robots are
monitored by inertial measurement units (IMUs) (accelerometers and gyroscopes), which play a role
comparable to the human inner ear’s vestibular system for detecting head orientation and acceleration.
These sensors enable reflex-like control loops in robots: for instance, a humanoid’s balance controller
might use IMU data to trigger a rapid ankle adjustment when the robot tilts, much as a human’s
vestibulo-spinal reflex triggers muscle responses to prevent a fall.</p>
<p>Modern humanoid designs tightly integrate such sensors with their motor controllers. For example, a
state-of-the-art bionic leg or humanoid joint module might include a motor encoder, a joint output
encoder, a 9-axis IMU, and even a 6-axis force sensor, all feeding into the control system . The motor
controller uses these inputs to close multiple feedback loops – position, velocity, torque (current), and
even impedance – in real time . This architecture parallels the multi-layered feedback in humans
(spinal reflex loops, higher-level posture control, etc.). In effect, the robot’s encoders and IMUs serve
as its proprioceptive sense, and the FOC-based controllers act like artificial motor neurons and
reflex circuits, keeping the machine balanced and on trajectory. The key difference is one of speed and
implementation: robotic sensors can be read thousands of times per second and control outputs
updated accordingly, whereas human proprioceptive feedback is slower but compensated by the body’s
mechanical design as discussed. Nonetheless, as robotics adopts bio-inspired approaches, the gap is
narrowing – robots are increasingly endowed with dense sensors and fast, layered control loops to
achieve a more human-like finesse in movement.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-need-for-high-frequency-feedback-in-robots">The Need for High-Frequency Feedback in Robots<a href="https://www.dronnex.com/doc/blog/hbhrd#the-need-for-high-frequency-feedback-in-robots" class="hash-link" aria-label="Direct link to The Need for High-Frequency Feedback in Robots" title="Direct link to The Need for High-Frequency Feedback in Robots">​</a></h2>
<p>One fundamental question arises: given the effectiveness of biological low-frequency control, why do
robotic systems typically require such high-frequency feedback loops (100 Hz to 1 kHz or more) to
remain stable? The answer lies in the differences in physical dynamics and inherent damping. Rigid
robotic systems have very low passive damping or compliance – a steel limb connected to a highstiffness servo will not naturally absorb shock or oscillations. Thus, if an unexpected perturbation
occurs (say the robot’s foot strikes an obstacle), the deviation in position or force can change very
rapidly (within a few milliseconds). To correct this before it amplifies or causes tipping, the robot’s
control system must sense the error and apply a counteracting torque almost immediately. This
demands a feedback cycle on the order of the system’s fastest significant dynamics. In many robots,
structural vibrations or actuator electrical dynamics can have frequencies in the hundreds of hertz,
requiring controller update rates in the high hundreds or thousands of hertz (by Nyquist’s criterion) .
A practical rule in robotics is to run the inner control loops as fast as possible (often 1 kHz for torque/
position loops) to effectively make the robot “feel” more damped and stable than its bare mechanical
structure would allow</p>
<p>Moreover, unlike biological muscles, electric motors and gear trains have almost no intrinsic shock
absorption. Any delay or slack in responding to a disturbance can let energy build up, leading to
oscillation or instability. High-frequency control effectively adds active damping. For instance, if a
humanoid sways slightly, a 500 Hz balance controller can apply corrective joint torques 500 times per
second, preventing the sway from growing. At 50 Hz control, the same robot might start to wobble or
overshoot because the corrections come too infrequently relative to its natural oscillation period. This is
why humanoid robots like Boston Dynamics’ Atlas or Honda’s ASIMO historically run their low-level joint
controllers at very high rates (0.5–1 kHz or more), and high-performance motor drivers often boast
PWM/FOC frequencies of 10–20 kHz for smooth torque output. High feedback rates also help filter
sensor noise and model uncertainties; with more frequent measurements and adjustments, the robot
can better reject disturbances.</p>
<p>It’s worth noting that biological systems avoid this necessity via design: the combination of
compliance, distributed control (reflexes), and safe low gain tuning lets animals remain stable with
slower updates. When engineers attempt to control robots with slower, human-like update rates
without modifying hardware, they often encounter instability – illustrating how tightly coupled a robot’s
required control frequency is to its mechanical design. Research in legged robotics has demonstrated
that introducing elastic elements or other passive dynamics can allow stable control with much lower
feedback rates. One study achieved stable hopping and landing of a robot leg with only a 20 Hz control
loop by incorporating a compliant parallel spring, effectively imitating the role of a tendon . The
robot could tolerate sensorimotor delays up to ~60 ms without falling over when this compliance was
present . This is a stark contrast to a fully rigid system, and it reinforces the principle that <strong>if you give
a robot some “muscle and tendon” properties, it no longer demands superhuman (or rather
supercomputer) reflexes to remain upright.</strong></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="rigid-actuators-vs-compliant-muscle-energy-and-responsiveness">Rigid Actuators vs. Compliant Muscle: Energy and Responsiveness<a href="https://www.dronnex.com/doc/blog/hbhrd#rigid-actuators-vs-compliant-muscle-energy-and-responsiveness" class="hash-link" aria-label="Direct link to Rigid Actuators vs. Compliant Muscle: Energy and Responsiveness" title="Direct link to Rigid Actuators vs. Compliant Muscle: Energy and Responsiveness">​</a></h2>
<p><strong>Limitations of Rigid Actuators:</strong> Most current humanoid robots use electric motors coupled with highreduction gearboxes or harmonic drives to move their joints. These actuators are essentially rigid – they
output a fixed displacement or torque with very little compliance. While this yields precise position
control and high force, it comes with several drawbacks. Firstly, rigid actuators cannot store significant
elastic energy. Any energy put into the system (e.g. an impact or the deceleration of a limb) must be
either actively dissipated by the motor (often heating it) or it will cause a rebound or oscillation. This
leads to poor energy efficiency, as motors work against sudden forces rather than with them. Secondly,
lack of compliance means high impact forces are transmitted to the mechanism and environment. A
stiff-legged robot landing from a jump generates impulsive forces that can damage hardware or make
the robot more prone to slipping. Humans, in contrast, have flex in their joints and tissues that cushion
impacts. Thirdly, rigid actuators make control extremely sensitive – a small error in commanded
position can create a large force (because the system is stiff), which can then lead to instability or jitter
unless the control gains are kept low or the loop frequency is high. In summary, purely rigid, positioncontrolled robots tend to be energy-hungry, shock-sensitive, and require careful tuning to avoid
oscillations.</p>
<p><strong>Biological Compliance and Softness:</strong> The human body, and animals in general, demonstrate
remarkable efficiency, agility, and robustness that outperform even state-of-the-art robots , and a
core reason is the compliance in biological actuators . Muscles and tendons act together as a
series elastic actuator: the tendon (and connective tissue) provides a spring in series with the muscle’s
force generation. This has multiple benefits. It inherently limits peak forces (the spring stretches under
extreme load, preventing infinite force transmission), thus protecting both the muscle and whatever the
muscle is pushing against – an aspect crucial for injury prevention and also for gentle interaction with
the environment. The elasticity allows energy storage: when you land from a jump, your Achilles
tendon and arch of the foot stretch, storing energy that is given back as you push off . A rigid robot
would instead dissipate that energy or require the motors to actively perform negative work to
decelerate, wasting energy as heat. Compliance also enables power amplification: muscles can prestretch a tendon and then release it to get a catapult-like burst of motion (as in a jumping flea or a
human vertical leap). In robotics, adding springs has shown to increase peak power output by releasing
stored energy faster than motors alone could provide . Furthermore, compliance improves
robustness and adaptability. If the ground is uneven or softer than expected, a compliant leg
automatically adapts its shape slightly on contact (a phenomenon known as self-stability in
locomotion). This means the system is less dependent on perfect sensor information or timing. As
Alexander and others have noted, animals use compliance to absorb impacts, to bounce like springs
during running, and to serve as return springs for repositioning limbs . Even metabolic efficiency is
higher: a review of animal locomotion studies found that compliance in muscles and tendons reduces
the metabolic cost of movement by taking on some of the work that muscles would otherwise have to
do, especially in cyclic tasks like hopping or running .</p>
<p><strong>Series Elastic Actuators in Robotics:</strong> To bridge this gap, roboticists have introduced compliance
intentionally into actuators. A well-known approach is the Series Elastic Actuator (SEA), where a spring
(often a steel coil or elastomer) is placed in series between the motor and the load. SEAs and related
designs (variable stiffness actuators, parallel elastic actuators) confer many of the advantages of
biological muscle-tendon units: they can absorb and store energy, filter out high-frequency shocks, and
produce smoother, more controllable forces . The presence of a calibrated spring also allows
direct measurement of force (from spring deflection) and more robust force control. Studies have
demonstrated that adding series or parallel elasticity in robots can <strong>improve energy efficiency and
reduce peak power requirements</strong> for motors . For example, adding a tailored compliance to a
robotic knee joint was shown to yield up to 50% energy savings in certain motions compared to a purely
rigid actuation . Another result showed that parallel elastic elements in a bipedal robot improved
locomotion efficiency and reduced motor loads . Compliance can also significantly enhance
interaction safety – a soft joint is inherently safer for a robot working around humans, as it will yield if
it hits a person, much like our muscles do, rather than imparting the full force of a rigid mechanism .
The trade-off is that introducing elasticity can make precise position control harder (since the spring can
oscillate); however, clever control strategies and the intrinsic benefits often outweigh this, especially for
dynamic tasks. The current state-of-the-art humanoids often incorporate at least some passive or active
compliance to better emulate human muscle behavior. In short, <strong>the softness of human tissue gives
biological systems a huge advantage in energy efficiency and responsiveness,</strong> and robotics is
increasingly embracing that lesson by softening rigid actuators.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="state-of-the-art-in-artificial-muscle-technologies">State of the Art in Artificial Muscle Technologies<a href="https://www.dronnex.com/doc/blog/hbhrd#state-of-the-art-in-artificial-muscle-technologies" class="hash-link" aria-label="Direct link to State of the Art in Artificial Muscle Technologies" title="Direct link to State of the Art in Artificial Muscle Technologies">​</a></h2>
<p>To truly replicate the human body’s capabilities, engineers are developing artificial muscle
technologies – actuators that more directly mimic the properties of biological muscles (soft, compliant,
high power-to-weight, efficient). Several cutting-edge approaches show promise.</p>
<p>In summary, artificial muscle technology is advancing rapidly, aiming to provide robots with contractile
elements that rival or surpass human muscles in certain aspects. Electroactive polymers promise
muscle-like form factors and response; pneumatic and soft fluidic muscles provide power and
compliance; and new smart materials continue to emerge. These technologies address the limitations
of traditional motors by bringing in softness, efficiency, and adaptability akin to biology. The challenge
remains to integrate them into complete robotic systems that can also leverage suitable control
strategies – which brings us to the control paradigm inspired by the human nervous system.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="neuromorphic-control-inspired-by-the-human-nervous-system">Neuromorphic Control: Inspired by the Human Nervous System<a href="https://www.dronnex.com/doc/blog/hbhrd#neuromorphic-control-inspired-by-the-human-nervous-system" class="hash-link" aria-label="Direct link to Neuromorphic Control: Inspired by the Human Nervous System" title="Direct link to Neuromorphic Control: Inspired by the Human Nervous System">​</a></h2>
<p>While new actuators mimic the muscles, neuromorphic control aims to mimic the brain and nervous
system’s way of processing information. Biological neural networks are fundamentally different from
conventional robot control algorithms – they compute with spikes (discrete pulses) and operate in a
highly parallel, event-driven manner. This makes animal nervous systems extraordinarily efficient and
robust in the real world, easily outperforming today’s robots in tasks like sensory integration, learning,
and adaptive control . For example, our brains and spinal cords effortlessly handle balancing,
grasping, and walking on uncertain terrain, tasks that still challenge robots. Neuromorphic engineering
seeks to capture some of these advantages by redesigning control systems to work more like neurons.</p>
<p>Spiking Neural Networks (SNNs) and Brain-Inspired Chips: A key development is the use of spiking
neural networks for robot control. In an SNN, neurons emit timed spikes and communicate in a way
analogous to real neurons, and such networks can be deployed on special neuromorphic hardware
chips that run networks very efficiently. Research has shown that SNN-based controllers can be
exceptionally fast and energy-efficient, since they naturally process information in parallel and only
when events (spikes) occur . This is well-suited to robotics, where sensors often produce sparse
signals (e.g. a vision sensor detecting a change, or a touch sensor feeling contact). A neuromorphic
controller can remain mostly idle (drawing negligible power) until relevant spikes indicate something to
respond to, much like our reflex circuits sit quiescent until triggered. Companies and research labs have
built neuromorphic chips (e.g. Intel’s Loihi 2, IBM’s TrueNorth, and others) that implement tens of
thousands of spiking neurons in hardware, with event-driven computation that can run in real time for
robotic tasks. These chips have been tested in scenarios such as controlling a balancing robot, doing
pattern generation for locomotion, and even real-time adaptation – all with very low energy
consumption compared to a traditional CPU running equivalent control algorithms.</p>
<p>Reflexes and CPGs in Silicon: Another neuromorphic approach is to emulate specific neural circuits
from biology, such as central pattern generators (CPGs) and reflex loops, using analog or spiking
models. Central pattern generators are networks in the spinal cord that produce rhythmic outputs (for
locomotion, breathing, etc.) without needing continuous input from the brain. Roboticists have
implemented CPG models that run on neuromorphic hardware to control legged robots, producing
adaptive gait patterns that are robust to perturbations – effectively giving the robot a “spinal cord” that
takes care of low-level leg coordination. For instance, a neuromorphic CPG controller can generate
walking or crawling rhythms and modulate them based on feedback spikes from sensors (like load
sensors on the feet) . This mirrors how, in animals, the spinal CPG can adjust step timing or force in
response to a sudden change (like stepping on a rock) quickly and without central intervention.
Similarly, neuromorphic implementations of reflex arcs have been explored: when a sensor spike
indicating, say, an excessive joint stretch arrives, a spiking interneuron can trigger a motor neuron to
counteract it – just like a biological stretch reflex. Because these computations are local and eventdriven, the latency can be very low (a few milliseconds) and the computing load minimal.</p>
<p>Advantages of Neuromorphic Control: Neuromorphic and bio-inspired control strategies offer several
potential advantages. They can be high-speed and low-latency (spikes propagate quickly through a
hardware neural net, and many operations happen in parallel). They are inherently robust to noise and
damage, as neural networks can often function even if some neurons/spikes are lost, much like our
brains gracefully handle partial injuries or noise. They also promise lower power consumption – brains
only use ~20 W of power, and neuromorphic chips, by avoiding the overhead of clocked synchronous
logic, can be extremely energy efficient for certain tasks. Neuromorphic controllers can learn and adapt
on the fly using mechanisms analogous to synaptic plasticity. For example, a robot might use spiketiming-dependent plasticity (STDP) rules to “learn” how to improve its balance or adapt its gait,
gradually tuning the synaptic weights in its network rather than requiring a programmer to tweak
control gains. This learning can even be done on the hardware in real time, leading to lifelong
adaptation, something very natural for animals but difficult for conventional control systems.</p>
<p>In essence, neuromorphic approaches are bringing the architecture of robotic control closer to that
of a biological nervous system. While a traditional robot might have a centralized processor running a
control loop at 1 kHz, a neuromorphic-controlled robot could have thousands of “neurons” monitoring
sensors and driving motors asynchronously. Such a robot might exhibit more reflexive, fluid motions
and be better at reacting to unforeseen events, since its control is distributed and event-driven rather
than strictly periodic. We are already seeing demonstrations of neuromorphic vision sensors (event
cameras) combined with spiking neural nets to do things like ultra-fast obstacle avoidance in drones,
where the entire perception-action loop is conducted with spiking hardware in microseconds. As these
technologies mature, we expect humanoid robots to gain more brain-like qualities: low-power
sensing, fast reflexes, and adaptive learning capabilities that traditional control architectures
struggle to realize.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://www.dronnex.com/doc/blog/hbhrd#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Engineering design for humanoid robots is increasingly informed by insights from human physiology.
The human body achieves a remarkable balancing act: it uses slow, low-frequency neural signals yet
maintains graceful, stable, and efficient movement. This is possible because of evolutionary adaptations
– elasticity in tissues, decentralized reflex loops, and a robust neural architecture – that work in
harmony with the body’s mechanics. In contrast, early robots were built rigid and controlled like
factories, needing high-frequency, high-gain feedback to even approximate stability. Today, the frontier
of humanoid robotics is all about closing the gap with biology: adding compliance and soft
materials to mimic muscle and tendon behavior, and adopting neuromorphic, brain-inspired control to
attain the efficiency and adaptability of neural systems.
In summary, a technical comparison reveals that for every clever strategy in human biomechanics or
neural control, roboticists are developing an analogue. Low-frequency neural control is compensated by
high intrinsic stability – so robots add springs or dampers to gain physical stability. Biological
proprioception and reflexes are swift and automatic – so robots incorporate better sensors and local
feedback loops (even on-chip neural networks) to respond in kind. Muscles are soft, efficient, and multifunctional – so artificial muscles and compliant actuators are being created to replace or augment
electric motors. The human nervous system computes with spikes and adapts continuously – so
neuromorphic processors and learning algorithms are being employed to give robots a more brain-like
edge.</p>
<p>Ultimately, <strong>the synergy of mechanical design and control architecture</strong> in the human body sets a high
bar that robotics is steadily striving toward. The comparative study of human vs. humanoid design not
only highlights our current technological limitations but also lights the way forward: future generalpurpose humanoid robots will likely blend soft, muscle-like actuators with brain-inspired controllers,
achieving a level of agility, efficiency, and resilience that today remains unique to biological organisms
. Each new development – be it a better artificial muscle or a smarter neural network controller –
is a step toward robots that move and respond with the fluidity of human beings, ultimately enabling
safer and more capable interactions in the real world.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="sources">Sources:<a href="https://www.dronnex.com/doc/blog/hbhrd#sources" class="hash-link" aria-label="Direct link to Sources:" title="Direct link to Sources:">​</a></h2>
<ul>
<li>Bertec Corp., “Body-Brain Connection Part II: Automatic Postural Control” – reflex latency data .</li>
<li>Kristinn Heinrichs, et al., on human postural reflex loops (2023) .</li>
<li>Frontiers in Robotics &amp; AI (Ashtiani et al., 2021) – comparison of animal vs. robot sensorimotor</li>
<li>delays and role of compliance .</li>
<li>Arthur Kuo et al., PLOS Comp. Biol. (2021) – on tendons saving energy in running .</li>
<li>ASME IDETC (2013) – on compliance improving efficiency, agility in animals vs robots .</li>
<li>Araz et al., Front. Bioeng. Biotech. (2023) – muscle preflex response in milliseconds .</li>
<li>Nature Biomedical Eng. (2021) – open-source bionic leg with FOC, encoders, IMUs .</li>
<li>StackExchange Robotics – practical note on 1 kHz control for 500 Hz dynamics .</li>
<li>MDPI Polymers (2025) – review of electroactive polymer muscles .</li>
<li>Daerden &amp; Lefeber (2002) – overview of pneumatic artificial muscles .</li>
<li>Nature Communications (Buchner et al., 2024) – electrohydraulic artificial muscle leg performance .</li>
<li>Frontiers in Neurorobotics (Bing et al., 2018) – survey of spiking neural network control and neuromorphic advantages .</li>
</ul>]]></content:encoded>
            <category>chatgpt</category>
        </item>
        <item>
            <title><![CDATA[Roadmap to Embedded & Robotics Engineering]]></title>
            <link>https://www.dronnex.com/doc/blog/rtere</link>
            <guid>https://www.dronnex.com/doc/blog/rtere</guid>
            <pubDate>Tue, 20 May 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Month 1: ROS 2 Foundations & Microcontroller Basics (Weeks 1–4)]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="month-1-ros-2-foundations--microcontroller-basics-weeks-14">Month 1: ROS 2 Foundations &amp; Microcontroller Basics (Weeks 1–4)<a href="https://www.dronnex.com/doc/blog/rtere#month-1-ros-2-foundations--microcontroller-basics-weeks-14" class="hash-link" aria-label="Direct link to Month 1: ROS 2 Foundations &amp; Microcontroller Basics (Weeks 1–4)" title="Direct link to Month 1: ROS 2 Foundations &amp; Microcontroller Basics (Weeks 1–4)">​</a></h2>
<ul>
<li>Week 1 – ROS 2 Setup and Fundamentals: Install ROS 2 (latest LTS) and set up your Linux development environment. Review ROS 2 architecture (nodes, topics, services, parameters) and command-line tools . Complete introductory tutorials (talker/listener, turtlesim) to build basic ROS 2 skills . Milestone: You can run a simple ROS 2 talker/listener demo and understand the core concepts.</li>
</ul>
<ul>
<li>
<p>Week 2 – ROS 2 Nodes &amp; Packages: Create your first custom ROS 2 package. Write a C++ node that publishes a dummy sensor message (e.g. a counter or random number) and a Python node that subscribes to it. Practice building with colcon and using rviz or rqt to visualize topics. Explore ROS 2 client libraries and differences from ROS 1 (if applicable). Milestone: A custom ROS 2 package with C++ and Python nodes communicating successfully.</p>
</li>
<li>
<p>Week 3 – Microcontroller Environment Setup: Set up development tools for STM32H753ZI (e.g. STM32CubeIDE or PlatformIO). Blink an LED and read a button input on the STM32 to verify the toolchain. If using an RTOS (FreeRTOS), create a basic task for blinking LED. Repeat a similar “blinky” test on the ESP32-S3 (using ESP-IDF or Arduino framework) to familiarize yourself with both controllers. Milestone: Both STM32 and ESP32 environments are verified by running basic firmware (LED blink, serial hello-world).</p>
</li>
<li>
<p>Week 4 – C++ on MCU &amp; Sensor I/O: Write a simple C++ program on the STM32 to interface with hardware peripherals. For example, read an IMU sensor over I²C (or SPI) and output the readings over UART. Implement a basic PID loop in the microcontroller: e.g., use a potentiometer as input and a LED brightness (PWM) as output to simulate controlling a “process” to a setpoint, just to practice control logic timing. Keep code efficient and use interrupts or RTOS for timing if needed. Milestone: You can read a sensor and actuate an output with a control loop on the STM32, and understand microcontroller C++ basics (registers, HAL libraries, interrupts).</p>
</li>
</ul>
<p>Checkpoint (End of Month 1): You have a solid grasp of ROS 2 basics and a working embedded dev setup. You can develop simple ROS 2 nodes  and microcontroller firmware in C++, setting the stage for integration.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="month-2-ros-2--microcontroller-integration-weeks-58">Month 2: ROS 2 ↔ Microcontroller Integration (Weeks 5–8)<a href="https://www.dronnex.com/doc/blog/rtere#month-2-ros-2--microcontroller-integration-weeks-58" class="hash-link" aria-label="Direct link to Month 2: ROS 2 ↔ Microcontroller Integration (Weeks 5–8)" title="Direct link to Month 2: ROS 2 ↔ Microcontroller Integration (Weeks 5–8)">​</a></h2>
<ul>
<li>
<p>Week 5 – Intro to micro-ROS: Learn how micro-ROS brings ROS 2 into microcontrollers . Read the micro-ROS docs about the client-agent architecture (ROS 2 nodes on MCU acting as clients to an agent on PC). Follow a tutorial to set up micro-ROS on the STM32 (e.g. using FreeRTOS + micro-ROS) . Start with a basic micro-ROS publisher on the STM32 that sends IMU or dummy data to a PC ROS 2 node (the micro-ROS agent). Milestone: STM32 board successfully publishes a ROS 2 topic via micro-ROS (e.g. a sensor_msgs/Imu or a custom message) and you can echo it on the PC.</p>
</li>
<li>
<p>Week 6 – Two-Way Communication: Extend to a subscriber on the microcontroller. For instance, a PC ROS 2 node could publish a command (like an LED toggle or motor speed), and the STM32 (or ESP32) running micro-ROS subscribes and acts on it. Ensure real-time behavior by running the micro-ROS node in its own task (if using RTOS). This solidifies understanding of DDS-XRCE communication. Experiment with both STM32 and ESP32-S3 for comparison – ESP32 has Wi-Fi, so you might try a UDP transport for micro-ROS. Milestone: Achieve bi-directional ROS 2 comms – send a command from a ROS 2 terminal to toggle an MCU LED or move a servo, and confirm feedback from MCU sensors in ROS 2.</p>
</li>
<li>
<p>Week 7 – Motor Control 101: Begin controlling motors with the microcontroller. Start simple: a DC motor or servo motor. If using a DC motor with encoder, set up a timer interrupt to read the encoder and a PWM output to drive the motor driver. Implement a basic velocity PID controller on the MCU to maintain a set speed. If using a servo (which has internal control), practice commanding positions via PWM. This will prepare you for Field-Oriented Control later by understanding control loops. Integrate ROS 2 by sending motor commands from a PC node to the MCU. Milestone: One motor can be controlled in closed-loop (speed or position) by the microcontroller, with commands and telemetry accessible in ROS 2 (e.g., you can set a speed via a ROS topic and the motor maintains it, reporting encoder ticks).</p>
</li>
<li>
<p>Week 8 – Real-Time Control &amp; Sensing: Refine the motor control with real-time considerations. Use FreeRTOS on STM32 to run the control loop at a fixed frequency (e.g. 1 kHz for motor PID) in one thread, and ROS communication in another. Add the IMU into the loop: mount the IMU on the motor or a platform and try a simple balancing experiment. For example, if you mounted a rod or platform on the motor, attempt to keep it upright (inverted pendulum style) using the motor – a classic control problem to tune your PID. This may be challenging, but even partial success teaches a lot. Meanwhile, stream IMU and encoder data to ROS 2 for debugging (e.g. plot angles in rqt_plot). Milestone: Hard real-time control is running on the MCU, and you’re comfortable with multitasking on FreeRTOS and tuning PIDs. You have possibly achieved a simple balance control or at least a well-tuned motor controller.</p>
</li>
</ul>
<p>Checkpoint (End of Month 2): Your microcontroller acts as a ROS 2 node , bridging the embedded and ROS worlds. You’ve controlled a motor with feedback, a critical step for any robot. You’re ready to build actual robotic hardware.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="month-3-building-a-mobile-robot--advanced-control-weeks-912">Month 3: Building a Mobile Robot &amp; Advanced Control (Weeks 9–12)<a href="https://www.dronnex.com/doc/blog/rtere#month-3-building-a-mobile-robot--advanced-control-weeks-912" class="hash-link" aria-label="Direct link to Month 3: Building a Mobile Robot &amp; Advanced Control (Weeks 9–12)" title="Direct link to Month 3: Building a Mobile Robot &amp; Advanced Control (Weeks 9–12)">​</a></h2>
<ul>
<li>
<p>Week 9 – Build a Basic Robot Platform: Using your hardware skills, construct a simple ground robot for testing control algorithms. One suggested project is a self-balancing two-wheeled robot (like a mini Segway): it uses two motors and an IMU – leveraging your Week 8 progress. Alternatively, build a single-leg test rig: a 2-DOF leg (hip and knee) with encoders. The balancing robot is easier as it’s a well-known project, so consider starting there. Assemble the chassis, mount the STM32 (or ESP32) as the controller, and wire up the IMU and motors (and motor drivers or ESCs). Milestone: Mechanical build completed for a small robot (balancing bot or leg prototype), and all sensors/motors are interfaced with the microcontroller.</p>
</li>
<li>
<p>Week 10 – Balance Control with Feedback: Program the balancing robot to achieve upright balance. This involves using the IMU to sense tilt and adjusting motor speeds to correct it (a classic inverted pendulum control problem). Use a PID or even more advanced controllers if needed. You might spend a lot of time tuning gains here – that’s normal. Integrate ROS 2 for monitoring and high-level control: for example, create a ROS 2 node that sends a target tilt angle or velocity to the robot. Achieve remote control: send a forward command, and the robot leans forward to roll that direction while self-balancing. If working with the single leg instead, focus on inverse kinematics – move the leg end-point in a straight line by coordinating hip and knee joints, using kinematic equations. Milestone: The balancing bot can stand upright for an extended period and respond to simple ROS 2 commands (e.g., move forward/backward while balancing), or the single leg can precisely move to commanded positions, showing mastery of kinematics and feedback control.</p>
</li>
<li>
<p>Week 11 – Toward a Quadruped – Actuators Setup: Begin planning your quadruped robot (robot dog). Determine the actuator setup: for high performance, use brushless BLDC motors with encoders on each joint to enable torque control; or initially, use simpler servos to prototype leg motion. Given your goal of 4–6 Field-Oriented Control (FOC) motors, you may choose to use 8–12 motors (3 per leg for a true quadruped). If you have brushless gimbal motors and encoders (common in drone gimbals), try the SimpleFOC library or reference designs to control them on the STM32. Each motor will run FOC for smooth torque control – critical for legged robots to act compliantly. Milestone: One joint module is set up with a BLDC motor + encoder under FOC control. For example, you can command a joint to hold a position or apply a certain torque. (FOC enables “superior torque control” for brushless motors , which is why we pursue it.)</p>
</li>
<li>
<p>Week 12 – Quadruped Mechanical Assembly: Design or obtain a frame for your quadruped. You might 3D print parts or use an open-source design like Solo 8 as inspiration (an open-source torque-controlled quadruped) . Assemble the quadruped’s legs and body, starting with a minimal setup (even two legs to form a biped can be a start, then add others). Mount motors, encoders, and necessary microcontrollers on the robot. If using multiple MCUs (one per leg or per few motors), set up a communication bus (CAN bus or serial) between them, or have each run micro-ROS and communicate via the ROS 2 agent on a central computer. Milestone: The physical quadruped robot is assembled with all actuators in place. When powered, you can individually command each joint via your microcontroller code (e.g., each leg can be moved through its range under control).</p>
</li>
</ul>
<p>Checkpoint (End of Month 3): You have a working small robot (balancer or leg) and a proto-quadruped assembled. Crucially, you’ve implemented FOC motor control and multi-motor coordination on microcontrollers, a big step toward dynamic leg control . At this stage, you’re comfortable with sensors, actuators, and ROS 2 integration on real hardware.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="month-4-gait-development--ros-2-integration-weeks-1316">Month 4: Gait Development &amp; ROS 2 Integration (Weeks 13–16)<a href="https://www.dronnex.com/doc/blog/rtere#month-4-gait-development--ros-2-integration-weeks-1316" class="hash-link" aria-label="Direct link to Month 4: Gait Development &amp; ROS 2 Integration (Weeks 13–16)" title="Direct link to Month 4: Gait Development &amp; ROS 2 Integration (Weeks 13–16)">​</a></h2>
<p>Solo 8 open-source quadruped performing dynamic motion. By this phase, you will have a basic quadruped robot and start developing locomotion gaits, leveraging torque-controlled joints similar to those in Solo 8 .</p>
<ul>
<li>
<p>Week 13 – Basic Gait Implementation: With the quadruped’s hardware ready, program a simple walking gait. Start with a crawl gait (one leg moves at a time, keeping 3 on ground for stability). Write a schedule for leg swings and use your motor controllers to move joints accordingly. Initially, use position control (move joints to predefined angles for each phase of gait). Test on the real robot slowly – you might need to support it to prevent falls during tuning. Use ROS 2 to send high-level commands like “walk forward” or “turn”, which your MCU code translates into gait patterns. Milestone: The quadruped takes its first steps, albeit slowly and quasi-statically. You can command it via ROS 2 to walk forward a short distance on flat ground.</p>
</li>
<li>
<p>Week 14 – Feedback and Stability: Incorporate feedback to improve stability. Use the IMU on the robot body to detect if it’s tilting too far. Implement a basic stabilization: e.g., adjust leg positions or timings if pitch or roll exceeds a threshold (a simple form of balance control). You could also add foot sensors (switches or pressure sensors) to detect contact – ensuring a leg has touched ground before lifting another. Start experimenting with more dynamic gaits like trot (moving diagonal legs together) once you have some feedback control. In trot, the robot will have moments with only two legs on ground, so your IMU-based balancing becomes crucial. Milestone: The robot can trot a few steps without falling, using IMU feedback to self-correct minor disturbances.</p>
</li>
<li>
<p>Week 15 – ROS 2 Perception Integration: Mount a camera (e.g. an ESP32-CAM or USB camera feeding into a PC) on the robot. While locomotion is still under refinement, begin integrating perception to move toward an “AI-capable” robot. For instance, use the camera feed in a ROS 2 node running on a companion computer (like a laptop or Raspberry Pi) and perform a simple vision task (like detecting a colored ball or AprilTag). This week, keep the task simple: e.g., the robot looks around (turns in place) until the camera sees a target, then you stop. This sets up the pipeline for more advanced uses where vision could inform locomotion (obstacle avoidance, path planning). Milestone: Camera data is accessible in ROS 2, and you have a basic vision-in-the-loop demo (even if just teleop: you can drive the robot via a gamepad using camera view).</p>
</li>
<li>
<p>Week 16 – Autonomous Control Mode: Combine your progress to enable a simple autonomous behavior. For example, implement a “go to goal”: place a colored object and have the robot walk towards it. This involves tying vision to locomotion – e.g., a ROS 2 node processes camera frames (perhaps using your YOLO knowledge to detect the object), computes a direction, and sends a velocity command to the quadruped. You may use ROS 2 Navigation stack’s planner for high-level path planning on a map, but since legged locomotion is complex, you can also do a simpler approach: if object is seen, turn towards it and move forward until a certain distance. Ensure your locomotion controller can handle commands for direction and speed (you might implement a rudimentary teleop interface that translates desired forward/turn velocities into gait parameters). Milestone: The robot demonstrates an autonomous task on flat terrain – e.g., it can walk toward a visual target or follow a simple preset route without constant human micromanagement.</p>
</li>
</ul>
<p>Checkpoint (End of Month 4): Your quadruped robot is walking under its own control. It handles basic gaits with sensor feedback and can be controlled via ROS 2 commands. You’ve also integrated a camera sensor, laying groundwork for AI and autonomy. At this point, your skill set spans embedded motor control, real-time feedback loops, ROS 2 middleware, and even some high-level perception – a huge stride toward advanced robotics engineering.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="month-5-reinforcement-learning-for-locomotion-weeks-1720">Month 5: Reinforcement Learning for Locomotion (Weeks 17–20)<a href="https://www.dronnex.com/doc/blog/rtere#month-5-reinforcement-learning-for-locomotion-weeks-1720" class="hash-link" aria-label="Direct link to Month 5: Reinforcement Learning for Locomotion (Weeks 17–20)" title="Direct link to Month 5: Reinforcement Learning for Locomotion (Weeks 17–20)">​</a></h2>
<ul>
<li>
<p>Week 17 – Simulation Setup for RL: Transition into the learning phase to push the robot’s capabilities. Set up a physics simulation of your quadruped (using Gazebo Ignition, PyBullet, or Isaac Gym). Import your robot’s URDF model into the simulator; simplify if needed (e.g., use a simplified collision model, approximate motor behavior). Verify that the simulated robot can stand and manually walk with the same gait commands as the real robot. Simulation allows safe experimentation with advanced control. Milestone: You have a working simulation environment for the quadruped, and you can teleop the simulated robot in ROS 2, confirming that the model behaves reasonably like the real hardware.</p>
</li>
<li>
<p>Week 18 – RL Problem Formulation: Define the reinforcement learning problem for locomotion. Decide on the state representation (e.g., IMU angles, body orientation, joint angles, joint velocities, foot contact indicators) and the action space (e.g., target joint positions or torques). A common approach is to have the policy output desired joint angles or torque offsets for each motor at each control step . Design a reward function that encourages forward walking while penalizing falls, high tilt, or unstable motions . For example, reward forward velocity and penalize deviation from upright posture. Use an existing RL library (Stable Baselines3 with PPO is a good choice, as Proximal Policy Optimization (PPO) is popular for legged robots ). Set up the training code, connecting it to the simulation (maybe through ROS topics or a direct physics engine API for speed). Milestone: Your RL training environment is ready – you can run episodes in simulation where the quadruped starts, attempts to walk, and gets rewards. You’ve essentially defined “what” the robot should learn (policy maps observations to motor commands) .</p>
</li>
<li>
<p>Week 19 – Train Locomotion Policy: Run training for your RL policy in simulation. This may take many iterations, so use your powerful PC or cloud if available, and consider parallel simulations for efficiency . Monitor training progress: the robot should start off random, but over time learn to move forward without falling. Tweak hyperparameters or reward terms if it gets stuck (for example, add a small reward for moving any leg to encourage exploration). Implement domain randomization during training – randomize factors like friction, motor strength, or sensor noise . This helps the learned policy be robust for the real world (improving sim-to-real transfer). By the end of the week, you might have a policy that can make the simulated robot walk with decent stability. Milestone: A trained RL policy (neural network controller) that enables the simulated quadruped to walk forward (and maybe turn) consistently. You should also document results – e.g., the policy achieves a certain average reward or distance without falling, in simulation.</p>
</li>
<li>
<p>Week 20 – Real-World Deployment Prep: Prepare to deploy the learned policy on the real robot (this is where things get exciting!). Ensure you can run the neural network in ROS 2 – likely on a companion computer (Raspberry Pi, NVIDIA Jetson, or a laptop) because a STM32/ESP32 is not powerful enough for runtime inference of a large network. You might use TensorFlow Lite or ONNX runtime on the companion computer. Connect the policy output to your robot’s ROS control interface: for instance, the policy outputs desired joint angles at 20 Hz, and you have a ROS 2 node sending those to the microcontrollers which execute position control. Plan safety measures: start with lower torques/speeds, have an e-stop ready. Milestone: The RL policy is integrated into the ROS 2 control stack, ready to drive the real robot in testing. All components (policy inference node, micro-ROS motor controllers, etc.) are communicating properly.</p>
</li>
</ul>
<p>Checkpoint (End of Month 5): You have gained experience with deep reinforcement learning for locomotion. In simulation, your robot learned to walk via trial and error – a cutting-edge capability  . You’ve set the stage to attempt sim-to-real transfer of this learned behavior onto your actual quadruped.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="month-6-sim-to-real-transfer--advanced-control-weeks-2124">Month 6: Sim-to-Real Transfer &amp; Advanced Control (Weeks 21–24)<a href="https://www.dronnex.com/doc/blog/rtere#month-6-sim-to-real-transfer--advanced-control-weeks-2124" class="hash-link" aria-label="Direct link to Month 6: Sim-to-Real Transfer &amp; Advanced Control (Weeks 21–24)" title="Direct link to Month 6: Sim-to-Real Transfer &amp; Advanced Control (Weeks 21–24)">​</a></h2>
<ul>
<li>
<p>Week 21 – Deploy RL Policy on Robot: Time to see your robot walk under AI control. Run the trained policy on the real quadruped. Initially, keep the robot on safety harness or support to prevent hard falls. It will likely stumble as the dynamics differ from simulation. Observe its behavior: does it move its legs in a coordinated way? If it falls quickly, you may need to refine the policy or increase randomization. You could also try online learning: have the robot attempt to walk and, if safe, let it continue learning on hardware in small bursts (though this is risky and requires careful reward handling). More practically, adjust the policy using slight system identification tweaks – e.g., scale outputs or add a secondary balance controller (PID using IMU) that corrects small errors on top of the policy. Milestone: The quadruped can take a few steps in reality with the RL policy. It might not be perfect, but even a few seconds of autonomous learned walking is a big achievement in sim-to-real reinforcement learning.</p>
</li>
<li>
<p>Week 22 – Iterate and Improve: Likely, you will iterate between simulation and real testing. Analyze failure cases on the real robot: for example, maybe the policy wasn’t exposed to enough friction variation and the feet slip, or it oscillates. Update your simulation environment to include those cases (e.g., add slight delays or sensor noise, or use a higher fidelity physics engine if needed). Train a new policy or fine-tune the existing one with domain randomization more aggressively to bridge the gap . Another approach is Residual Learning: keep a classical controller (from Month 4) and have the RL policy learn a residual correction on top of it – this can ease the learning burden. Continue this sim-to-real refinement cycle. Milestone: A revised policy that significantly improves real-world performance – e.g., the robot can walk across your living room floor reliably for a minute or more without falling. Each iteration reduces the gap between simulated and real behavior.</p>
</li>
<li>
<p>Week 23 – Advanced Locomotion Skills: With a working walking policy, you can push further. For instance, train (or program) the robot to turn and maneuver to follow a path. Extend the observation space to include a target direction or point, so the policy can guide the robot toward it. You could also work on speed control – train the policy to walk at different commanded speeds (include the desired speed as an input). Meanwhile, test the robot on slightly uneven terrain (a foam mat or a small ramp) to see how it adapts. This will highlight the robustness of your controller. You might incorporate Rapid Motor Adaptation (RMA) techniques or other recent research that allow on-the-fly adaptation to terrain by estimating ground parameters – cutting-edge stuff, but worth reading up on if interested . Milestone: The robot demonstrates turning and variable-speed walking, possibly using the learned policy with additional inputs. It can handle mild terrain variations or recover from small pushes, showing a degree of robustness and adaptability.</p>
</li>
<li>
<p>Week 24 – Evaluation and Demo: Consolidate everything you’ve achieved in a final demonstration. For example, set up an obstacle course or a set of tasks: the robot must walk to a location, avoid an obstacle (you can use simple logic or a separate planner for avoidance), and maybe perform a trick like standing up after a fall or climbing a small step (if hardware permits). Use ROS 2 bags to record data during these tests for later analysis of performance (e.g., analyze how stable the roll/pitch were, how often it had to stop to regain balance, etc.). Compare the performance of your RL controller to the earlier hand-crafted gait: is it smoother or more efficient? Often RL can discover gait patterns that are non-intuitive but effective . Document these findings. Milestone: A complete demonstration of your robot dog’s capabilities is recorded – you’ve essentially built and trained a legged robot that can perceive and navigate simple scenarios on its own.</p>
</li>
</ul>
<p>Checkpoint (End of Month 6): You have brought state-of-the-art techniques to fruition: your robot learned to walk through reinforcement learning and operates in the real world. This experience is directly in line with modern legged robotics research, where learning-based controllers are unlocking agility previously seen only in very expensive systems . You’re now truly functioning at the level of a professional embedded/robotics engineer pushing the boundaries of locomotion.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="months-79-mastery-and-expansion-weeks-2536">Months 7–9: Mastery and Expansion (Weeks 25–36)<a href="https://www.dronnex.com/doc/blog/rtere#months-79-mastery-and-expansion-weeks-2536" class="hash-link" aria-label="Direct link to Months 7–9: Mastery and Expansion (Weeks 25–36)" title="Direct link to Months 7–9: Mastery and Expansion (Weeks 25–36)">​</a></h2>
<ul>
<li>
<p>Weeks 25–28 – Deep Dive into Embedded Systems: Use this period to reinforce low-level expertise that underpins advanced robots. Revisit your STM32 motor control firmware and attempt to optimize it. For example, implement Field Oriented Control from scratch (without relying on SimpleFOC) to deepen your understanding of the math (Park/Clarke transforms, PID for Id/Iq currents). Test the custom FOC on one joint and compare torque control performance to the library version. Additionally, explore advanced microcontroller features: use DMA to handle sensor data without CPU, implement CAN bus communications between multiple controllers for coordinating legs (many quadrupeds use one MCU per leg or per joint, networked over CAN for synchronization ). Also experiment with FreeRTOS features like message queues or timers to make your firmware more robust. This deepens your embedded systems mastery, ensuring you can build reliable real-time systems.</p>
</li>
<li>
<p>Weeks 29–32 – Bipedal Robotics (Stretch Goal): Tackle the next challenge: bipedal robots. Repurpose your quadruped by using only two legs (or design a new small biped). Start with getting a biped to stand using feedback control (this is essentially two balancing robots stacked). Implement a balance controller using both feet (shifting weight). Reuse your IMU and foot sensors for detecting when a foot is on the ground. Gradually work on walking – start with side-to-side weight shifting, then stepping one foot forward (perhaps with a boom or support to catch it). This project will test everything you’ve learned: real-time control, feedback loops, and possibly RL (bipedal walking is even harder than quadrupedal). Even if full dynamic walking is not achieved in this short time, the attempt will massively increase your understanding of humanoid balance and control algorithms (inverted pendulum models, zero-moment point, etc.). By the end of Week 32, you might have a biped that can take a step or at least stand and recover from a push for a few seconds. This is excellent preparation for eventually tackling drones again, since balancing a biped has parallels to stabilizing a drone (both are complex dynamic systems).</p>
</li>
<li>
<p>Weeks 33–36 – Final Projects and Transition to Drones: In the final stretch, consolidate your knowledge by undertaking a capstone project. One idea: integrate an arm or manipulator onto your quadruped (or use a 6th motor on one of the legs as a small grabbing arm) and program a manipulation task (like pushing a door or picking up an object). This adds another layer (manipulator kinematics and control) on top of locomotion – reflecting real-world robotics problems where a robot needs to both move and interact with objects. Another idea is to revisit drones, now armed with ROS 2 and advanced control skills: build a drone that uses ROS 2 flight controllers (PX4 has ROS 2 support) and maybe implement an autonomous vision-based landing or navigation system to tie in AI skills. Allocate time to document your projects, polish your code, and perhaps contribute to open-source projects or publish a blog/video about your robot dog’s journey. This will not only solidify what you learned by teaching others, but also showcase your skills to potential employers or collaborators. Milestone (End of Month 9): You have one or two impressive portfolio projects (e.g., a walking robot dog with an arm, or an autonomous drone mission) demonstrating broad mastery. You can confidently call yourself an Embedded &amp; Robotics Engineer with expertise in ROS 2, microcontrollers, motor control, and machine learning for robots.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-and-next-steps">Conclusion and Next Steps<a href="https://www.dronnex.com/doc/blog/rtere#conclusion-and-next-steps" class="hash-link" aria-label="Direct link to Conclusion and Next Steps" title="Direct link to Conclusion and Next Steps">​</a></h2>
<p>By following this 9-month roadmap, dedicating ~3 hours on weekdays and heavy practice on weekends, you’ve progressed from a software developer to a hands-on roboticist. Along the way, you focused on ROS 2 and relevant tools (no time wasted on unrelated simulations), dove deep into STM32 and ESP32 microcontroller programming (timers, RTOS, drivers, etc.), and implemented real-time control for multiple FOC motors with sensor feedback. You built a legged robot from scratch and leveraged modern reinforcement learning to achieve locomotion that would be very difficult to hand-engineer, aligning with cutting-edge developments in the field . Each phase included 1–2 week projects and milestones, ensuring steady progress and a sense of achievement.
Going forward, you can extend these foundations to more complex systems (like full humanoid robots or aerial vehicles). The focus on ROS 2 and microcontrollers will serve you well in any advanced robotics project – including future drone development when you circle back to it with your new skill set. By continuously challenging yourself with projects (as we structured with regular checkpoints), you’ve built not only knowledge but also a robust engineering workflow for solving robotics problems. Congratulations on your journey to becoming a professional embedded/robotics engineer, and best of luck building the next robot dog or whatever innovative robot comes next!</p>
<p>Sources:</p>
<ul>
<li>ROS 2 Tutorials – ROS 2 official documentation (Humble)</li>
<li>micro-ROS Overview – ROS 2 for Microcontrollers</li>
<li>FOC for Torque Control – Brushless motor controller for legged robots</li>
<li>Solo 8 Open-Source Quadruped – NYU &amp; MPI torque-controlled robot</li>
<li>RL Locomotion Training – Federico Sarrocco, “Quadrupeds Learning to Walk”</li>
<li>Robot Learning Applications – Solo 8 press release (NYU)</li>
</ul>]]></content:encoded>
            <category>chatgpt</category>
        </item>
        <item>
            <title><![CDATA[Companion Computers in Drones]]></title>
            <link>https://www.dronnex.com/doc/blog/ccid</link>
            <guid>https://www.dronnex.com/doc/blog/ccid</guid>
            <pubDate>Sun, 20 Apr 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://www.dronnex.com/doc/blog/ccid#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>Drones across various industries are increasingly equipped with onboard computers (also called companion computers) that serve as the “brain” alongside the flight controller. These onboard systems process sensor data (especially camera feeds) in real time to enable high-level functions such as autonomous navigation, obstacle avoidance, and AI-based analysis (Top 5 Companion Computers for UAVs | ModalAI, Inc.).
Traditionally, a drone’s flight controller handled basic stabilization and GPS waypoint following, but modern use cases demand greater autonomy and on-site intelligence. Advances in compact, powerful processors (GPUs, NPUs, etc.) now allow drones to perform complex tasks like object detection, tracking, and mapping locally at the edge, which was not feasible just a few years ago (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). This shift to onboard edge computing yields low latency decisions and reduces reliance on constant communication links, an important benefit since drones often operate beyond reliable network coverage (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs).</p>
<p><strong>Onboard Hardware Choices:</strong> The choice of onboard computer depends on the required complexity and performance. NVIDIA’s Jetson family (e.g. Nano, TX2, Xavier, Orin) is popular for drone AI workloads, featuring CPU/GPU architectures that can run neural networks for vision tasks in real time (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). These deliver high TOPS (trillions of operations per second) for deep learning, enabling on-device inference for applications like image classification, object detection (e.g. using YOLO models), and SLAM. In contrast, hobbyist boards like the Raspberry Pi or Radxa single-board computers offer a low-cost, lightweight platform suitable for less intensive tasks or early prototyping (Top 5 Companion Computers for UAVs | ModalAI, Inc.) (Top 5 Companion Computers for UAVs | ModalAI, Inc.). However, such boards lack dedicated neural accelerators – heavy computer vision models run slowly on them, incurring latency for deep learning frameworks (Top 5 Companion Computers for UAVs | ModalAI, Inc.). To bridge this gap, developers sometimes augment lower-power boards with USB AI accelerators (e.g. Intel Neural Compute Stick or Google Coral) to offload neural network inference. In practice, real-world drone deployments span this spectrum: from simple microcontrollers paired with a Raspberry Pi for basic image capture, up to powerful AI mission computers like Jetson Xavier or Qualcomm Snapdragon Flight in fully autonomous drones. The following sections provide a comprehensive overview of how onboard computers are used in major industries – agriculture, security, delivery, mapping/inspection, and other industrial applications – detailing the typical hardware, vision vs. non-vision workloads, and why on-board computing is essential in each context.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="agriculture-precision-farming">Agriculture (Precision Farming)<a href="https://www.dronnex.com/doc/blog/ccid#agriculture-precision-farming" class="hash-link" aria-label="Direct link to Agriculture (Precision Farming)" title="Direct link to Agriculture (Precision Farming)">​</a></h2>
<p>Agriculture was an early adopter of drone technology for monitoring crops and automating fieldwork. Today, many precision farming drones carry onboard computers to analyze sensor data and make split-second decisions in the field. In crop scouting and surveying, a drone might capture multispectral or RGB images of fields and use onboard processing to stitch images or calculate vegetation indices on the fly. More advanced systems perform real-time computer vision: for example, identifying crop stress, detecting diseased plants, or locating weeds among crops during the flight. UAVs are now widely used in precision agriculture for crop monitoring and targeted spraying, which improves farming efficiency and reduces environmental impact (Frontiers | Ag-YOLO: A Real-Time Low-Cost Detector for Precise Spraying With Case Study of Palms). An onboard computer can directly interpret camera feeds to differentiate healthy crops from weeds or pests, enabling immediate action such as precision spraying of agrochemicals only where needed. This is a leap from the traditional method of capturing images for later analysis – instead, the “see-and-spray” drone can act during the same flight.</p>
<p>On the hardware side, agricultural drones often leverage lightweight AI computers. NVIDIA Jetson modules (Nano, TX2, etc.) have been used in research prototypes to run neural networks that segment weeds vs. crops in real time, guiding an attached spray mechanism (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism) (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). For instance, one study deployed a Jetson Nano onboard a drone to perform semantic segmentation of weeds at ~25 FPS, enabling a UAV to spray herbicide precisely on detected weeds (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism) (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). Another low-cost approach integrated an Intel Neural Compute Stick 2 (Myriad X VPU) with a small single-board computer to run a custom “Ag-YOLO” object detection model for palm tree disease, achieving 36 FPS detection with only a 1.5 W, 18-gram device (Frontiers | Ag-YOLO: A Real-Time Low-Cost Detector for Precise Spraying With Case Study of Palms) (Frontiers | Ag-YOLO: A Real-Time Low-Cost Detector for Precise Spraying With Case Study of Palms). These examples highlight that both high-end GPUs and specialized accelerators are employed to meet the performance needs of vision-based farming tasks.</p>
<p>Not all agricultural use cases are vision-based, however. Some drones carry other sensors (such as thermal cameras, LiDAR altimeters, or hyperspectral sensors) and use onboard computing to interpret this data. For example, a crop-spraying drone might use a LiDAR or ultrasonic sensor to maintain ultra-low flight altitude over uneven fields; the onboard computer reads this sensor input in real time to adjust the drone’s height, ensuring even coverage. Another non-vision example is soil and microclimate sensing: a drone might measure temperature, humidity, or soil moisture via IoT sensors and map these readings to GPS coordinates, requiring onboard logic to synchronize sensor data with location. In all these cases, the onboard computer is essential because farmland environments often lack reliable internet connectivity – the drone must make decisions on-site (where to spray, which areas need attention) without offloading data. This on-the-fly intelligence boosts efficiency (e.g. less chemical usage by targeting only weed-infested spots) and enables greater autonomy in agricultural operations. Below are some key use cases in agriculture leveraging onboard computation:</p>
<ul>
<li>
<p>Real-Time Weed Detection &amp; Spraying: Drones equipped with AI vision can distinguish weeds from crops in real time and actuate spot-spraying. For example, a Jetson-based drone uses a trained deep learning model (YOLO/segmentation) to identify weeds among crops and trigger a targeted herbicide spray, significantly reducing chemical use (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism) (Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism). This requires considerable processing power on the drone to run inference with low latency as the UAV moves.</p>
</li>
<li>
<p>Crop Health Monitoring: Onboard computers process multispectral or RGB images to assess crop health indicators (NDVI, pigment indices) during flight. The drone can immediately flag stressed crop regions (due to drought or disease) and perhaps even alter its route to closer inspect problem spots. These calculations can be done on modest hardware (a Raspberry Pi or Radxa board) since they involve simpler math, though more advanced analysis (like identifying specific diseases from images) would necessitate an AI module.</p>
</li>
<li>
<p>Autonomous Field Navigation: Farmland drones often fly beyond visual line of sight, so onboard processors handle path planning and obstacle avoidance. For instance, navigating around trees, power lines, or terrain is handled by computer vision (stereo cameras) or LiDAR sensors feeding into the onboard computer, which in turn instructs the flight controller to adjust course. This autonomy ensures safe operation at low altitudes over crops.</p>
</li>
<li>
<p>Variable-Rate Application: By merging sensor data and GPS maps, an onboard computer can control the variable release of seeds, water, or fertilizer. As the drone surveys a field, it might decide to increase fertilizer drop on an area of poor crop growth and decrease it elsewhere, all based on real-time processed data. While not strictly “vision,” this use of onboard analytics leads to site-specific farming that maximizes yield.</p>
</li>
</ul>
<p>Overall, agriculture showcases a range of onboard computing needs – from relatively low-complexity tasks like geo-tagging sensor readings (achievable with basic SBCs) to high-complexity AI tasks like vision-based weed control (demanding GPU-level performance). The autonomy level also varies: some drones merely assist a farmer by collecting data, whereas others (with onboard AI) can autonomously take actions like precision spraying with minimal human input. In each case, the onboard computer is a critical enabler for making timely decisions in the field, which improves efficiency, conserves resources, and reduces the need for constant human supervision.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="surveillance-and-security">Surveillance and Security<a href="https://www.dronnex.com/doc/blog/ccid#surveillance-and-security" class="hash-link" aria-label="Direct link to Surveillance and Security" title="Direct link to Surveillance and Security">​</a></h2>
<p>In surveillance, security, and public safety applications, drones act as mobile observers – often tasked with detecting intruders, monitoring crowds, or securing perimeters. These scenarios heavily rely on computer vision running on the drone’s onboard computer to interpret video feeds in real time. A security drone might patrol a fenced facility and use onboard object detection to spot humans or vehicles in restricted areas, immediately alerting security personnel. Similarly, law enforcement can deploy drones at events or in crime response, where the UAV’s onboard AI could track persons of interest through a crowd or follow a fleeing suspect from the air. The common thread is that the drone must “understand” what its camera sees, without waiting to stream video back to a control center – this demands robust edge processing. Indeed, one of the key requirements for UAV surveillance is the ability to detect and track objects of interest in real time (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). Modern deep-learning algorithms (object detectors like YOLO or trackers like SORT/DeepSORT) enable this but are computation-intensive, so a powerful onboard computer (often a GPU-equipped module) is typically used.</p>
<p><strong>Typical Onboard Setup:</strong> Drones for surveillance and security often carry high-end companion computers such as the NVIDIA Jetson TX2, Xavier NX, or newer Orin, which provide the CUDA cores and tensor accelerators needed for real-time vision. For example, researchers demonstrated a drone-based 4K pedestrian tracking system that achieved real-time performance by exploiting both the CPU and GPU of an onboard Jetson TX2 (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). This system could detect and follow people in ultra-high-resolution video (captured from 50 meters altitude) without any ground server, proving that onboard computing can handle even demanding surveillance tasks (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). In testing, the Jetson-based drone successfully detected and tracked individuals in 4K footage at full frame-rate, illustrating the level of performance now attainable on a compact drone (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). Another example is border security drones, which may use thermal cameras for night monitoring; the onboard computer can run thermal image analytics to pick out human heat signatures or vehicles in darkness. These use cases might leverage specialized models (e.g. thermal-trained object detectors) running on the same kind of GPU hardware. Some security drones also employ multiple sensors – optical cameras for day, IR cameras for night, and even acoustic or RF sensors – requiring sensor fusion on the onboard computer to make coherent decisions about potential threats.</p>
<p>Why is onboard processing essential for surveillance? Firstly, drones often operate in remote or wide areas (e.g. national borders, large industrial sites) where sending high-bandwidth video to a central server is impractical. The drone must be able to analyze video autonomously because real-time response is critical (it might need to decide to follow an intruder immediately). As one paper noted, wireless connections are not guaranteed aloft, so drones must rely on onboard mission computers for real-time tasks even though deep learning workloads demand high computing power (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). Secondly, latency needs to be minimal – a few seconds delay in detecting a person could mean the difference between losing or maintaining visual contact. Onboard AI eliminates the round-trip latency of streaming to the cloud and back. Finally, there’s a privacy aspect: processing video on the device means only relevant alerts (e.g. a detected face or license plate) might be transmitted, rather than raw video, which can be important for data security in sensitive surveillance (this benefit of edge computing for privacy is also documented in research (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs)).
Key vision-centered use cases in the security domain include:</p>
<ul>
<li>
<p>Perimeter Intrusion Detection: Drones can autonomously patrol fences or borders using onboard vision to detect humans, vehicles, or boats entering restricted zones. A Jetson Xavier-class computer running object detection (like YOLOv5) can identify an intruder and geo-tag their location for response teams. The drone might also track the intruder’s movement with onboard multi-object tracking algorithms, maintaining visual until ground units arrive. This use case demands robust performance to avoid missed detections and often uses thermal imaging at night (the onboard computer handles both IR and daylight video streams).</p>
</li>
<li>
<p>Crowd Monitoring and Anomaly Detection: Law enforcement or event security drones monitor large gatherings for public safety. Onboard image processing can count people, detect fights or unrest by recognizing specific motion patterns, or spot dangerous objects (e.g. a drone scanning a parking lot for unattended bags). These tasks use computer vision and sometimes machine learning classifiers (for action recognition) running on the drone’s GPU. The autonomy level can range from decision support (flagging something for a remote operator) to fully automated response (e.g. circling a flagged person).</p>
</li>
<li>
<p>Follow and Track Missions: In police or military operations, a drone may be tasked to follow a target vehicle or person semi-autonomously. Using onboard vision, the drone locks onto the target and navigates to keep the target in frame. This involves real-time object tracking and continuous adjustment of flight – a compute-heavy loop handled by the onboard computer. Skydio drones, for example, are known for their autonomy in tracking moving subjects through complex environments by leveraging their onboard NVIDIA Orin AI engines ( Skydio Autonomy™ | Skydio ). Such capability has been applied in tactical situations to reduce risk to human officers.</p>
</li>
<li>
<p>Non-Visual Sensors for Security: Although vision is primary, some security drones use other onboard sensor processing. For instance, a drone could carry a mini radar or LIDAR to detect other drones (counter-UAS scenarios) – the onboard computer would process these sensor signals to identify and triangulate rogue drones in protected airspace. Similarly, acoustic sensors onboard can pick up gunshot sounds and the computer can classify and localize them (functioning like a flying ShotSpotter). These examples show onboard computing beyond cameras, often supplementing vision for a more robust security solution.</p>
</li>
</ul>
<p>In summary, surveillance/security drones typically represent high-complexity, high-autonomy use cases. They often operate independently for extended periods, making on-board AI indispensable. The performance requirements are among the highest in the drone world: real-time processing of high-res video, detection of small or fast-moving objects, and reliable operation in unpredictable environments. This is why we see cutting-edge embedded GPUs and sophisticated AI models deployed on security drones – effectively turning them into flying edge computers that “see” and interpret the world in order to keep it secure.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="delivery-and-logistics">Delivery and Logistics<a href="https://www.dronnex.com/doc/blog/ccid#delivery-and-logistics" class="hash-link" aria-label="Direct link to Delivery and Logistics" title="Direct link to Delivery and Logistics">​</a></h2>
<p>Drone delivery – transporting packages by air – presents another major industry segment benefiting from onboard computers. In a delivery scenario, an unmanned aircraft must navigate from a distribution center to a customer’s location, then execute a safe drop-off or landing, all with minimal human oversight. This requires a high degree of autonomy and environmental awareness, since the drone will encounter dynamic obstacles (birds, wires, trees, buildings) and must ensure safety around people on the ground. To achieve this, delivery drones rely on a suite of sensors (visual cameras, depth sensors, sonars, sometimes radar) connected to powerful onboard computing for Detect-and-Avoid (DAA) and precision landing. In fact, for drone delivery to be truly autonomous, the UAV “needs to be able to see the world around it” via computer vision (Computer vision is key to Amazon Prime Air drone deliveries - Yahoo). Companies like Amazon Prime Air and Alphabet’s Wing have invested heavily in onboard vision systems that allow drones to identify hazards in flight and at the drop site in real time.</p>
<p>A prominent example is Amazon’s Prime Air delivery drones. Amazon has developed an onboard “detect-and-avoid” system with thermal cameras, depth cameras, and sonar working in concert, and machine learning models on the onboard computer to automatically identify obstacles and navigate around them (Amazon Drone Delivery Service Prime Air Is Advancing | Vaughn College). This means the drone’s AI can detect objects such as people, animals, power lines, and even something as thin as a clothesline, and then alter course or abort a landing if needed (Amazon Drone Delivery Service Prime Air Is Advancing | Vaughn College). The drone effectively makes split-second decisions based on its perceptions – for instance, if during descent to a customer’s yard the cameras spot a dog running underneath, the onboard computer will command the drone to pause or climb to avoid a close encounter. These capabilities are essential not only for safety but also for regulatory approval, as aviation authorities (FAA, etc.) require robust DAA for beyond-visual-line-of-sight flights.</p>
<p>The hardware used in delivery drones is typically at the high end of the spectrum. NVIDIA Jetson platforms or custom system-on-chips with AI accelerators are common, because the drone may be running multiple neural networks: one for sense-and-avoid in the air (finding other aircraft or birds), one for ground hazard detection while landing, perhaps another for reading markers (some delivery systems use fiducial markers or QR codes on landing pads to guide final approach). The autonomy level is very high – the goal is a drone that can depart, fly complex routes, and deliver with zero human intervention. Onboard computing is what makes this possible, handling tasks such as: route planning and re-planning (if it encounters a new no-fly zone or bad weather en route), real-time adjustment to wind or weather (using IMU and airspeed sensor data in control algorithms), and coordinating the delivery mechanism (like lowering a package by tether and releasing it when conditions are correct).</p>
<p>Importantly, delivery drones must also communicate and integrate with logistics systems, and the onboard computer often manages this. For example, it might encrypt and transmit telemetry and video snapshots back to an operations center over LTE/5G, or receive last-second updates (e.g. “customer moved 100m, update drop location”). While a lot of heavy lifting (like large-scale route optimization) is done by cloud services, the drone’s onboard system is the real-time executor that adapts on the fly.</p>
<p>Let’s break down key use cases and why onboard computing is crucial:</p>
<ul>
<li>
<p>In-Flight Obstacle Avoidance: Delivery routes often go through suburban areas with trees, poles, buildings, and possibly low-flying aircraft. Onboard sensors feed into algorithms (stereo vision or LiDAR-based obstacle detection) that create a 3D map of the drone’s surroundings as it flies. The onboard computer then either autonomously deviates around obstacles or notifies the autopilot to change course. This must happen within fractions of a second. For instance, Prime Air drones scan the skies and ahead for other aircraft or unexpected objects using computer vision, rather than relying solely on ADS-B or external trackers (A drone program taking flight - About Amazon) (Amazon gets FAA approval to expand drone deliveries - Axios). Such real-time perception and navigation absolutely require an onboard AI engine given the latency and reliability constraints.</p>
</li>
<li>
<p>Precision Landing and Drop-off: When arriving at the destination, a delivery drone might use downward-facing cameras and rangefinders to identify a safe landing zone or to position for dropping a package. Some systems use visual markers on the ground (like an augmented reality tag placed by the customer) – the drone’s onboard vision will recognize the marker and home in to that spot. Other times, the drone simply analyzes the camera feed to ensure no people, pets, or obstacles are in the immediate landing area. All of this logic lives on the drone. If any hazard is detected last-moment (e.g., the customer walks under the drone), the onboard computer can decide to delay or move the drop location. These decisions can’t wait for a human and are enabled by onboard image recognition and path planning.</p>
</li>
<li>
<p>Adaptive Route Autonomy: Delivery drones may be given a pre-planned route, but conditions can change. If wind conditions deteriorate or light rain begins, the drone’s onboard computer might adjust its flight envelope or speed. If an area becomes GPS-denied (perhaps near tall buildings), the drone could switch to visual-inertial navigation, again handled by onboard processing of camera/IMU data. In congested airspace, some delivery drones might coordinate via vehicle-to-vehicle comms; the onboard computer would handle such coordination protocols to avoid other drones. These are non-vision computations but still require robust processing and software logic running locally for autonomy.</p>
</li>
<li>
<p>Payload Management and Other Sensors: In specialized deliveries (like medical deliveries of blood samples or organs, as done by Zipline and others), maintaining the payload’s condition is critical. Onboard systems might regulate a cooler or monitor temperature sensors, ensuring the payload stays within safe conditions, and adjusting mid-flight if needed (e.g., return to base if a problem is detected). While not vision-related, this is another task for the onboard computer, integrating sensor data and acting on it. Additionally, post-delivery, the drone’s computer might run a quick self-diagnostic before returning to ensure no damage occurred during landing – for example, analyzing motor currents or camera feed for any anomalies like a snagged tether.</p>
</li>
</ul>
<p>In essence, drone delivery pushes the envelope on autonomy and complexity. These drones operate in unstructured environments among the general public, so the performance bar for onboard systems is very high: they must be fast, reliable, and redundant. It’s common to have redundancy (multiple cameras, perhaps two onboard computers cross-checking in critical systems) for safety. The onboard computer is the linchpin that ties together all sensor inputs and executes the flight logic that makes autonomous delivery possible. Without powerful onboard processing, a delivery drone would be limited to very controlled environments or would require a human pilot, which negates the scalability. Thanks to modern edge computers and AI, companies have demonstrated drones that can deliver packages beyond visual line of sight while dynamically reacting to their surroundings in real time (Amazon Drone Delivery Service Prime Air Is Advancing | Vaughn College).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="mapping-and-inspection">Mapping and Inspection<a href="https://www.dronnex.com/doc/blog/ccid#mapping-and-inspection" class="hash-link" aria-label="Direct link to Mapping and Inspection" title="Direct link to Mapping and Inspection">​</a></h2>
<p>One of the most widespread drone applications is aerial mapping and infrastructure inspection. This spans use cases like land surveying (creating orthomosaic maps and 3D terrain models), inspecting bridges, wind turbines, power lines, solar farms, and more. Traditionally, many of these missions were semi-autonomous: the drone followed a pre-set path to capture photos, and the heavy data processing (stitching photos into maps or analyzing them for defects) was done later on a ground station or cloud service. However, the advent of powerful onboard computers has begun to transform mapping and inspection into real-time or near-real-time endeavors. Drones in this domain increasingly carry companion computers to perform tasks such as in-flight image processing, immediate detection of anomalies, or even constructing 3D models on the fly. The level of autonomy is also rising – some advanced inspection drones can navigate around structures and decide their own camera angles using onboard Spatial AI engines.</p>
<p>(image) A drone inspects power substation infrastructure. Modern inspection drones utilize onboard vision and AI for collision avoidance and data analysis around complex structures. For instance, Skydio – a leading drone in autonomous inspection – is backed by an onboard NVIDIA Jetson Orin GPU, giving it the compute capacity to see, understand, and react in real time while inspecting assets ( Skydio Autonomy™ | Skydio ). With six 360° navigation cameras feeding its neural networks, the drone builds an understanding of the environment and avoids even small obstacles (like wires) automatically ( Skydio Autonomy™ | Skydio ) ( Skydio Autonomy™ | Skydio ). This allows it to be flown in cluttered spaces by users with minimal training. Furthermore, Skydio’s system can conduct targeted inspections and mapping on-board – its Spatial AI engine enables features like automated cell-tower scans and the creation of 3D models on the vehicle, in the field, in minutes ( Skydio Autonomy™ | Skydio ). This is a breakthrough for mapping jobs: rather than waiting to process data later, the drone itself can generate a usable 2D map or 3D point cloud before it even lands, letting teams evaluate results immediately and take action (or re-fly if needed) ( Skydio Autonomy™ | Skydio ) ( Skydio Autonomy™ | Skydio ).
There are two broad categories of tasks here: mapping (surveying, reconstruction) and inspection (defect or feature identification). Both benefit from onboard computing in different ways.</p>
<ul>
<li>
<p>Aerial Mapping &amp; Surveying: In a typical mapping mission (say, mapping a construction site or farmland for a survey), a drone captures hundreds of images with GPS tags. While high-precision processing (to create an orthomosaic or 3D mesh) is usually done on powerful computers after the flight, an onboard computer can perform quick intermediate processing. For example, it might stitch a low-resolution preview map on the fly, or use SLAM (Simultaneous Localization and Mapping) techniques to ensure it has covered the area without gaps. In long linear mapping (like pipeline or railway surveys), an onboard computer can monitor image quality and coverage and prompt the drone to take additional images of any missed segment. Some research projects have even implemented on-board photogrammetry to continuously build maps – though limited by compute, this is improving with devices like the Jetson Orin. The advantage is instant insights: field crews can know right away if they have the data they need. Also, if the drone has RTK GPS and a companion computer, it could georeference images in real time, outputting a map almost ready for use upon landing.</p>
</li>
<li>
<p>Infrastructure Inspection (Vision-Based): When inspecting physical structures, drones now often carry computer vision models to detect faults or areas of interest. For instance, an inspection drone may use an onboard convolutional neural network to analyze video frames for cracks in a concrete bridge or corrosion on a tower. If the model flags a potential defect, the drone can autonomously hover and circle that spot to collect more data. This kind of adaptive inspection is empowered by onboard processing – the drone doesn’t have to send all video to an operator; it can make preliminary judgments itself. Thermal cameras are also used (e.g. detecting hot spots in power lines or solar panels), and onboard analysis can immediately highlight abnormal heat signatures for the pilot. Using edge AI for this speeds up the inspection workflow and can reduce human error (the AI might see a subtle crack that a human could overlook in a live feed).</p>
</li>
<li>
<p>SLAM and GPS-Denied Navigation: Many inspection targets are indoors or in GPS-denied environments (inside large storage tanks, boilers, caves, or industrial plants). Here, drones rely on SLAM algorithms running on onboard computers to both map and navigate. For example, Flyability’s Elios 3 drone is equipped with a LiDAR and onboard SLAM engine called FlyAware™; as it flies inside a dark, cluttered space, it builds a 3D map in real time and uses that for stabilization and pilot feedback (Elios 3 vs. Elios 2: How do the Flyability drones compare) (Elios 3 vs. Elios 2: How do the Flyability drones compare). The Elios can instantaneously render a 3D map of its surroundings on the pilot’s tablet by processing LiDAR and visual odometry data on-board (Elios 3 vs. Elios 2: How do the Flyability drones compare). This is invaluable for inspecting confined spaces safely – the pilot can see areas the drone might have missed and navigate accordingly, and the final map ensures no spot is left uninspected. SLAM is computationally heavy (involves processing point clouds and running algorithms like ICP or graph optimization), so a robust onboard computer (often an ARM CPU paired with GPU or specialized VIO hardware) is required. Drones like Elios or indoor mapping drones may use Qualcomm Snapdragon-based flight cores or Jetson NX modules to achieve this balance of weight and processing.</p>
</li>
<li>
<p>Adaptive Flight Planning: With on-board intelligence, an inspection drone can make decisions like a human pilot would. For example, Skydio’s 3D Scan software (running partly onboard) allows the drone to autonomously map out a scanning pattern around a structure, adjusting angles to get complete coverage. It leverages the drone’s real-time mapping to know where it has a line-of-sight and where it hasn’t captured yet ( Skydio Autonomy™ | Skydio ). This level of autonomy turns what used to be a manual, labor-intensive flight into an automated routine, made possible by the drone’s onboard continuous computation of its position relative to the structure and the camera view planning.</p>
</li>
</ul>
<p>It’s clear that mapping and inspection use cases can range from moderate to very high complexity. Some mapping missions might use the onboard computer lightly (just for navigational assistance), whereas advanced inspection in tight environments pushes the limits of onboard processing (doing SLAM, AI defect detection, and path planning concurrently). In all cases, the onboard computer’s role is to increase the autonomy and data quality of the mission: ensuring the drone can go places and capture information that a human or a less-intelligent drone might miss or struggle with. The performance needs scale with the difficulty – a basic quadcopter surveying an open field might get by with a Raspberry Pi coordinating camera triggers, but a drone inspecting a wind turbine in gusty winds while identifying blade damage in real time will likely have an NVIDIA Orin or similar at its core. The trend in the industry is clearly toward edge computing on drones for mapping/inspection, as it reduces the amount of data to transfer (only results or models are sent back) and enables faster decision-making. Research has shown that equipping drones with edge AI drastically cuts the latency and bandwidth needed for remote sensing tasks – drones can now detect and recognize objects onboard, making quick local decisions (e.g., find a person to rescue) before sending any data to the cloud (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). This paradigm is making drones more efficient tools for mapping our world and inspecting critical infrastructure in real time.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="industrial-applications-warehousing-manufacturing-and-beyond">Industrial Applications (Warehousing, Manufacturing, and Beyond)<a href="https://www.dronnex.com/doc/blog/ccid#industrial-applications-warehousing-manufacturing-and-beyond" class="hash-link" aria-label="Direct link to Industrial Applications (Warehousing, Manufacturing, and Beyond)" title="Direct link to Industrial Applications (Warehousing, Manufacturing, and Beyond)">​</a></h2>
<p>Beyond the well-defined sectors above, onboard computers in drones are also driving innovative use cases in general industrial and enterprise environments. These include warehouse inventory drones, drones in manufacturing or chemical plants for monitoring, and other specialty uses like mining or oil-and-gas inspections that don’t squarely fall under “mapping” or “security.” In many of these scenarios, drones operate indoors or in close proximity to industrial equipment and workers, requiring extremely reliable autonomous navigation and often integration with business systems. Onboard computers play a pivotal role by handling the necessary computer vision, sensor fusion, and planning to allow drones to carry out tasks that used to be manual.</p>
<p>(Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology) An inventory drone scans warehouse shelves. In warehouses, drones are being used to automate inventory tracking by scanning barcodes on high shelves – a task traditionally done via forklifts and manual barcode readers. A prime example is Corvus One, an autonomous inventory drone. It carries an array of 14 cameras and an AI-based onboard system that lets it navigate in GPS-denied warehouse aisles and read pallet labels from the air (Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). Using computer vision, the drone localizes itself among the racks (without needing any external markers or infrastructure) and identifies barcodes on products or QR codes on shelving, associating them with the warehouse database. The on-board computer cross-references these scans with expected inventory and flags discrepancies automatically (Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology) (Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). Corvus Robotics (the company behind it) had to develop a learning-based autonomy stack for robust indoor flight, noting that traditional vision techniques alone were insufficient for “lifelong” autonomy in such environments (Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). This means the drone likely uses deep neural networks (perhaps for visual localization or obstacle avoidance) running onboard to adapt to changing warehouse scenes. The result is an infrastructure-free solution – no need for AR tags or external sensors – the drone’s onboard intelligence handles it all, making deployment much easier (Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). The complexity here is significant: the drone must avoid moving obstacles (workers, forklifts), adjust to varying lighting, and navigate narrow spaces, which demands high-performance processing and reliable algorithms on-board. Drones like these often utilize specialized computers; while exact hardware isn’t always disclosed, a powerful NVIDIA Jetson or Qualcomm QRB board with multiple camera inputs and NPUs is a likely choice, given the need to process many video streams and run neural networks for navigation and reading text (OCR).</p>
<p>Another industrial use is inspection and monitoring inside facilities (complementing what we discussed in mapping/inspection, but focusing on routine operations in factories or plants). For example, a drone can fly along pipelines or machinery to check for leaks, abnormal vibrations, or temperature spikes. Equipped with an infrared camera and perhaps a sniffer sensor (for gas leaks), the drone’s onboard computer can detect anomalies: e.g., using thermal image processing to spot an overheating motor, or reading analog gauges via computer vision to log pressure readings. Some drones have been prototyped to read instrument panels – using OCR on onboard video to record meter values, which is far faster than a human doing rounds. These tasks might not require as much AI as other examples; often classical image processing or simple threshold-based alerts are enough, which a Raspberry Pi-level computer could handle. However, as these drones become more autonomous (finding their own way through a factory floor), adding AI for navigation is key. We see some deployments combining SLAM for indoor navigation with specific sensor payloads. The onboard computer thus simultaneously maps the facility (so it knows its location and path) and analyzes sensor data for the monitoring task. This fusion ensures the drone can, say, autonomously inspect a series of checkpoints in a large plant at scheduled times, and alert personnel if it finds an anomaly – all without human control.</p>
<p>A particularly challenging environment is underground mining or industrial confined spaces. In mines, drones are used to map tunnels or assess ore stockpile volumes. They rely on onboard LiDAR processing to avoid collisions in tight shafts and to create volumetric models of mined material (to calculate how much has been extracted or remains). The onboard computer might run point cloud processing to estimate the volume in real time. Likewise, in large oil tanks or pressure vessels, drones (like the aforementioned Elios) fly inside to look for corrosion; their onboard SLAM and lighting systems allow them to operate without GPS or external light. The data (videos, 3D models) is recorded onboard and often also streamed to operators, but initial processing (stabilization, mapping) is done locally to facilitate the mission.
From a performance standpoint, industrial applications can vary widely. Warehouse inventory drones like Corvus require advanced vision and AI comparable to security drones – in fact, Corvus claims to be the first to deploy a neural network-based autonomy for indoor drones (Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology), highlighting the cutting-edge nature of that application. On the other hand, a drone that simply carries a gas sensor around a refinery might have modest computing needs (mostly for navigation and data logging). That said, even “simple” tasks benefit from edge computing: by processing data on-board, the drone can react immediately (e.g., hover in place when a gas reading spikes, to take more measurements, instead of continuing on a pre-planned path). Edge computing also ensures that data isn’t lost if communication drops out in a steel-and-concrete facility – the drone keeps a local record and can upload when back in range.
Let’s summarize some industrial use case categories and how they use onboard computers:</p>
<p>Warehouse Inventory Management: Drones navigate indoor warehouses to scan inventory (barcodes, RFID, images of products). They use onboard vision for localization and item identification. This is a vision-heavy use requiring AI/ML on board. The drone’s computer interfaces with warehouse management systems, updating stock counts in real time. The benefit is continuous, error-free inventory tracking without interrupting operations (Startup’s autonomous drones precisely track warehouse inventories | MIT News | Massachusetts Institute of Technology). The complexity is high: multi-camera SLAM, object detection (for codes), and safe autonomous flight in tight spaces are all handled onboard.</p>
<ul>
<li>
<p>Industrial Inspections (Interior): Drones perform scheduled inspections of equipment (pipes, tanks, conveyors) in factories or oil/gas facilities. Onboard computing may run anomaly detection on sensor data – for example, comparing current thermal images to baseline to catch a hotspot, or using sound/vibration analysis if the drone has a microphone. These drones often have to navigate cluttered interiors, so obstacle avoidance via LiDAR or vision is needed (again handled by the companion computer). Many such solutions emphasize removing humans from dangerous inspection jobs, so the drone must be trusted to fly safely and get the data. High autonomy and moderate to high compute is needed, depending on the sensors used.</p>
</li>
<li>
<p>Logistics and Parcel Movement: Beyond last-mile delivery outdoors, some drones are being tested for moving parts or goods within large facilities (e.g., shuttling components between factory stations or from storage to assembly line). Here the onboard computer manages routing in dynamic indoor environments. It might integrate with elevator or door controls, signaling them to open, etc. This is similar to an indoor warehouse drone but with a focus on carrying payloads. Vision or LiDAR can be used for navigation instead of relying on pre-laid routes, giving flexibility.</p>
</li>
<li>
<p>Non-Visual Sensing Missions: Industrial drones also carry non-camera payloads like chemical sensors, magnetometers, radiation detectors (for nuclear plant inspection). The onboard computer reads these sensors and can perform immediate data processing. For example, a radiation drone might map radiation levels to locations in real time to guide itself to hotspots; a magnetometer drone could follow a magnetic anomaly to find a crack in a pipeline. These tasks involve sensor fusion and decision-making by the onboard computer, albeit not as computationally heavy as image analysis. They demonstrate that onboard computing isn’t just about vision – it’s equally important for other sensor-driven missions.</p>
</li>
</ul>
<p>Across these industrial use cases, the differences in requirements are notable. Warehouse drones and complex inspection drones require cutting-edge autonomy, rivaling that of security drones, because they work around people and assets in a complex 3D environment (with no GPS). They often use the highest-performance onboard computers available (e.g., Jetson AGX Xavier or Orin with multiple CPU cores and GPU acceleration) to run their AI and SLAM pipelines. Simpler tasks in industrial sensing might be done with lower-cost boards or microprocessor-based flight controllers, but even these benefit from at least some onboard computing to format data and respond to triggers. Industrial users also value reliability and integration – the onboard computer might need industrial-grade safety certifications or the ability to log data in formats directly usable for compliance. This adds another layer of complexity: the computing platform must be robust and secure.</p>
<p>Finally, a common theme is connectivity: Industrial drones usually form part of a bigger system (warehouse management software, maintenance scheduling systems, etc.). The onboard computer enables this integration by running middleware or APIs that sync data when a link is available. But if the link is down or the drone is in a radio shadow, it can continue its mission thanks to local processing (improved reliability) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). It can store the collected info (inventory data or inspection footage) and later upload it. Thus, onboard computing in industrial contexts not only provides intelligence but also a buffer against network disruptions, ensuring the drone’s operation is autonomous and fail-resilient.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion-and-comparative-overview">Conclusion and Comparative Overview<a href="https://www.dronnex.com/doc/blog/ccid#conclusion-and-comparative-overview" class="hash-link" aria-label="Direct link to Conclusion and Comparative Overview" title="Direct link to Conclusion and Comparative Overview">​</a></h2>
<p>From the above, it’s evident that onboard computers have become a fundamental enabling technology for drones in virtually every industry. They allow drones to go from remote-controlled cameras in the sky to truly autonomous robots that can interpret and interact with their environment. By processing data on the drone, these systems drastically reduce latency, improve reliability, and unlock capabilities that would be impossible with a strictly human-in-the-loop or offboard processing model (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs).
Differences Across Use Cases: The complexity and performance requirements of onboard computing vary across applications. In agriculture, many missions are pre-planned and relatively structured (flying over flat fields), so the autonomy demands are lower – a mid-range onboard computer can handle tasks like image capture and even basic AI for weed detection. However, for precision tasks (e.g. weed spraying) the complexity jumps, approaching that of security drones, as the system must recognize small objects (weeds) and act immediately. Surveillance and security drones consistently require high complexity and high autonomy – they deal with unpredictable environments and critical real-time constraints (tracking fast or small targets), so they use the most powerful processors (Jetson, Snapdragon, etc.) and sophisticated algorithms. Delivery drones also occupy the high end: the safety-critical nature and need to handle all corner cases (wind, moving objects, privacy of people below) force a robust, high-performance onboard computing solution with redundancy. Mapping and inspection drones cover a wide range: a simple mapping drone might not push its onboard computer hard (saving most processing for later), whereas an advanced inspection drone essentially functions as an AI-powered coworker, navigating and analyzing concurrently, which is a high-performance, high-autonomy scenario. Industrial indoor drones tend to need the most reliable autonomy (to operate around valuable equipment and people safely), often matching the complexity of security drones but in a constrained setting – they typically run cutting-edge SLAM and vision onboard. That said, some industrial tasks can be achieved with simpler means if the environment is controlled (e.g. following fixed routes with a light sensor payload).</p>
<p>Vision vs. Non-Vision: Another important distinction is between vision-centric use cases and non-vision ones. Many of the celebrated advances (obstacle avoidance, object tracking, etc.) are vision-based and thus demand GPUs and complex models. Non-vision tasks (like carrying a gas sensor or reading a digital meter) may not need as much compute; they could run on microcontrollers or low-power CPUs. Yet, even those often get paired with vision for navigation. For example, a gas-sensing drone might still use a camera to avoid obstacles. So in practice, pure non-vision drones are uncommon in advanced use cases – most have at least a camera for navigation if not for the primary mission. The onboard computer’s job is to balance all these sensor inputs.</p>
<p>Why Onboard is Essential: Across all industries, a few common reasons explain why having a computer on the drone (versus processing data on the ground or cloud) is essential:
Latency and Real-Time Action: Drones often must react in milliseconds (to avoid a collision or target a spray). Onboard processing enables this instantaneous response; sending data to a remote server would be too slow (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs).</p>
<ul>
<li>
<p>Autonomy/Offline Capability: Drones operate in areas without reliable comms (farms, disaster zones, indoor facilities). Onboard intelligence lets them complete missions and make decisions even with zero connectivity (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). This autonomy also reduces the burden on pilots – one operator can supervise multiple drones that largely pilot themselves.</p>
</li>
<li>
<p>Bandwidth and Data Reduction: HD cameras generate huge data. Instead of streaming all of it, drones can process video onboard and send back only key results or alerts. This makes beyond-visual-line-of-sight operation feasible over limited bandwidth radio links (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs). It also lowers cloud compute costs and can preserve privacy by not broadcasting raw feeds (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs).</p>
</li>
<li>
<p>Task-Specific Optimization: An onboard computer can be tightly integrated with the drone’s control system. For instance, a vision algorithm can directly influence the drone’s flight (closing the control loop internally). This synergy can achieve maneuvers (like precision landings or smooth target tracking) that would be hard if the processing were offboard due to induced lag.</p>
</li>
<li>
<p>Increasing Capabilities: As drone roles expand (swarm coordination, package delivery networks, etc.), onboard computing provides a scalable way to add capabilities. Each drone can carry its own “intelligence” without needing a proportionate increase in manpower or constant remote control. This is crucial for scaling drone operations commercially.</p>
</li>
</ul>
<p>In practice, we see a blend of computing happening – some things are still done offboard (e.g., detailed 3D reconstruction for survey-grade maps) when not time-sensitive. But the frontier is continually moving towards more onboard processing as hardware improves. The NVIDIA Jetson series, Qualcomm Flight platforms, Intel Movidius VPUs, Google Coral TPUs, and others are all evolving to provide greater AI performance in smaller, more power-efficient packages, directly benefiting drone applications. For example, the latest Jetson Orin modules deliver dozens of TOPS of AI performance at under 20 W, enabling complex multi-model AI tasks on a drone in real time that used to require a full laptop or desktop GPU a few years ago (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs) (Towards Real-Time On-Drone Pedestrian Tracking in 4K Inputs). This trend will likely continue, with drones getting even more capable of understanding their environment – blurring the line between a flying camera and a thinking agent.
Commercial and Research Momentum: Many commercial solutions mentioned (DJI in agriculture, Skydio in inspection, Amazon in delivery, Corvus in inventory, etc.) showcase what is already possible with today’s onboard computers. Research projects often push the envelope further – for instance, swarms of drones performing cooperative tasks with minimal communication, each relying on onboard processing and only high-level coordination. The use cases will keep expanding as technology allows. One can envision future drones in emergency response that map a collapsing building interior in real time while searching for survivors using thermal and CO2 sensors, or agricultural drones that identify individual pest insects via onboard vision and precisely dispense biocontrol agents. All these hinge on edge AI and computing on the drone.</p>
<p>In conclusion, onboard computers have moved drones into a new era of autonomous, intelligent operation. Different industries leverage this capability in unique ways, but all benefit from the drone’s ability to make sense of its world and act immediately. The complexity ranges from basic autopilot assistance to full AI-driven missions, and choosing the right onboard computer (from Raspberry Pi class to Jetson Orin class) is a matter of matching the use case needs for vision processing, decision speed, and reliability. What’s common is that the drone is no longer just a remote sensor platform – it’s an active node in the IoT/edge computing network, often deciding locally and only then sharing results. This improves efficiency and opens up missions that were once thought too difficult or too risky for unmanned systems. As hardware and algorithms continue to advance, we will see even greater autonomy and a proliferation of drone applications across industries, all made possible by that little onboard computer humming away in the sky. (Top 5 Companion Computers for UAVs | ModalAI, Inc.) (AERO: AI-Enabled Remote Sensing Observation with Onboard Edge Computing in UAVs)</p>]]></content:encoded>
            <category>chatgpt</category>
        </item>
    </channel>
</rss>